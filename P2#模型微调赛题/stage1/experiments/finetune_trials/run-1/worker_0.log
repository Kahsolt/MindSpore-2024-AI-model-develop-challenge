/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:48:52.671.968 [mindspore/ccsrc/distributed/rpc/tcp/tcp_comm.cc:464] Connect] Waiting for the state of the connection to 127.0.0.1:8118 to be connected...Retry number: 1
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:48:53.672.758 [mindspore/ccsrc/distributed/cluster/cluster_context.cc:194] BuildCluster] Topology build timed out., retry(1/200).
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:48:56.672.917 [mindspore/ccsrc/distributed/cluster/cluster_context.cc:194] BuildCluster] Topology build timed out., retry(2/200).
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:48:59.673.099 [mindspore/ccsrc/distributed/cluster/cluster_context.cc:196] BuildCluster] Cluster is successfully initialized.
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:48:59.673.358 [mindspore/ccsrc/distributed/cluster/cluster_context.cc:260] PostProcess] This node 0 rank id: 0
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:49:24.417.746 [mindspore/ccsrc/distributed/collective/collective_manager.cc:259] CreateCommunicationGroup] Start to create communication group: hccl_world_group [const vector]{0, 1, 2, 3}
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:49:24.420.348 [mindspore/ccsrc/distributed/collective/collective_manager.cc:335] CreateCommunicationGroup] Begin initialize communication group on the device side: hccl_world_group
[WARNING] DISTRIBUTED(73857,ffff9337e010,python):2024-07-06-20:49:25.909.436 [mindspore/ccsrc/distributed/collective/collective_manager.cc:345] CreateCommunicationGroup] End initialize communication group on the device side: hccl_world_group
2024-07-06 20:49:25,912 - mindformers[mindformers/tools/utils.py:168] - INFO - set strategy path to './output/strategy/ckpt_strategy_rank_0.ckpt'
2024-07-06 20:49:25,945 - mindformers[mindformers/trainer/trainer.py:919] - INFO - Load configs in /home/ma-user/work/mindformers/configs/gpt2/run_gpt2.yaml to build trainer.
2024-07-06 20:49:25,946 - mindformers[mindformers/trainer/trainer.py:949] - INFO - ..........Init Config..........
2024-07-06 20:49:25,946 - mindformers[mindformers/core/parallel_config.py:45] - INFO - initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
2024-07-06 20:49:25,946 - mindformers[mindformers/core/parallel_config.py:51] - INFO - initial parallel_config from dict: {'data_parallel': 1, 'model_parallel': 4, 'pipeline_stage': 1, 'use_seq_parallel': False, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
2024-07-06 20:49:25,947 - mindformers[mindformers/tools/utils.py:153] - INFO - set output path to '/home/ma-user/work/mindformers/research/output'
2024-07-06 20:49:25,947 - mindformers[mindformers/trainer/base_trainer.py:85] - INFO - Now Running Task is: text_generation, Model is: llama3_8b
2024-07-06 20:49:25,948 - mindformers[mindformers/trainer/base_trainer.py:111] - WARNING - Input model name is not in the supported list or unspecified.
2024-07-06 20:49:25,948 - mindformers[mindformers/trainer/base_trainer.py:112] - WARNING - See the list of supported task and model name: ['baichuan2_13b', 'baichuan2_7b', 'baichuan_7b', 'bloom_176b', 'bloom_560m', 'bloom_65b', 'bloom_7.1b', 'codegeex2_6b', 'codellama_34b', 'common', 'deepseek_33b', 'glm2_6b', 'glm2_6b_lora', 'glm2_6b_ptuning2', 'glm3_6b', 'glm_6b', 'glm_6b_chat', 'glm_6b_lora', 'glm_6b_lora_chat', 'gpt2', 'gpt2_13b', 'gpt2_52b', 'gpt2_lora', 'gpt2_xl', 'gpt2_xl_lora', 'internlm_7b', 'internlm_7b_lora', 'llama2_13b', 'llama2_70b', 'llama2_7b', 'llama_13b', 'llama_65b', 'llama_7b', 'llama_7b_lora', 'pangualpha_13b', 'pangualpha_2_6b', 'qwen_7b', 'qwen_7b_lora', 'skywork_13b', 'yi_34b', 'yi_6b', 'ziya_13b']
2024-07-06 20:49:25,948 - mindformers[mindformers/trainer/base_trainer.py:113] - WARNING - The default model config: /home/ma-user/work/mindformers/configs/gpt2/run_gpt2.yaml will now be used for the text_generation task 
2024-07-06 20:49:25,948 - mindformers[mindformers/trainer/trainer.py:1004] - INFO - ..........Init Model..........
2024-07-06 20:49:25,948 - mindformers[mindformers/trainer/trainer.py:1027] - INFO - ..........Init Train Dataset..........
2024-07-06 20:49:25,949 - mindformers[mindformers/trainer/trainer.py:335] - INFO - ==========Trainer Init Success!==========
2024-07-06 20:49:25,949 - mindformers[mindformers/trainer/trainer.py:476] - WARNING - The `finetune_checkpoint` will be deprecated. Please use `resume_from_checkpoint` instead.
2024-07-06 20:49:25,949 - mindformers[mindformers/trainer/trainer.py:1004] - INFO - ..........Init Model..........
2024-07-06 20:49:25,949 - mindformers[mindformers/trainer/base_trainer.py:176] - INFO - The current parallel mode is semi_auto_parallel, full batch is True,so global batch size will be changed: global_batch_size = batch_size * data_parallel * micro_batch_interleave_num * gradient_accumulation_steps = 64 = 64 * 1 * 1 * 1
2024-07-06 20:49:25,950 - mindformers[mindformers/trainer/base_trainer.py:624] - INFO - .........Build Dataset For Train..........
2024-07-06 20:49:25,950 - mindformers[mindformers/trainer/base_trainer.py:353] - INFO - .........Build Dataset From Config..........
2024-07-06 20:49:25,950 - mindformers[mindformers/dataset/causal_language_model_dataset.py:166] - INFO - Now Create Causal Language Model Dataset.
2024-07-06 20:49:25,956 - mindformers[mindformers/trainer/base_trainer.py:626] - INFO - Create train dataset finish, dataset size:234
2024-07-06 20:49:25,957 - mindformers[mindformers/trainer/utils.py:171] - INFO - Will be Training epochs:5, sink_size:2
2024-07-06 20:49:25,957 - mindformers[mindformers/trainer/utils.py:173] - INFO - Create training dataset finish, dataset size:234
2024-07-06 20:49:25,957 - mindformers[mindformers/trainer/base_trainer.py:656] - INFO - .........Build Net For Train..........
2024-07-06 20:49:25,958 - mindformers[mindformers/trainer/base_trainer.py:387] - INFO - .........Build Network From Config..........
2024-07-06 20:49:25,958 - mindformers[mindformers/version_control.py:61] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
2024-07-06 20:49:25,959 - mindformers[mindformers/version_control.py:65] - INFO - 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
2024-07-06 20:49:25,959 - mindformers[mindformers/version_control.py:74] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:49:25.960.960 [/home/ma-user/work/mindformers/mindformers/modules/transformer/op_parallel_config.py:244] The optimizer shard True in auto_parallel_context is not equal to the optimizer_shard None in the OpParallelConfig. Please check the optimizer_shard to make them consistent.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:49:25.963.555 [/home/ma-user/work/mindformers/mindformers/modules/transformer/op_parallel_config.py:244] The optimizer shard True in auto_parallel_context is not equal to the optimizer_shard None in the OpParallelConfig. Please check the optimizer_shard to make them consistent.
2024-07-06 20:49:26,069 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:49:26.712.36 [mindspore/common/_decorator.py:40] 'Parameter' is deprecated from version 2.3 and will be removed in a future version, use 'add_pipeline_stage' instead.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:49:26.713.68 [mindspore/common/parameter.py:806] This interface may be deleted in the future.
2024-07-06 20:49:26,091 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,111 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,132 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,153 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,173 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,194 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,214 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,241 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,264 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 20:49:26,730 - mindformers[mindformers/models/modeling_utils.py:1438] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
2024-07-06 20:49:27,022 - mindformers[mindformers/models/modeling_utils.py:1438] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[INFO] 2024-07-06 20:49:27,024 [73857] [SDK] : Start to freeze model for delta, mode: lora, include list: None, exclude list: None
[INFO] 2024-07-06 20:49:27,025 [73857] [SDK] : Start to freeze model, include list: ['*'], exclude list: ['*mindpet_delta_lora*']
[INFO] 2024-07-06 20:49:27,031 [73857] [SDK] : End to freeze model.
[INFO] 2024-07-06 20:49:27,032 [73857] [SDK] : End to freeze model for delta.
2024-07-06 20:49:27,047 - mindformers[mindformers/trainer/base_trainer.py:543] - INFO - Network Parameters: 4718592.
2024-07-06 20:49:27,048 - mindformers[mindformers/trainer/base_trainer.py:678] - INFO - .........Build Optimizer For Train..........
2024-07-06 20:49:27,048 - mindformers[mindformers/trainer/base_trainer.py:426] - INFO - .........Build Optimizer From Config..........
2024-07-06 20:49:27,048 - mindformers[mindformers/trainer/base_trainer.py:459] - INFO - .........Build LR Schedule From Config..........
2024-07-06 20:49:27,054 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:74] - WARNING - dynamic_lr_schedule will be reset and invalid when layer_scale is False.
2024-07-06 20:49:27,058 - mindformers[mindformers/trainer/optimizer_grouped_parameters.py:113] - INFO - Param groups = {
  "decay": {
    "weight_decay": 0.0,
    "params": [
      "model.layers.0.attention.wq.mindpet_delta_lora_a",
      "model.layers.0.attention.wq.mindpet_delta_lora_b",
      "model.layers.0.attention.wk.mindpet_delta_lora_a",
      "model.layers.0.attention.wk.mindpet_delta_lora_b",
      "model.layers.0.attention.wv.mindpet_delta_lora_a",
      "model.layers.0.attention.wv.mindpet_delta_lora_b",
      "model.layers.1.attention.wq.mindpet_delta_lora_a",
      "model.layers.1.attention.wq.mindpet_delta_lora_b",
      "model.layers.1.attention.wk.mindpet_delta_lora_a",
      "model.layers.1.attention.wk.mindpet_delta_lora_b",
      "model.layers.1.attention.wv.mindpet_delta_lora_a",
      "model.layers.1.attention.wv.mindpet_delta_lora_b",
      "model.layers.2.attention.wq.mindpet_delta_lora_a",
      "model.layers.2.attention.wq.mindpet_delta_lora_b",
      "model.layers.2.attention.wk.mindpet_delta_lora_a",
      "model.layers.2.attention.wk.mindpet_delta_lora_b",
      "model.layers.2.attention.wv.mindpet_delta_lora_a",
      "model.layers.2.attention.wv.mindpet_delta_lora_b",
      "model.layers.3.attention.wq.mindpet_delta_lora_a",
      "model.layers.3.attention.wq.mindpet_delta_lora_b",
      "model.layers.3.attention.wk.mindpet_delta_lora_a",
      "model.layers.3.attention.wk.mindpet_delta_lora_b",
      "model.layers.3.attention.wv.mindpet_delta_lora_a",
      "model.layers.3.attention.wv.mindpet_delta_lora_b",
      "model.layers.4.attention.wq.mindpet_delta_lora_a",
      "model.layers.4.attention.wq.mindpet_delta_lora_b",
      "model.layers.4.attention.wk.mindpet_delta_lora_a",
      "model.layers.4.attention.wk.mindpet_delta_lora_b",
      "model.layers.4.attention.wv.mindpet_delta_lora_a",
      "model.layers.4.attention.wv.mindpet_delta_lora_b",
      "model.layers.5.attention.wq.mindpet_delta_lora_a",
      "model.layers.5.attention.wq.mindpet_delta_lora_b",
      "model.layers.5.attention.wk.mindpet_delta_lora_a",
      "model.layers.5.attention.wk.mindpet_delta_lora_b",
      "model.layers.5.attention.wv.mindpet_delta_lora_a",
      "model.layers.5.attention.wv.mindpet_delta_lora_b",
      "model.layers.6.attention.wq.mindpet_delta_lora_a",
      "model.layers.6.attention.wq.mindpet_delta_lora_b",
      "model.layers.6.attention.wk.mindpet_delta_lora_a",
      "model.layers.6.attention.wk.mindpet_delta_lora_b",
      "model.layers.6.attention.wv.mindpet_delta_lora_a",
      "model.layers.6.attention.wv.mindpet_delta_lora_b",
      "model.layers.7.attention.wq.mindpet_delta_lora_a",
      "model.layers.7.attention.wq.mindpet_delta_lora_b",
      "model.layers.7.attention.wk.mindpet_delta_lora_a",
      "model.layers.7.attention.wk.mindpet_delta_lora_b",
      "model.layers.7.attention.wv.mindpet_delta_lora_a",
      "model.layers.7.attention.wv.mindpet_delta_lora_b",
      "model.layers.8.attention.wq.mindpet_delta_lora_a",
      "model.layers.8.attention.wq.mindpet_delta_lora_b",
      "model.layers.8.attention.wk.mindpet_delta_lora_a",
      "model.layers.8.attention.wk.mindpet_delta_lora_b",
      "model.layers.8.attention.wv.mindpet_delta_lora_a",
      "model.layers.8.attention.wv.mindpet_delta_lora_b",
      "model.layers.9.attention.wq.mindpet_delta_lora_a",
      "model.layers.9.attention.wq.mindpet_delta_lora_b",
      "model.layers.9.attention.wk.mindpet_delta_lora_a",
      "model.layers.9.attention.wk.mindpet_delta_lora_b",
      "model.layers.9.attention.wv.mindpet_delta_lora_a",
      "model.layers.9.attention.wv.mindpet_delta_lora_b",
      "model.layers.10.attention.wq.mindpet_delta_lora_a",
      "model.layers.10.attention.wq.mindpet_delta_lora_b",
      "model.layers.10.attention.wk.mindpet_delta_lora_a",
      "model.layers.10.attention.wk.mindpet_delta_lora_b",
      "model.layers.10.attention.wv.mindpet_delta_lora_a",
      "model.layers.10.attention.wv.mindpet_delta_lora_b",
      "model.layers.11.attention.wq.mindpet_delta_lora_a",
      "model.layers.11.attention.wq.mindpet_delta_lora_b",
      "model.layers.11.attention.wk.mindpet_delta_lora_a",
      "model.layers.11.attention.wk.mindpet_delta_lora_b",
      "model.layers.11.attention.wv.mindpet_delta_lora_a",
      "model.layers.11.attention.wv.mindpet_delta_lora_b",
      "model.layers.12.attention.wq.mindpet_delta_lora_a",
      "model.layers.12.attention.wq.mindpet_delta_lora_b",
      "model.layers.12.attention.wk.mindpet_delta_lora_a",
      "model.layers.12.attention.wk.mindpet_delta_lora_b",
      "model.layers.12.attention.wv.mindpet_delta_lora_a",
      "model.layers.12.attention.wv.mindpet_delta_lora_b",
      "model.layers.13.attention.wq.mindpet_delta_lora_a",
      "model.layers.13.attention.wq.mindpet_delta_lora_b",
      "model.layers.13.attention.wk.mindpet_delta_lora_a",
      "model.layers.13.attention.wk.mindpet_delta_lora_b",
      "model.layers.13.attention.wv.mindpet_delta_lora_a",
      "model.layers.13.attention.wv.mindpet_delta_lora_b",
      "model.layers.14.attention.wq.mindpet_delta_lora_a",
      "model.layers.14.attention.wq.mindpet_delta_lora_b",
      "model.layers.14.attention.wk.mindpet_delta_lora_a",
      "model.layers.14.attention.wk.mindpet_delta_lora_b",
      "model.layers.14.attention.wv.mindpet_delta_lora_a",
      "model.layers.14.attention.wv.mindpet_delta_lora_b",
      "model.layers.15.attention.wq.mindpet_delta_lora_a",
      "model.layers.15.attention.wq.mindpet_delta_lora_b",
      "model.layers.15.attention.wk.mindpet_delta_lora_a",
      "model.layers.15.attention.wk.mindpet_delta_lora_b",
      "model.layers.15.attention.wv.mindpet_delta_lora_a",
      "model.layers.15.attention.wv.mindpet_delta_lora_b",
      "model.layers.16.attention.wq.mindpet_delta_lora_a",
      "model.layers.16.attention.wq.mindpet_delta_lora_b",
      "model.layers.16.attention.wk.mindpet_delta_lora_a",
      "model.layers.16.attention.wk.mindpet_delta_lora_b",
      "model.layers.16.attention.wv.mindpet_delta_lora_a",
      "model.layers.16.attention.wv.mindpet_delta_lora_b",
      "model.layers.17.attention.wq.mindpet_delta_lora_a",
      "model.layers.17.attention.wq.mindpet_delta_lora_b",
      "model.layers.17.attention.wk.mindpet_delta_lora_a",
      "model.layers.17.attention.wk.mindpet_delta_lora_b",
      "model.layers.17.attention.wv.mindpet_delta_lora_a",
      "model.layers.17.attention.wv.mindpet_delta_lora_b",
      "model.layers.18.attention.wq.mindpet_delta_lora_a",
      "model.layers.18.attention.wq.mindpet_delta_lora_b",
      "model.layers.18.attention.wk.mindpet_delta_lora_a",
      "model.layers.18.attention.wk.mindpet_delta_lora_b",
      "model.layers.18.attention.wv.mindpet_delta_lora_a",
      "model.layers.18.attention.wv.mindpet_delta_lora_b",
      "model.layers.19.attention.wq.mindpet_delta_lora_a",
      "model.layers.19.attention.wq.mindpet_delta_lora_b",
      "model.layers.19.attention.wk.mindpet_delta_lora_a",
      "model.layers.19.attention.wk.mindpet_delta_lora_b",
      "model.layers.19.attention.wv.mindpet_delta_lora_a",
      "model.layers.19.attention.wv.mindpet_delta_lora_b",
      "model.layers.20.attention.wq.mindpet_delta_lora_a",
      "model.layers.20.attention.wq.mindpet_delta_lora_b",
      "model.layers.20.attention.wk.mindpet_delta_lora_a",
      "model.layers.20.attention.wk.mindpet_delta_lora_b",
      "model.layers.20.attention.wv.mindpet_delta_lora_a",
      "model.layers.20.attention.wv.mindpet_delta_lora_b",
      "model.layers.21.attention.wq.mindpet_delta_lora_a",
      "model.layers.21.attention.wq.mindpet_delta_lora_b",
      "model.layers.21.attention.wk.mindpet_delta_lora_a",
      "model.layers.21.attention.wk.mindpet_delta_lora_b",
      "model.layers.21.attention.wv.mindpet_delta_lora_a",
      "model.layers.21.attention.wv.mindpet_delta_lora_b",
      "model.layers.22.attention.wq.mindpet_delta_lora_a",
      "model.layers.22.attention.wq.mindpet_delta_lora_b",
      "model.layers.22.attention.wk.mindpet_delta_lora_a",
      "model.layers.22.attention.wk.mindpet_delta_lora_b",
      "model.layers.22.attention.wv.mindpet_delta_lora_a",
      "model.layers.22.attention.wv.mindpet_delta_lora_b",
      "model.layers.23.attention.wq.mindpet_delta_lora_a",
      "model.layers.23.attention.wq.mindpet_delta_lora_b",
      "model.layers.23.attention.wk.mindpet_delta_lora_a",
      "model.layers.23.attention.wk.mindpet_delta_lora_b",
      "model.layers.23.attention.wv.mindpet_delta_lora_a",
      "model.layers.23.attention.wv.mindpet_delta_lora_b",
      "model.layers.24.attention.wq.mindpet_delta_lora_a",
      "model.layers.24.attention.wq.mindpet_delta_lora_b",
      "model.layers.24.attention.wk.mindpet_delta_lora_a",
      "model.layers.24.attention.wk.mindpet_delta_lora_b",
      "model.layers.24.attention.wv.mindpet_delta_lora_a",
      "model.layers.24.attention.wv.mindpet_delta_lora_b",
      "model.layers.25.attention.wq.mindpet_delta_lora_a",
      "model.layers.25.attention.wq.mindpet_delta_lora_b",
      "model.layers.25.attention.wk.mindpet_delta_lora_a",
      "model.layers.25.attention.wk.mindpet_delta_lora_b",
      "model.layers.25.attention.wv.mindpet_delta_lora_a",
      "model.layers.25.attention.wv.mindpet_delta_lora_b",
      "model.layers.26.attention.wq.mindpet_delta_lora_a",
      "model.layers.26.attention.wq.mindpet_delta_lora_b",
      "model.layers.26.attention.wk.mindpet_delta_lora_a",
      "model.layers.26.attention.wk.mindpet_delta_lora_b",
      "model.layers.26.attention.wv.mindpet_delta_lora_a",
      "model.layers.26.attention.wv.mindpet_delta_lora_b",
      "model.layers.27.attention.wq.mindpet_delta_lora_a",
      "model.layers.27.attention.wq.mindpet_delta_lora_b",
      "model.layers.27.attention.wk.mindpet_delta_lora_a",
      "model.layers.27.attention.wk.mindpet_delta_lora_b",
      "model.layers.27.attention.wv.mindpet_delta_lora_a",
      "model.layers.27.attention.wv.mindpet_delta_lora_b",
      "model.layers.28.attention.wq.mindpet_delta_lora_a",
      "model.layers.28.attention.wq.mindpet_delta_lora_b",
      "model.layers.28.attention.wk.mindpet_delta_lora_a",
      "model.layers.28.attention.wk.mindpet_delta_lora_b",
      "model.layers.28.attention.wv.mindpet_delta_lora_a",
      "model.layers.28.attention.wv.mindpet_delta_lora_b",
      "model.layers.29.attention.wq.mindpet_delta_lora_a",
      "model.layers.29.attention.wq.mindpet_delta_lora_b",
      "model.layers.29.attention.wk.mindpet_delta_lora_a",
      "model.layers.29.attention.wk.mindpet_delta_lora_b",
      "model.layers.29.attention.wv.mindpet_delta_lora_a",
      "model.layers.29.attention.wv.mindpet_delta_lora_b",
      "model.layers.30.attention.wq.mindpet_delta_lora_a",
      "model.layers.30.attention.wq.mindpet_delta_lora_b",
      "model.layers.30.attention.wk.mindpet_delta_lora_a",
      "model.layers.30.attention.wk.mindpet_delta_lora_b",
      "model.layers.30.attention.wv.mindpet_delta_lora_a",
      "model.layers.30.attention.wv.mindpet_delta_lora_b",
      "model.layers.31.attention.wq.mindpet_delta_lora_a",
      "model.layers.31.attention.wq.mindpet_delta_lora_b",
      "model.layers.31.attention.wk.mindpet_delta_lora_a",
      "model.layers.31.attention.wk.mindpet_delta_lora_b",
      "model.layers.31.attention.wv.mindpet_delta_lora_a",
      "model.layers.31.attention.wv.mindpet_delta_lora_b"
    ]
  }
}
2024-07-06 20:49:27,206 - mindformers[mindformers/trainer/base_trainer.py:683] - INFO - .........Build Running Wrapper From Config For Train..........
2024-07-06 20:49:27,206 - mindformers[mindformers/trainer/base_trainer.py:496] - INFO - .........Build Model Wrapper for Train From Config..........
2024-07-06 20:49:27,214 - mindformers[mindformers/trainer/base_trainer.py:687] - INFO - .........Build Callbacks For Train..........
2024-07-06 20:49:27,215 - mindformers[mindformers/core/callback/callback.py:533] - INFO - Integrated_save is changed to False when using auto_parallel.
2024-07-06 20:49:27,216 - mindformers[mindformers/trainer/base_trainer.py:721] - INFO - .........Starting Init Train Model..........
2024-07-06 20:49:27,217 - mindformers[mindformers/trainer/utils.py:736] - INFO - ............Start load checkpoint from checkpoint............
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:16.755.427 [mindspore/train/serialization.py:1369] model.tok_embeddings.embedding_weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:21.845.198 [mindspore/train/serialization.py:1369] model.layers.0.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:21.845.922 [mindspore/train/serialization.py:1369] model.layers.0.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:21.846.409 [mindspore/train/serialization.py:1369] model.layers.0.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:21.984.652 [mindspore/train/serialization.py:1369] model.layers.0.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:22.241.92 [mindspore/train/serialization.py:1369] model.layers.0.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:22.622.62 [mindspore/train/serialization.py:1369] model.layers.0.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:22.204.328 [mindspore/train/serialization.py:1369] model.layers.0.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:22.780.468 [mindspore/train/serialization.py:1369] model.layers.0.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:23.346.538 [mindspore/train/serialization.py:1369] model.layers.0.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:23.914.715 [mindspore/train/serialization.py:1369] model.layers.1.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:23.915.565 [mindspore/train/serialization.py:1369] model.layers.1.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:23.915.987 [mindspore/train/serialization.py:1369] model.layers.1.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:24.593.44 [mindspore/train/serialization.py:1369] model.layers.1.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:24.100.823 [mindspore/train/serialization.py:1369] model.layers.1.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:24.139.785 [mindspore/train/serialization.py:1369] model.layers.1.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:24.284.119 [mindspore/train/serialization.py:1369] model.layers.1.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:24.847.447 [mindspore/train/serialization.py:1369] model.layers.1.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:25.415.934 [mindspore/train/serialization.py:1369] model.layers.1.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:25.985.519 [mindspore/train/serialization.py:1369] model.layers.2.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:25.986.384 [mindspore/train/serialization.py:1369] model.layers.2.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:25.986.823 [mindspore/train/serialization.py:1369] model.layers.2.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:26.133.061 [mindspore/train/serialization.py:1369] model.layers.2.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:26.171.871 [mindspore/train/serialization.py:1369] model.layers.2.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:26.208.608 [mindspore/train/serialization.py:1369] model.layers.2.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:26.355.855 [mindspore/train/serialization.py:1369] model.layers.2.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:26.936.211 [mindspore/train/serialization.py:1369] model.layers.2.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:27.501.555 [mindspore/train/serialization.py:1369] model.layers.2.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:28.693.88 [mindspore/train/serialization.py:1369] model.layers.3.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:28.702.54 [mindspore/train/serialization.py:1369] model.layers.3.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:28.706.83 [mindspore/train/serialization.py:1369] model.layers.3.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:28.228.198 [mindspore/train/serialization.py:1369] model.layers.3.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:28.267.380 [mindspore/train/serialization.py:1369] model.layers.3.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:28.305.236 [mindspore/train/serialization.py:1369] model.layers.3.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:28.470.560 [mindspore/train/serialization.py:1369] model.layers.3.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:29.488.20 [mindspore/train/serialization.py:1369] model.layers.3.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:29.620.170 [mindspore/train/serialization.py:1369] model.layers.3.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:30.177.372 [mindspore/train/serialization.py:1369] model.layers.4.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:30.178.248 [mindspore/train/serialization.py:1369] model.layers.4.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:30.178.666 [mindspore/train/serialization.py:1369] model.layers.4.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:30.345.985 [mindspore/train/serialization.py:1369] model.layers.4.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:30.385.779 [mindspore/train/serialization.py:1369] model.layers.4.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:30.425.041 [mindspore/train/serialization.py:1369] model.layers.4.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:30.592.457 [mindspore/train/serialization.py:1369] model.layers.4.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:31.170.801 [mindspore/train/serialization.py:1369] model.layers.4.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:31.870.628 [mindspore/train/serialization.py:1369] model.layers.4.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:32.450.828 [mindspore/train/serialization.py:1369] model.layers.5.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:32.451.690 [mindspore/train/serialization.py:1369] model.layers.5.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:32.452.166 [mindspore/train/serialization.py:1369] model.layers.5.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:32.619.100 [mindspore/train/serialization.py:1369] model.layers.5.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:32.657.027 [mindspore/train/serialization.py:1369] model.layers.5.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:32.697.955 [mindspore/train/serialization.py:1369] model.layers.5.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:32.884.047 [mindspore/train/serialization.py:1369] model.layers.5.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:33.458.414 [mindspore/train/serialization.py:1369] model.layers.5.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:34.359.23 [mindspore/train/serialization.py:1369] model.layers.5.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:34.645.403 [mindspore/train/serialization.py:1369] model.layers.6.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:34.646.245 [mindspore/train/serialization.py:1369] model.layers.6.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:34.646.666 [mindspore/train/serialization.py:1369] model.layers.6.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:34.812.293 [mindspore/train/serialization.py:1369] model.layers.6.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:34.853.975 [mindspore/train/serialization.py:1369] model.layers.6.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:34.894.349 [mindspore/train/serialization.py:1369] model.layers.6.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:35.679.09 [mindspore/train/serialization.py:1369] model.layers.6.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:35.666.786 [mindspore/train/serialization.py:1369] model.layers.6.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:36.271.217 [mindspore/train/serialization.py:1369] model.layers.6.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:36.861.849 [mindspore/train/serialization.py:1369] model.layers.7.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:36.862.692 [mindspore/train/serialization.py:1369] model.layers.7.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:36.863.115 [mindspore/train/serialization.py:1369] model.layers.7.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:37.298.21 [mindspore/train/serialization.py:1369] model.layers.7.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:37.745.04 [mindspore/train/serialization.py:1369] model.layers.7.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:37.120.593 [mindspore/train/serialization.py:1369] model.layers.7.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:37.294.968 [mindspore/train/serialization.py:1369] model.layers.7.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:37.855.985 [mindspore/train/serialization.py:1369] model.layers.7.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:38.427.149 [mindspore/train/serialization.py:1369] model.layers.7.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:38.996.761 [mindspore/train/serialization.py:1369] model.layers.8.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:38.997.616 [mindspore/train/serialization.py:1369] model.layers.8.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:38.998.033 [mindspore/train/serialization.py:1369] model.layers.8.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:39.165.963 [mindspore/train/serialization.py:1369] model.layers.8.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:39.214.655 [mindspore/train/serialization.py:1369] model.layers.8.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:39.264.423 [mindspore/train/serialization.py:1369] model.layers.8.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:39.431.107 [mindspore/train/serialization.py:1369] model.layers.8.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:40.927. [mindspore/train/serialization.py:1369] model.layers.8.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:40.578.839 [mindspore/train/serialization.py:1369] model.layers.8.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:41.150.382 [mindspore/train/serialization.py:1369] model.layers.9.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:41.151.180 [mindspore/train/serialization.py:1369] model.layers.9.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:41.151.599 [mindspore/train/serialization.py:1369] model.layers.9.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:41.319.455 [mindspore/train/serialization.py:1369] model.layers.9.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:41.379.379 [mindspore/train/serialization.py:1369] model.layers.9.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:41.430.773 [mindspore/train/serialization.py:1369] model.layers.9.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:41.595.100 [mindspore/train/serialization.py:1369] model.layers.9.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:42.160.846 [mindspore/train/serialization.py:1369] model.layers.9.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:42.729.218 [mindspore/train/serialization.py:1369] model.layers.9.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:43.291.991 [mindspore/train/serialization.py:1369] model.layers.10.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:43.292.659 [mindspore/train/serialization.py:1369] model.layers.10.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:43.293.066 [mindspore/train/serialization.py:1369] model.layers.10.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:43.462.791 [mindspore/train/serialization.py:1369] model.layers.10.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:43.517.022 [mindspore/train/serialization.py:1369] model.layers.10.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:43.564.328 [mindspore/train/serialization.py:1369] model.layers.10.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:43.729.364 [mindspore/train/serialization.py:1369] model.layers.10.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:44.292.069 [mindspore/train/serialization.py:1369] model.layers.10.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:44.874.755 [mindspore/train/serialization.py:1369] model.layers.10.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:45.446.144 [mindspore/train/serialization.py:1369] model.layers.11.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:45.447.002 [mindspore/train/serialization.py:1369] model.layers.11.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:45.447.441 [mindspore/train/serialization.py:1369] model.layers.11.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:45.621.544 [mindspore/train/serialization.py:1369] model.layers.11.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:45.677.770 [mindspore/train/serialization.py:1369] model.layers.11.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:45.722.059 [mindspore/train/serialization.py:1369] model.layers.11.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:45.893.582 [mindspore/train/serialization.py:1369] model.layers.11.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:46.460.510 [mindspore/train/serialization.py:1369] model.layers.11.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:47.306.39 [mindspore/train/serialization.py:1369] model.layers.11.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:47.604.009 [mindspore/train/serialization.py:1369] model.layers.12.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:47.604.825 [mindspore/train/serialization.py:1369] model.layers.12.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:47.605.250 [mindspore/train/serialization.py:1369] model.layers.12.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:47.777.638 [mindspore/train/serialization.py:1369] model.layers.12.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:47.836.002 [mindspore/train/serialization.py:1369] model.layers.12.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:47.885.410 [mindspore/train/serialization.py:1369] model.layers.12.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:48.557.51 [mindspore/train/serialization.py:1369] model.layers.12.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:48.631.023 [mindspore/train/serialization.py:1369] model.layers.12.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:49.199.765 [mindspore/train/serialization.py:1369] model.layers.12.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:49.770.102 [mindspore/train/serialization.py:1369] model.layers.13.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:49.770.936 [mindspore/train/serialization.py:1369] model.layers.13.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:49.771.355 [mindspore/train/serialization.py:1369] model.layers.13.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:49.940.580 [mindspore/train/serialization.py:1369] model.layers.13.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:49.996.541 [mindspore/train/serialization.py:1369] model.layers.13.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:50.479.82 [mindspore/train/serialization.py:1369] model.layers.13.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:50.226.403 [mindspore/train/serialization.py:1369] model.layers.13.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:50.796.620 [mindspore/train/serialization.py:1369] model.layers.13.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:51.377.183 [mindspore/train/serialization.py:1369] model.layers.13.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:51.949.664 [mindspore/train/serialization.py:1369] model.layers.14.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:51.950.595 [mindspore/train/serialization.py:1369] model.layers.14.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:51.951.041 [mindspore/train/serialization.py:1369] model.layers.14.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:52.123.085 [mindspore/train/serialization.py:1369] model.layers.14.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:52.173.167 [mindspore/train/serialization.py:1369] model.layers.14.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:52.225.743 [mindspore/train/serialization.py:1369] model.layers.14.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:52.417.232 [mindspore/train/serialization.py:1369] model.layers.14.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:53.107.2 [mindspore/train/serialization.py:1369] model.layers.14.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:53.577.992 [mindspore/train/serialization.py:1369] model.layers.14.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:54.160.448 [mindspore/train/serialization.py:1369] model.layers.15.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:54.161.279 [mindspore/train/serialization.py:1369] model.layers.15.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:54.161.738 [mindspore/train/serialization.py:1369] model.layers.15.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:54.337.384 [mindspore/train/serialization.py:1369] model.layers.15.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:54.387.934 [mindspore/train/serialization.py:1369] model.layers.15.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:54.432.003 [mindspore/train/serialization.py:1369] model.layers.15.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:54.617.000 [mindspore/train/serialization.py:1369] model.layers.15.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:55.202.137 [mindspore/train/serialization.py:1369] model.layers.15.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:55.783.851 [mindspore/train/serialization.py:1369] model.layers.15.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:56.353.105 [mindspore/train/serialization.py:1369] model.layers.16.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:56.353.915 [mindspore/train/serialization.py:1369] model.layers.16.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:56.354.367 [mindspore/train/serialization.py:1369] model.layers.16.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:56.527.185 [mindspore/train/serialization.py:1369] model.layers.16.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:56.578.444 [mindspore/train/serialization.py:1369] model.layers.16.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:56.626.517 [mindspore/train/serialization.py:1369] model.layers.16.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:56.798.477 [mindspore/train/serialization.py:1369] model.layers.16.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:57.371.587 [mindspore/train/serialization.py:1369] model.layers.16.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:57.938.109 [mindspore/train/serialization.py:1369] model.layers.16.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:58.508.533 [mindspore/train/serialization.py:1369] model.layers.17.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:58.509.388 [mindspore/train/serialization.py:1369] model.layers.17.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:58.509.834 [mindspore/train/serialization.py:1369] model.layers.17.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:58.681.568 [mindspore/train/serialization.py:1369] model.layers.17.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:58.735.322 [mindspore/train/serialization.py:1369] model.layers.17.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:58.785.627 [mindspore/train/serialization.py:1369] model.layers.17.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:58.968.585 [mindspore/train/serialization.py:1369] model.layers.17.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:51:59.554.878 [mindspore/train/serialization.py:1369] model.layers.17.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:00.141.983 [mindspore/train/serialization.py:1369] model.layers.17.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:00.721.644 [mindspore/train/serialization.py:1369] model.layers.18.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:00.722.636 [mindspore/train/serialization.py:1369] model.layers.18.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:00.723.111 [mindspore/train/serialization.py:1369] model.layers.18.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:00.897.756 [mindspore/train/serialization.py:1369] model.layers.18.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:00.955.220 [mindspore/train/serialization.py:1369] model.layers.18.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:01.454.4 [mindspore/train/serialization.py:1369] model.layers.18.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:01.174.387 [mindspore/train/serialization.py:1369] model.layers.18.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:01.741.193 [mindspore/train/serialization.py:1369] model.layers.18.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:02.309.159 [mindspore/train/serialization.py:1369] model.layers.18.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:02.883.479 [mindspore/train/serialization.py:1369] model.layers.19.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:02.884.345 [mindspore/train/serialization.py:1369] model.layers.19.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:02.884.776 [mindspore/train/serialization.py:1369] model.layers.19.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:03.603.28 [mindspore/train/serialization.py:1369] model.layers.19.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:03.106.472 [mindspore/train/serialization.py:1369] model.layers.19.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:03.150.248 [mindspore/train/serialization.py:1369] model.layers.19.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:03.328.268 [mindspore/train/serialization.py:1369] model.layers.19.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:03.897.234 [mindspore/train/serialization.py:1369] model.layers.19.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:04.516.102 [mindspore/train/serialization.py:1369] model.layers.19.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:05.112.498 [mindspore/train/serialization.py:1369] model.layers.20.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:05.113.356 [mindspore/train/serialization.py:1369] model.layers.20.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:05.113.781 [mindspore/train/serialization.py:1369] model.layers.20.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:05.290.689 [mindspore/train/serialization.py:1369] model.layers.20.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:05.335.210 [mindspore/train/serialization.py:1369] model.layers.20.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:05.380.647 [mindspore/train/serialization.py:1369] model.layers.20.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:05.550.616 [mindspore/train/serialization.py:1369] model.layers.20.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:06.122.336 [mindspore/train/serialization.py:1369] model.layers.20.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:06.707.840 [mindspore/train/serialization.py:1369] model.layers.20.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:07.278.913 [mindspore/train/serialization.py:1369] model.layers.21.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:07.279.745 [mindspore/train/serialization.py:1369] model.layers.21.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:07.280.169 [mindspore/train/serialization.py:1369] model.layers.21.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:07.450.319 [mindspore/train/serialization.py:1369] model.layers.21.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:07.499.796 [mindspore/train/serialization.py:1369] model.layers.21.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:07.546.031 [mindspore/train/serialization.py:1369] model.layers.21.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:07.714.277 [mindspore/train/serialization.py:1369] model.layers.21.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:08.293.136 [mindspore/train/serialization.py:1369] model.layers.21.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:08.866.940 [mindspore/train/serialization.py:1369] model.layers.21.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:09.445.862 [mindspore/train/serialization.py:1369] model.layers.22.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:09.446.736 [mindspore/train/serialization.py:1369] model.layers.22.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:09.447.160 [mindspore/train/serialization.py:1369] model.layers.22.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:09.626.226 [mindspore/train/serialization.py:1369] model.layers.22.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:09.674.225 [mindspore/train/serialization.py:1369] model.layers.22.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:09.726.570 [mindspore/train/serialization.py:1369] model.layers.22.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:09.900.118 [mindspore/train/serialization.py:1369] model.layers.22.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:10.471.884 [mindspore/train/serialization.py:1369] model.layers.22.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:11.457.02 [mindspore/train/serialization.py:1369] model.layers.22.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:11.610.218 [mindspore/train/serialization.py:1369] model.layers.23.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:11.611.062 [mindspore/train/serialization.py:1369] model.layers.23.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:11.611.492 [mindspore/train/serialization.py:1369] model.layers.23.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:11.777.947 [mindspore/train/serialization.py:1369] model.layers.23.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:11.830.136 [mindspore/train/serialization.py:1369] model.layers.23.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:11.877.545 [mindspore/train/serialization.py:1369] model.layers.23.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:12.155.301 [mindspore/train/serialization.py:1369] model.layers.23.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:12.781.405 [mindspore/train/serialization.py:1369] model.layers.23.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:13.348.580 [mindspore/train/serialization.py:1369] model.layers.23.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:13.910.828 [mindspore/train/serialization.py:1369] model.layers.24.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:13.911.677 [mindspore/train/serialization.py:1369] model.layers.24.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:13.912.102 [mindspore/train/serialization.py:1369] model.layers.24.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:14.813.24 [mindspore/train/serialization.py:1369] model.layers.24.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:14.134.286 [mindspore/train/serialization.py:1369] model.layers.24.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:14.182.824 [mindspore/train/serialization.py:1369] model.layers.24.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:14.365.207 [mindspore/train/serialization.py:1369] model.layers.24.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:14.952.896 [mindspore/train/serialization.py:1369] model.layers.24.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:15.537.585 [mindspore/train/serialization.py:1369] model.layers.24.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:16.115.426 [mindspore/train/serialization.py:1369] model.layers.25.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:16.116.296 [mindspore/train/serialization.py:1369] model.layers.25.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:16.116.727 [mindspore/train/serialization.py:1369] model.layers.25.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:16.291.047 [mindspore/train/serialization.py:1369] model.layers.25.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:16.338.630 [mindspore/train/serialization.py:1369] model.layers.25.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:16.389.145 [mindspore/train/serialization.py:1369] model.layers.25.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:16.562.448 [mindspore/train/serialization.py:1369] model.layers.25.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:17.155.733 [mindspore/train/serialization.py:1369] model.layers.25.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:17.776.452 [mindspore/train/serialization.py:1369] model.layers.25.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:18.361.019 [mindspore/train/serialization.py:1369] model.layers.26.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:18.361.902 [mindspore/train/serialization.py:1369] model.layers.26.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:18.362.354 [mindspore/train/serialization.py:1369] model.layers.26.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:18.534.382 [mindspore/train/serialization.py:1369] model.layers.26.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:18.584.108 [mindspore/train/serialization.py:1369] model.layers.26.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:18.634.387 [mindspore/train/serialization.py:1369] model.layers.26.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:18.810.142 [mindspore/train/serialization.py:1369] model.layers.26.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:19.402.990 [mindspore/train/serialization.py:1369] model.layers.26.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:20.926.0 [mindspore/train/serialization.py:1369] model.layers.26.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:20.670.728 [mindspore/train/serialization.py:1369] model.layers.27.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:20.671.596 [mindspore/train/serialization.py:1369] model.layers.27.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:20.672.030 [mindspore/train/serialization.py:1369] model.layers.27.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:20.852.439 [mindspore/train/serialization.py:1369] model.layers.27.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:20.957.388 [mindspore/train/serialization.py:1369] model.layers.27.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:21.123.21 [mindspore/train/serialization.py:1369] model.layers.27.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:21.202.938 [mindspore/train/serialization.py:1369] model.layers.27.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:21.796.395 [mindspore/train/serialization.py:1369] model.layers.27.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:22.379.622 [mindspore/train/serialization.py:1369] model.layers.27.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:23.145.92 [mindspore/train/serialization.py:1369] model.layers.28.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:23.155.07 [mindspore/train/serialization.py:1369] model.layers.28.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:23.159.80 [mindspore/train/serialization.py:1369] model.layers.28.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:23.252.255 [mindspore/train/serialization.py:1369] model.layers.28.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:23.307.500 [mindspore/train/serialization.py:1369] model.layers.28.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:23.356.750 [mindspore/train/serialization.py:1369] model.layers.28.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:23.545.393 [mindspore/train/serialization.py:1369] model.layers.28.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:24.122.418 [mindspore/train/serialization.py:1369] model.layers.28.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:24.690.875 [mindspore/train/serialization.py:1369] model.layers.28.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:25.289.580 [mindspore/train/serialization.py:1369] model.layers.29.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:25.290.473 [mindspore/train/serialization.py:1369] model.layers.29.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:25.290.932 [mindspore/train/serialization.py:1369] model.layers.29.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:25.481.926 [mindspore/train/serialization.py:1369] model.layers.29.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:25.531.073 [mindspore/train/serialization.py:1369] model.layers.29.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:25.581.832 [mindspore/train/serialization.py:1369] model.layers.29.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:25.760.771 [mindspore/train/serialization.py:1369] model.layers.29.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:26.362.799 [mindspore/train/serialization.py:1369] model.layers.29.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:26.958.779 [mindspore/train/serialization.py:1369] model.layers.29.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:27.552.730 [mindspore/train/serialization.py:1369] model.layers.30.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:27.553.532 [mindspore/train/serialization.py:1369] model.layers.30.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:27.553.963 [mindspore/train/serialization.py:1369] model.layers.30.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:27.736.613 [mindspore/train/serialization.py:1369] model.layers.30.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:27.794.664 [mindspore/train/serialization.py:1369] model.layers.30.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:27.843.598 [mindspore/train/serialization.py:1369] model.layers.30.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:28.464.16 [mindspore/train/serialization.py:1369] model.layers.30.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:28.620.199 [mindspore/train/serialization.py:1369] model.layers.30.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:29.192.412 [mindspore/train/serialization.py:1369] model.layers.30.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:29.790.768 [mindspore/train/serialization.py:1369] model.layers.31.ffn_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:29.791.630 [mindspore/train/serialization.py:1369] model.layers.31.attention_norm.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:29.792.062 [mindspore/train/serialization.py:1369] model.layers.31.attention.wq.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:29.961.987 [mindspore/train/serialization.py:1369] model.layers.31.attention.wk.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:30.119.26 [mindspore/train/serialization.py:1369] model.layers.31.attention.wv.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:30.630.15 [mindspore/train/serialization.py:1369] model.layers.31.attention.wo.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:30.257.384 [mindspore/train/serialization.py:1369] model.layers.31.feed_forward.w1.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:30.894.281 [mindspore/train/serialization.py:1369] model.layers.31.feed_forward.w2.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:31.509.506 [mindspore/train/serialization.py:1369] model.layers.31.feed_forward.w3.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:32.104.076 [mindspore/train/serialization.py:1369] model.norm_out.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:32.104.936 [mindspore/train/serialization.py:1369] lm_head.weight is not init while load ckpt.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:57.170.228 [mindspore/train/serialization.py:195] The type of model.layers.0.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:52:57.174.366 [mindspore/train/serialization.py:195] The type of model.layers.0.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:04.180.797 [mindspore/train/serialization.py:195] The type of model.layers.1.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:04.184.675 [mindspore/train/serialization.py:195] The type of model.layers.1.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:11.146.341 [mindspore/train/serialization.py:195] The type of model.layers.2.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:11.150.245 [mindspore/train/serialization.py:195] The type of model.layers.2.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:18.159.250 [mindspore/train/serialization.py:195] The type of model.layers.3.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:18.163.102 [mindspore/train/serialization.py:195] The type of model.layers.3.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:25.109.463 [mindspore/train/serialization.py:195] The type of model.layers.4.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:25.113.308 [mindspore/train/serialization.py:195] The type of model.layers.4.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:32.102.28 [mindspore/train/serialization.py:195] The type of model.layers.5.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:32.142.34 [mindspore/train/serialization.py:195] The type of model.layers.5.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:39.249.91 [mindspore/train/serialization.py:195] The type of model.layers.6.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:39.288.27 [mindspore/train/serialization.py:195] The type of model.layers.6.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:46.140.09 [mindspore/train/serialization.py:195] The type of model.layers.7.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:46.178.27 [mindspore/train/serialization.py:195] The type of model.layers.7.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:52.991.303 [mindspore/train/serialization.py:195] The type of model.layers.8.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:53:52.995.284 [mindspore/train/serialization.py:195] The type of model.layers.8.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:00.132.90 [mindspore/train/serialization.py:195] The type of model.layers.9.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:00.172.18 [mindspore/train/serialization.py:195] The type of model.layers.9.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:06.961.709 [mindspore/train/serialization.py:195] The type of model.layers.10.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:06.965.659 [mindspore/train/serialization.py:195] The type of model.layers.10.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:13.941.688 [mindspore/train/serialization.py:195] The type of model.layers.11.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:13.945.625 [mindspore/train/serialization.py:195] The type of model.layers.11.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:20.879.405 [mindspore/train/serialization.py:195] The type of model.layers.12.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:20.883.245 [mindspore/train/serialization.py:195] The type of model.layers.12.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:27.802.240 [mindspore/train/serialization.py:195] The type of model.layers.13.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:27.806.095 [mindspore/train/serialization.py:195] The type of model.layers.13.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:34.705.869 [mindspore/train/serialization.py:195] The type of model.layers.14.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:34.709.844 [mindspore/train/serialization.py:195] The type of model.layers.14.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:41.718.778 [mindspore/train/serialization.py:195] The type of model.layers.15.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:41.722.611 [mindspore/train/serialization.py:195] The type of model.layers.15.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:48.642.631 [mindspore/train/serialization.py:195] The type of model.layers.16.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:48.646.523 [mindspore/train/serialization.py:195] The type of model.layers.16.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:55.584.244 [mindspore/train/serialization.py:195] The type of model.layers.17.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:54:55.588.128 [mindspore/train/serialization.py:195] The type of model.layers.17.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:02.537.647 [mindspore/train/serialization.py:195] The type of model.layers.18.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:02.541.645 [mindspore/train/serialization.py:195] The type of model.layers.18.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:09.537.748 [mindspore/train/serialization.py:195] The type of model.layers.19.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:09.541.682 [mindspore/train/serialization.py:195] The type of model.layers.19.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:16.482.146 [mindspore/train/serialization.py:195] The type of model.layers.20.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:16.486.012 [mindspore/train/serialization.py:195] The type of model.layers.20.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:23.520.957 [mindspore/train/serialization.py:195] The type of model.layers.21.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:23.525.015 [mindspore/train/serialization.py:195] The type of model.layers.21.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:30.545.834 [mindspore/train/serialization.py:195] The type of model.layers.22.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:30.549.787 [mindspore/train/serialization.py:195] The type of model.layers.22.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:37.499.002 [mindspore/train/serialization.py:195] The type of model.layers.23.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:37.502.903 [mindspore/train/serialization.py:195] The type of model.layers.23.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:44.470.262 [mindspore/train/serialization.py:195] The type of model.layers.24.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:44.474.110 [mindspore/train/serialization.py:195] The type of model.layers.24.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:51.455.356 [mindspore/train/serialization.py:195] The type of model.layers.25.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:51.459.494 [mindspore/train/serialization.py:195] The type of model.layers.25.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:58.504.902 [mindspore/train/serialization.py:195] The type of model.layers.26.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:55:58.508.889 [mindspore/train/serialization.py:195] The type of model.layers.26.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:05.517.594 [mindspore/train/serialization.py:195] The type of model.layers.27.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:05.521.621 [mindspore/train/serialization.py:195] The type of model.layers.27.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:12.484.277 [mindspore/train/serialization.py:195] The type of model.layers.28.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:12.488.293 [mindspore/train/serialization.py:195] The type of model.layers.28.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:19.478.591 [mindspore/train/serialization.py:195] The type of model.layers.29.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:19.482.687 [mindspore/train/serialization.py:195] The type of model.layers.29.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:26.430.388 [mindspore/train/serialization.py:195] The type of model.layers.30.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:26.434.433 [mindspore/train/serialization.py:195] The type of model.layers.30.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:33.430.024 [mindspore/train/serialization.py:195] The type of model.layers.31.ffn_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:33.434.072 [mindspore/train/serialization.py:195] The type of model.layers.31.attention_norm.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:40.449.666 [mindspore/train/serialization.py:195] The type of model.norm_out.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from BFloat16 to Float32 in the network.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:57.380.440 [mindspore/train/serialization.py:1456] For 'load_param_into_net', 192 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:57.380.971 [mindspore/train/serialization.py:1460] ['model.layers.0.attention.wq.mindpet_delta_lora_a', 'model.layers.0.attention.wq.mindpet_delta_lora_b', 'model.layers.0.attention.wk.mindpet_delta_lora_a', 'model.layers.0.attention.wk.mindpet_delta_lora_b', 'model.layers.0.attention.wv.mindpet_delta_lora_a', 'model.layers.0.attention.wv.mindpet_delta_lora_b', 'model.layers.1.attention.wq.mindpet_delta_lora_a', 'model.layers.1.attention.wq.mindpet_delta_lora_b', 'model.layers.1.attention.wk.mindpet_delta_lora_a', 'model.layers.1.attention.wk.mindpet_delta_lora_b', 'model.layers.1.attention.wv.mindpet_delta_lora_a', 'model.layers.1.attention.wv.mindpet_delta_lora_b', 'model.layers.2.attention.wq.mindpet_delta_lora_a', 'model.layers.2.attention.wq.mindpet_delta_lora_b', 'model.layers.2.attention.wk.mindpet_delta_lora_a', 'model.layers.2.attention.wk.mindpet_delta_lora_b', 'model.layers.2.attention.wv.mindpet_delta_lora_a', 'model.layers.2.attention.wv.mindpet_delta_lora_b', 'model.layers.3.attention.wq.mindpet_delta_lora_a', 'model.layers.3.attention.wq.mindpet_delta_lora_b', 'model.layers.3.attention.wk.mindpet_delta_lora_a', 'model.layers.3.attention.wk.mindpet_delta_lora_b', 'model.layers.3.attention.wv.mindpet_delta_lora_a', 'model.layers.3.attention.wv.mindpet_delta_lora_b', 'model.layers.4.attention.wq.mindpet_delta_lora_a', 'model.layers.4.attention.wq.mindpet_delta_lora_b', 'model.layers.4.attention.wk.mindpet_delta_lora_a', 'model.layers.4.attention.wk.mindpet_delta_lora_b', 'model.layers.4.attention.wv.mindpet_delta_lora_a', 'model.layers.4.attention.wv.mindpet_delta_lora_b', 'model.layers.5.attention.wq.mindpet_delta_lora_a', 'model.layers.5.attention.wq.mindpet_delta_lora_b', 'model.layers.5.attention.wk.mindpet_delta_lora_a', 'model.layers.5.attention.wk.mindpet_delta_lora_b', 'model.layers.5.attention.wv.mindpet_delta_lora_a', 'model.layers.5.attention.wv.mindpet_delta_lora_b', 'model.layers.6.attention.wq.mindpet_delta_lora_a', 'model.layers.6.attention.wq.mindpet_delta_lora_b', 'model.layers.6.attention.wk.mindpet_delta_lora_a', 'model.layers.6.attention.wk.mindpet_delta_lora_b', 'model.layers.6.attention.wv.mindpet_delta_lora_a', 'model.layers.6.attention.wv.mindpet_delta_lora_b', 'model.layers.7.attention.wq.mindpet_delta_lora_a', 'model.layers.7.attention.wq.mindpet_delta_lora_b', 'model.layers.7.attention.wk.mindpet_delta_lora_a', 'model.layers.7.attention.wk.mindpet_delta_lora_b', 'model.layers.7.attention.wv.mindpet_delta_lora_a', 'model.layers.7.attention.wv.mindpet_delta_lora_b', 'model.layers.8.attention.wq.mindpet_delta_lora_a', 'model.layers.8.attention.wq.mindpet_delta_lora_b', 'model.layers.8.attention.wk.mindpet_delta_lora_a', 'model.layers.8.attention.wk.mindpet_delta_lora_b', 'model.layers.8.attention.wv.mindpet_delta_lora_a', 'model.layers.8.attention.wv.mindpet_delta_lora_b', 'model.layers.9.attention.wq.mindpet_delta_lora_a', 'model.layers.9.attention.wq.mindpet_delta_lora_b', 'model.layers.9.attention.wk.mindpet_delta_lora_a', 'model.layers.9.attention.wk.mindpet_delta_lora_b', 'model.layers.9.attention.wv.mindpet_delta_lora_a', 'model.layers.9.attention.wv.mindpet_delta_lora_b', 'model.layers.10.attention.wq.mindpet_delta_lora_a', 'model.layers.10.attention.wq.mindpet_delta_lora_b', 'model.layers.10.attention.wk.mindpet_delta_lora_a', 'model.layers.10.attention.wk.mindpet_delta_lora_b', 'model.layers.10.attention.wv.mindpet_delta_lora_a', 'model.layers.10.attention.wv.mindpet_delta_lora_b', 'model.layers.11.attention.wq.mindpet_delta_lora_a', 'model.layers.11.attention.wq.mindpet_delta_lora_b', 'model.layers.11.attention.wk.mindpet_delta_lora_a', 'model.layers.11.attention.wk.mindpet_delta_lora_b', 'model.layers.11.attention.wv.mindpet_delta_lora_a', 'model.layers.11.attention.wv.mindpet_delta_lora_b', 'model.layers.12.attention.wq.mindpet_delta_lora_a', 'model.layers.12.attention.wq.mindpet_delta_lora_b', 'model.layers.12.attention.wk.mindpet_delta_lora_a', 'model.layers.12.attention.wk.mindpet_delta_lora_b', 'model.layers.12.attention.wv.mindpet_delta_lora_a', 'model.layers.12.attention.wv.mindpet_delta_lora_b', 'model.layers.13.attention.wq.mindpet_delta_lora_a', 'model.layers.13.attention.wq.mindpet_delta_lora_b', 'model.layers.13.attention.wk.mindpet_delta_lora_a', 'model.layers.13.attention.wk.mindpet_delta_lora_b', 'model.layers.13.attention.wv.mindpet_delta_lora_a', 'model.layers.13.attention.wv.mindpet_delta_lora_b', 'model.layers.14.attention.wq.mindpet_delta_lora_a', 'model.layers.14.attention.wq.mindpet_delta_lora_b', 'model.layers.14.attention.wk.mindpet_delta_lora_a', 'model.layers.14.attention.wk.mindpet_delta_lora_b', 'model.layers.14.attention.wv.mindpet_delta_lora_a', 'model.layers.14.attention.wv.mindpet_delta_lora_b', 'model.layers.15.attention.wq.mindpet_delta_lora_a', 'model.layers.15.attention.wq.mindpet_delta_lora_b', 'model.layers.15.attention.wk.mindpet_delta_lora_a', 'model.layers.15.attention.wk.mindpet_delta_lora_b', 'model.layers.15.attention.wv.mindpet_delta_lora_a', 'model.layers.15.attention.wv.mindpet_delta_lora_b', 'model.layers.16.attention.wq.mindpet_delta_lora_a', 'model.layers.16.attention.wq.mindpet_delta_lora_b', 'model.layers.16.attention.wk.mindpet_delta_lora_a', 'model.layers.16.attention.wk.mindpet_delta_lora_b', 'model.layers.16.attention.wv.mindpet_delta_lora_a', 'model.layers.16.attention.wv.mindpet_delta_lora_b', 'model.layers.17.attention.wq.mindpet_delta_lora_a', 'model.layers.17.attention.wq.mindpet_delta_lora_b', 'model.layers.17.attention.wk.mindpet_delta_lora_a', 'model.layers.17.attention.wk.mindpet_delta_lora_b', 'model.layers.17.attention.wv.mindpet_delta_lora_a', 'model.layers.17.attention.wv.mindpet_delta_lora_b', 'model.layers.18.attention.wq.mindpet_delta_lora_a', 'model.layers.18.attention.wq.mindpet_delta_lora_b', 'model.layers.18.attention.wk.mindpet_delta_lora_a', 'model.layers.18.attention.wk.mindpet_delta_lora_b', 'model.layers.18.attention.wv.mindpet_delta_lora_a', 'model.layers.18.attention.wv.mindpet_delta_lora_b', 'model.layers.19.attention.wq.mindpet_delta_lora_a', 'model.layers.19.attention.wq.mindpet_delta_lora_b', 'model.layers.19.attention.wk.mindpet_delta_lora_a', 'model.layers.19.attention.wk.mindpet_delta_lora_b', 'model.layers.19.attention.wv.mindpet_delta_lora_a', 'model.layers.19.attention.wv.mindpet_delta_lora_b', 'model.layers.20.attention.wq.mindpet_delta_lora_a', 'model.layers.20.attention.wq.mindpet_delta_lora_b', 'model.layers.20.attention.wk.mindpet_delta_lora_a', 'model.layers.20.attention.wk.mindpet_delta_lora_b', 'model.layers.20.attention.wv.mindpet_delta_lora_a', 'model.layers.20.attention.wv.mindpet_delta_lora_b', 'model.layers.21.attention.wq.mindpet_delta_lora_a', 'model.layers.21.attention.wq.mindpet_delta_lora_b', 'model.layers.21.attention.wk.mindpet_delta_lora_a', 'model.layers.21.attention.wk.mindpet_delta_lora_b', 'model.layers.21.attention.wv.mindpet_delta_lora_a', 'model.layers.21.attention.wv.mindpet_delta_lora_b', 'model.layers.22.attention.wq.mindpet_delta_lora_a', 'model.layers.22.attention.wq.mindpet_delta_lora_b', 'model.layers.22.attention.wk.mindpet_delta_lora_a', 'model.layers.22.attention.wk.mindpet_delta_lora_b', 'model.layers.22.attention.wv.mindpet_delta_lora_a', 'model.layers.22.attention.wv.mindpet_delta_lora_b', 'model.layers.23.attention.wq.mindpet_delta_lora_a', 'model.layers.23.attention.wq.mindpet_delta_lora_b', 'model.layers.23.attention.wk.mindpet_delta_lora_a', 'model.layers.23.attention.wk.mindpet_delta_lora_b', 'model.layers.23.attention.wv.mindpet_delta_lora_a', 'model.layers.23.attention.wv.mindpet_delta_lora_b', 'model.layers.24.attention.wq.mindpet_delta_lora_a', 'model.layers.24.attention.wq.mindpet_delta_lora_b', 'model.layers.24.attention.wk.mindpet_delta_lora_a', 'model.layers.24.attention.wk.mindpet_delta_lora_b', 'model.layers.24.attention.wv.mindpet_delta_lora_a', 'model.layers.24.attention.wv.mindpet_delta_lora_b', 'model.layers.25.attention.wq.mindpet_delta_lora_a', 'model.layers.25.attention.wq.mindpet_delta_lora_b', 'model.layers.25.attention.wk.mindpet_delta_lora_a', 'model.layers.25.attention.wk.mindpet_delta_lora_b', 'model.layers.25.attention.wv.mindpet_delta_lora_a', 'model.layers.25.attention.wv.mindpet_delta_lora_b', 'model.layers.26.attention.wq.mindpet_delta_lora_a', 'model.layers.26.attention.wq.mindpet_delta_lora_b', 'model.layers.26.attention.wk.mindpet_delta_lora_a', 'model.layers.26.attention.wk.mindpet_delta_lora_b', 'model.layers.26.attention.wv.mindpet_delta_lora_a', 'model.layers.26.attention.wv.mindpet_delta_lora_b', 'model.layers.27.attention.wq.mindpet_delta_lora_a', 'model.layers.27.attention.wq.mindpet_delta_lora_b', 'model.layers.27.attention.wk.mindpet_delta_lora_a', 'model.layers.27.attention.wk.mindpet_delta_lora_b', 'model.layers.27.attention.wv.mindpet_delta_lora_a', 'model.layers.27.attention.wv.mindpet_delta_lora_b', 'model.layers.28.attention.wq.mindpet_delta_lora_a', 'model.layers.28.attention.wq.mindpet_delta_lora_b', 'model.layers.28.attention.wk.mindpet_delta_lora_a', 'model.layers.28.attention.wk.mindpet_delta_lora_b', 'model.layers.28.attention.wv.mindpet_delta_lora_a', 'model.layers.28.attention.wv.mindpet_delta_lora_b', 'model.layers.29.attention.wq.mindpet_delta_lora_a', 'model.layers.29.attention.wq.mindpet_delta_lora_b', 'model.layers.29.attention.wk.mindpet_delta_lora_a', 'model.layers.29.attention.wk.mindpet_delta_lora_b', 'model.layers.29.attention.wv.mindpet_delta_lora_a', 'model.layers.29.attention.wv.mindpet_delta_lora_b', 'model.layers.30.attention.wq.mindpet_delta_lora_a', 'model.layers.30.attention.wq.mindpet_delta_lora_b', 'model.layers.30.attention.wk.mindpet_delta_lora_a', 'model.layers.30.attention.wk.mindpet_delta_lora_b', 'model.layers.30.attention.wv.mindpet_delta_lora_a', 'model.layers.30.attention.wv.mindpet_delta_lora_b', 'model.layers.31.attention.wq.mindpet_delta_lora_a', 'model.layers.31.attention.wq.mindpet_delta_lora_b', 'model.layers.31.attention.wk.mindpet_delta_lora_a', 'model.layers.31.attention.wk.mindpet_delta_lora_b', 'model.layers.31.attention.wv.mindpet_delta_lora_a', 'model.layers.31.attention.wv.mindpet_delta_lora_b'] are not loaded.
2024-07-06 20:56:57,381 - mindformers[mindformers/trainer/utils.py:767] - INFO - Network parameters are not loaded: (['model.layers.0.attention.wq.mindpet_delta_lora_a', 'model.layers.0.attention.wq.mindpet_delta_lora_b', 'model.layers.0.attention.wk.mindpet_delta_lora_a', 'model.layers.0.attention.wk.mindpet_delta_lora_b', 'model.layers.0.attention.wv.mindpet_delta_lora_a', 'model.layers.0.attention.wv.mindpet_delta_lora_b', 'model.layers.1.attention.wq.mindpet_delta_lora_a', 'model.layers.1.attention.wq.mindpet_delta_lora_b', 'model.layers.1.attention.wk.mindpet_delta_lora_a', 'model.layers.1.attention.wk.mindpet_delta_lora_b', 'model.layers.1.attention.wv.mindpet_delta_lora_a', 'model.layers.1.attention.wv.mindpet_delta_lora_b', 'model.layers.2.attention.wq.mindpet_delta_lora_a', 'model.layers.2.attention.wq.mindpet_delta_lora_b', 'model.layers.2.attention.wk.mindpet_delta_lora_a', 'model.layers.2.attention.wk.mindpet_delta_lora_b', 'model.layers.2.attention.wv.mindpet_delta_lora_a', 'model.layers.2.attention.wv.mindpet_delta_lora_b', 'model.layers.3.attention.wq.mindpet_delta_lora_a', 'model.layers.3.attention.wq.mindpet_delta_lora_b', 'model.layers.3.attention.wk.mindpet_delta_lora_a', 'model.layers.3.attention.wk.mindpet_delta_lora_b', 'model.layers.3.attention.wv.mindpet_delta_lora_a', 'model.layers.3.attention.wv.mindpet_delta_lora_b', 'model.layers.4.attention.wq.mindpet_delta_lora_a', 'model.layers.4.attention.wq.mindpet_delta_lora_b', 'model.layers.4.attention.wk.mindpet_delta_lora_a', 'model.layers.4.attention.wk.mindpet_delta_lora_b', 'model.layers.4.attention.wv.mindpet_delta_lora_a', 'model.layers.4.attention.wv.mindpet_delta_lora_b', 'model.layers.5.attention.wq.mindpet_delta_lora_a', 'model.layers.5.attention.wq.mindpet_delta_lora_b', 'model.layers.5.attention.wk.mindpet_delta_lora_a', 'model.layers.5.attention.wk.mindpet_delta_lora_b', 'model.layers.5.attention.wv.mindpet_delta_lora_a', 'model.layers.5.attention.wv.mindpet_delta_lora_b', 'model.layers.6.attention.wq.mindpet_delta_lora_a', 'model.layers.6.attention.wq.mindpet_delta_lora_b', 'model.layers.6.attention.wk.mindpet_delta_lora_a', 'model.layers.6.attention.wk.mindpet_delta_lora_b', 'model.layers.6.attention.wv.mindpet_delta_lora_a', 'model.layers.6.attention.wv.mindpet_delta_lora_b', 'model.layers.7.attention.wq.mindpet_delta_lora_a', 'model.layers.7.attention.wq.mindpet_delta_lora_b', 'model.layers.7.attention.wk.mindpet_delta_lora_a', 'model.layers.7.attention.wk.mindpet_delta_lora_b', 'model.layers.7.attention.wv.mindpet_delta_lora_a', 'model.layers.7.attention.wv.mindpet_delta_lora_b', 'model.layers.8.attention.wq.mindpet_delta_lora_a', 'model.layers.8.attention.wq.mindpet_delta_lora_b', 'model.layers.8.attention.wk.mindpet_delta_lora_a', 'model.layers.8.attention.wk.mindpet_delta_lora_b', 'model.layers.8.attention.wv.mindpet_delta_lora_a', 'model.layers.8.attention.wv.mindpet_delta_lora_b', 'model.layers.9.attention.wq.mindpet_delta_lora_a', 'model.layers.9.attention.wq.mindpet_delta_lora_b', 'model.layers.9.attention.wk.mindpet_delta_lora_a', 'model.layers.9.attention.wk.mindpet_delta_lora_b', 'model.layers.9.attention.wv.mindpet_delta_lora_a', 'model.layers.9.attention.wv.mindpet_delta_lora_b', 'model.layers.10.attention.wq.mindpet_delta_lora_a', 'model.layers.10.attention.wq.mindpet_delta_lora_b', 'model.layers.10.attention.wk.mindpet_delta_lora_a', 'model.layers.10.attention.wk.mindpet_delta_lora_b', 'model.layers.10.attention.wv.mindpet_delta_lora_a', 'model.layers.10.attention.wv.mindpet_delta_lora_b', 'model.layers.11.attention.wq.mindpet_delta_lora_a', 'model.layers.11.attention.wq.mindpet_delta_lora_b', 'model.layers.11.attention.wk.mindpet_delta_lora_a', 'model.layers.11.attention.wk.mindpet_delta_lora_b', 'model.layers.11.attention.wv.mindpet_delta_lora_a', 'model.layers.11.attention.wv.mindpet_delta_lora_b', 'model.layers.12.attention.wq.mindpet_delta_lora_a', 'model.layers.12.attention.wq.mindpet_delta_lora_b', 'model.layers.12.attention.wk.mindpet_delta_lora_a', 'model.layers.12.attention.wk.mindpet_delta_lora_b', 'model.layers.12.attention.wv.mindpet_delta_lora_a', 'model.layers.12.attention.wv.mindpet_delta_lora_b', 'model.layers.13.attention.wq.mindpet_delta_lora_a', 'model.layers.13.attention.wq.mindpet_delta_lora_b', 'model.layers.13.attention.wk.mindpet_delta_lora_a', 'model.layers.13.attention.wk.mindpet_delta_lora_b', 'model.layers.13.attention.wv.mindpet_delta_lora_a', 'model.layers.13.attention.wv.mindpet_delta_lora_b', 'model.layers.14.attention.wq.mindpet_delta_lora_a', 'model.layers.14.attention.wq.mindpet_delta_lora_b', 'model.layers.14.attention.wk.mindpet_delta_lora_a', 'model.layers.14.attention.wk.mindpet_delta_lora_b', 'model.layers.14.attention.wv.mindpet_delta_lora_a', 'model.layers.14.attention.wv.mindpet_delta_lora_b', 'model.layers.15.attention.wq.mindpet_delta_lora_a', 'model.layers.15.attention.wq.mindpet_delta_lora_b', 'model.layers.15.attention.wk.mindpet_delta_lora_a', 'model.layers.15.attention.wk.mindpet_delta_lora_b', 'model.layers.15.attention.wv.mindpet_delta_lora_a', 'model.layers.15.attention.wv.mindpet_delta_lora_b', 'model.layers.16.attention.wq.mindpet_delta_lora_a', 'model.layers.16.attention.wq.mindpet_delta_lora_b', 'model.layers.16.attention.wk.mindpet_delta_lora_a', 'model.layers.16.attention.wk.mindpet_delta_lora_b', 'model.layers.16.attention.wv.mindpet_delta_lora_a', 'model.layers.16.attention.wv.mindpet_delta_lora_b', 'model.layers.17.attention.wq.mindpet_delta_lora_a', 'model.layers.17.attention.wq.mindpet_delta_lora_b', 'model.layers.17.attention.wk.mindpet_delta_lora_a', 'model.layers.17.attention.wk.mindpet_delta_lora_b', 'model.layers.17.attention.wv.mindpet_delta_lora_a', 'model.layers.17.attention.wv.mindpet_delta_lora_b', 'model.layers.18.attention.wq.mindpet_delta_lora_a', 'model.layers.18.attention.wq.mindpet_delta_lora_b', 'model.layers.18.attention.wk.mindpet_delta_lora_a', 'model.layers.18.attention.wk.mindpet_delta_lora_b', 'model.layers.18.attention.wv.mindpet_delta_lora_a', 'model.layers.18.attention.wv.mindpet_delta_lora_b', 'model.layers.19.attention.wq.mindpet_delta_lora_a', 'model.layers.19.attention.wq.mindpet_delta_lora_b', 'model.layers.19.attention.wk.mindpet_delta_lora_a', 'model.layers.19.attention.wk.mindpet_delta_lora_b', 'model.layers.19.attention.wv.mindpet_delta_lora_a', 'model.layers.19.attention.wv.mindpet_delta_lora_b', 'model.layers.20.attention.wq.mindpet_delta_lora_a', 'model.layers.20.attention.wq.mindpet_delta_lora_b', 'model.layers.20.attention.wk.mindpet_delta_lora_a', 'model.layers.20.attention.wk.mindpet_delta_lora_b', 'model.layers.20.attention.wv.mindpet_delta_lora_a', 'model.layers.20.attention.wv.mindpet_delta_lora_b', 'model.layers.21.attention.wq.mindpet_delta_lora_a', 'model.layers.21.attention.wq.mindpet_delta_lora_b', 'model.layers.21.attention.wk.mindpet_delta_lora_a', 'model.layers.21.attention.wk.mindpet_delta_lora_b', 'model.layers.21.attention.wv.mindpet_delta_lora_a', 'model.layers.21.attention.wv.mindpet_delta_lora_b', 'model.layers.22.attention.wq.mindpet_delta_lora_a', 'model.layers.22.attention.wq.mindpet_delta_lora_b', 'model.layers.22.attention.wk.mindpet_delta_lora_a', 'model.layers.22.attention.wk.mindpet_delta_lora_b', 'model.layers.22.attention.wv.mindpet_delta_lora_a', 'model.layers.22.attention.wv.mindpet_delta_lora_b', 'model.layers.23.attention.wq.mindpet_delta_lora_a', 'model.layers.23.attention.wq.mindpet_delta_lora_b', 'model.layers.23.attention.wk.mindpet_delta_lora_a', 'model.layers.23.attention.wk.mindpet_delta_lora_b', 'model.layers.23.attention.wv.mindpet_delta_lora_a', 'model.layers.23.attention.wv.mindpet_delta_lora_b', 'model.layers.24.attention.wq.mindpet_delta_lora_a', 'model.layers.24.attention.wq.mindpet_delta_lora_b', 'model.layers.24.attention.wk.mindpet_delta_lora_a', 'model.layers.24.attention.wk.mindpet_delta_lora_b', 'model.layers.24.attention.wv.mindpet_delta_lora_a', 'model.layers.24.attention.wv.mindpet_delta_lora_b', 'model.layers.25.attention.wq.mindpet_delta_lora_a', 'model.layers.25.attention.wq.mindpet_delta_lora_b', 'model.layers.25.attention.wk.mindpet_delta_lora_a', 'model.layers.25.attention.wk.mindpet_delta_lora_b', 'model.layers.25.attention.wv.mindpet_delta_lora_a', 'model.layers.25.attention.wv.mindpet_delta_lora_b', 'model.layers.26.attention.wq.mindpet_delta_lora_a', 'model.layers.26.attention.wq.mindpet_delta_lora_b', 'model.layers.26.attention.wk.mindpet_delta_lora_a', 'model.layers.26.attention.wk.mindpet_delta_lora_b', 'model.layers.26.attention.wv.mindpet_delta_lora_a', 'model.layers.26.attention.wv.mindpet_delta_lora_b', 'model.layers.27.attention.wq.mindpet_delta_lora_a', 'model.layers.27.attention.wq.mindpet_delta_lora_b', 'model.layers.27.attention.wk.mindpet_delta_lora_a', 'model.layers.27.attention.wk.mindpet_delta_lora_b', 'model.layers.27.attention.wv.mindpet_delta_lora_a', 'model.layers.27.attention.wv.mindpet_delta_lora_b', 'model.layers.28.attention.wq.mindpet_delta_lora_a', 'model.layers.28.attention.wq.mindpet_delta_lora_b', 'model.layers.28.attention.wk.mindpet_delta_lora_a', 'model.layers.28.attention.wk.mindpet_delta_lora_b', 'model.layers.28.attention.wv.mindpet_delta_lora_a', 'model.layers.28.attention.wv.mindpet_delta_lora_b', 'model.layers.29.attention.wq.mindpet_delta_lora_a', 'model.layers.29.attention.wq.mindpet_delta_lora_b', 'model.layers.29.attention.wk.mindpet_delta_lora_a', 'model.layers.29.attention.wk.mindpet_delta_lora_b', 'model.layers.29.attention.wv.mindpet_delta_lora_a', 'model.layers.29.attention.wv.mindpet_delta_lora_b', 'model.layers.30.attention.wq.mindpet_delta_lora_a', 'model.layers.30.attention.wq.mindpet_delta_lora_b', 'model.layers.30.attention.wk.mindpet_delta_lora_a', 'model.layers.30.attention.wk.mindpet_delta_lora_b', 'model.layers.30.attention.wv.mindpet_delta_lora_a', 'model.layers.30.attention.wv.mindpet_delta_lora_b', 'model.layers.31.attention.wq.mindpet_delta_lora_a', 'model.layers.31.attention.wq.mindpet_delta_lora_b', 'model.layers.31.attention.wk.mindpet_delta_lora_a', 'model.layers.31.attention.wk.mindpet_delta_lora_b', 'model.layers.31.attention.wv.mindpet_delta_lora_a', 'model.layers.31.attention.wv.mindpet_delta_lora_b'], [])
2024-07-06 20:56:57,382 - mindformers[mindformers/trainer/base_trainer.py:770] - INFO - .........Starting Training Model..........
{'auto_trans_ckpt': False,
 'auto_tune': False,
 'autotune_per_step': 10,
 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),
               OrderedDict([('type', 'CheckpointMointor'),
                            ('prefix', 'llama3_8b'),
                            ('save_checkpoint_steps', 535),
                            ('integrated_save', False),
                            ('async_save', False)]),
               OrderedDict([('type', 'ObsMonitor')])],
 'context': {'device_target': 'Ascend',
             'enable_graph_kernel': False,
             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '
                                   '--enable_parallel_fusion=true '
                                   '--reduce_fuse_depth=8 '
                                   '--enable_auto_tensor_inplace=true',
             'max_call_depth': 10000,
             'runtime_num_threads': 1,
             'save_graphs': False,
             'save_graphs_path': './graph'},
 'data_size': 234,
 'device_num': 4,
 'do_eval': False,
 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor')])],
 'eval_dataset': {'auto_tune': False,
                  'autotune_per_step': 10,
                  'batch_size': 64,
                  'data_loader': {'dataset_dir': '',
                                  'shuffle': False,
                                  'type': 'MindDataset'},
                  'do_eval': True,
                  'drop_remainder': False,
                  'filepath_prefix': './autotune',
                  'input_columns': ['input_ids'],
                  'num_parallel_workers': 8,
                  'numa_enable': False,
                  'output_columns': ['input_ids'],
                  'prefetch_size': 1,
                  'profile': False,
                  'python_multiprocessing': False,
                  'repeat': 1,
                  'seed': 0},
 'eval_dataset_task': {'dataset_config': {'auto_tune': False,
                                          'autotune_per_step': 10,
                                          'batch_size': 64,
                                          'data_loader': {'dataset_dir': '',
                                                          'shuffle': False,
                                                          'type': 'MindDataset'},
                                          'do_eval': True,
                                          'drop_remainder': False,
                                          'filepath_prefix': './autotune',
                                          'input_columns': ['input_ids'],
                                          'num_parallel_workers': 8,
                                          'numa_enable': False,
                                          'output_columns': ['input_ids'],
                                          'prefetch_size': 1,
                                          'profile': False,
                                          'python_multiprocessing': False,
                                          'repeat': 1,
                                          'seed': 0},
                       'type': 'CausalLanguageModelDataset'},
 'filepath_prefix': './autotune',
 'init_start_profile': False,
 'layer_decay': 0.65,
 'layer_scale': False,
 'load_checkpoint': '/home/ma-user/work/llama3-8B.ckpt',
 'local_rank': 0,
 'lr_scale_factor': 256,
 'lr_schedule': {'learning_rate': 0.0001,
                 'lr_end': 0.0,
                 'total_steps': 1170,
                 'type': 'CosineWithWarmUpLR',
                 'warmup_steps': 35},
 'metric': [{'type': 'PerplexityMetric'}],
 'micro_batch_interleave_num': 1,
 'model': {'arch': {'type': 'LlamaForCausalLM'},
           'model_config': {'batch_size': 1,
                            'bos_token_id': 128000,
                            'checkpoint_name_or_path': None,
                            'compute_dtype': 'bfloat16',
                            'do_sample': False,
                            'eos_token_id': 128001,
                            'extend_method': 'None',
                            'fine_grain_interleave': 1,
                            'hidden_size': 4096,
                            'ignore_token_id': -100,
                            'intermediate_size': 14336,
                            'layernorm_compute_type': 'float32',
                            'max_decode_length': 512,
                            'n_kv_heads': 8,
                            'num_heads': 32,
                            'num_layers': 32,
                            'offset': 0,
                            'pad_token_id': 128002,
                            'param_init_type': 'bfloat16',
                            'pet_config': {'lora_alpha': 16,
                                           'lora_dropout': 0.05,
                                           'lora_rank': 8,
                                           'target_modules': '.*wq|.*wv|.*wk'},
                            'repetition_penalty': 1,
                            'rms_norm_eps': 1e-05,
                            'rotary_dtype': 'float32',
                            'scaling_factor': 1.0,
                            'seq_length': 256,
                            'softmax_compute_type': 'float32',
                            'theta': 500000,
                            'top_k': 3,
                            'top_p': 1,
                            'type': 'LlamaConfig',
                            'use_flash_attention': True,
                            'use_past': False,
                            'vocab_size': 128256}},
 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xffff26184730>,
 'only_save_strategy': False,
 'optimizer': {'beta1': 0.9,
               'beta2': 0.95,
               'eps': 1e-08,
               'type': 'FP32StateAdamWeightDecay'},
 'output_dir': './output',
 'parallel': {'device_num': 4,
              'enable_alltoall': False,
              'enable_parallel_optimizer': True,
              'full_batch': True,
              'gradients_mean': False,
              'parallel_mode': 'semi_auto_parallel',
              'parallel_optimizer_config': {'gradient_accumulation_shard': False,
                                            'parallel_optimizer_threshold': 64},
              'search_mode': 'sharding_propagation',
              'strategy_ckpt_config': {'only_trainable_params': False,
                                       'save_file': './ckpt_strategy.ckpt'},
              'strategy_ckpt_save_file': './output/strategy/ckpt_strategy_rank_0.ckpt'},
 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffdd04281c0>,
 'profile': False,
 'profile_communication': False,
 'profile_memory': True,
 'profile_start_step': 4,
 'profile_stop_step': 8,
 'rank_id': 0,
 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xfffdd04286a0>,
 'remote_save_url': '',
 'resume_training': False,
 'run_mode': 'finetune',
 'runner_config': {'batch_size': 64,
                   'epochs': 585,
                   'gradient_accumulation_steps': 1,
                   'initial_epoch': 0,
                   'initial_step': 0,
                   'origin_epochs': 5,
                   'sink_mode': True,
                   'sink_size': 2},
 'runner_wrapper': {'scale_sense': 1.0,
                    'type': 'MFTrainOneStepCell',
                    'use_clip_grad': True},
 'seed': 0,
 'src_strategy_path_or_dir': '',
 'train_dataset': {'auto_tune': False,
                   'autotune_per_step': 10,
                   'batch_size': 64,
                   'data_loader': {'dataset_dir': '/home/ma-user/work/train-arith15000.mindrecord',
                                   'shuffle': True,
                                   'type': 'MindDataset'},
                   'do_eval': False,
                   'drop_remainder': True,
                   'filepath_prefix': './autotune',
                   'input_columns': ['input_ids', 'labels'],
                   'num_parallel_workers': 8,
                   'numa_enable': False,
                   'output_columns': ['input_ids', 'labels'],
                   'prefetch_size': 1,
                   'profile': False,
                   'python_multiprocessing': False,
                   'repeat': 1,
                   'seed': 0},
 'train_dataset_task': {'dataset_config': {'auto_tune': False,
                                           'autotune_per_step': 10,
                                           'batch_size': 64,
                                           'data_loader': {'dataset_dir': '/home/ma-user/work/train-arith15000.mindrecord',
                                                           'shuffle': True,
                                                           'type': 'MindDataset'},
                                           'do_eval': False,
                                           'drop_remainder': True,
                                           'filepath_prefix': './autotune',
                                           'input_columns': ['input_ids',
                                                             'labels'],
                                           'num_parallel_workers': 8,
                                           'numa_enable': False,
                                           'output_columns': ['input_ids',
                                                              'labels'],
                                           'prefetch_size': 1,
                                           'profile': False,
                                           'python_multiprocessing': False,
                                           'repeat': 1,
                                           'seed': 0},
                        'type': 'CausalLanguageModelDataset'},
 'trainer': {'model_name': 'llama3_8b',
             'type': 'CausalLanguageModelingTrainer'},
 'use_parallel': True}
2024-07-06 20:56:57,388 - mindformers[mindformers/trainer/base_trainer.py:773] - INFO - .........Model Compiling, Please Wait a Moment...........
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:57.388.929 [mindspore/train/model.py:1120] For MFLossMonitor callback, {'step_end', 'epoch_begin', 'step_begin', 'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.
[WARNING] ME(73857:281473151655952,MainProcess):2024-07-06-20:56:57.389.065 [mindspore/train/model.py:1120] For Local2ObsMonitor callback, {'step_end', 'epoch_end'} methods may not be supported in later version, Use methods prefixed with 'on_train' or 'on_eval' instead when using customized callbacks.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.472.938 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/31-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.473.272 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/31-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.473.764 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/30-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.474.024 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/30-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.474.527 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/29-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.474.789 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/29-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.475.290 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/28-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.475.537 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/28-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.476.012 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/27-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.476.263 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/27-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.476.737 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/26-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.476.996 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/26-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.477.473 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/25-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.477.727 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/25-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.478.226 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/24-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.478.482 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/24-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.478.936 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/23-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.479.182 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/23-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.479.644 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/22-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.479.891 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/22-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.480.360 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/21-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.480.609 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/21-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.481.084 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/20-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.481.343 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/20-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.481.806 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/19-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.482.057 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/19-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.482.535 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/18-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.482.790 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/18-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.483.260 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/17-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.483.506 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/17-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.483.983 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/16-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.484.233 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/16-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.484.692 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/15-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.484.941 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/15-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.485.405 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/14-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.485.650 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/14-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.486.107 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/13-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.486.371 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/13-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.486.852 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/12-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.487.101 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/12-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.487.567 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/11-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.487.812 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/11-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.488.264 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/10-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.488.502 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/10-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.488.956 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/9-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.489.193 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/9-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.489.657 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/8-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.489.897 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/8-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.490.369 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/7-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.490.622 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/7-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.491.081 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/6-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.491.327 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/6-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.491.786 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/5-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.492.028 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/5-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.492.492 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/4-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.492.727 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/4-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.493.187 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/3-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.493.430 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/3-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.493.898 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/2-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.494.144 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/2-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.494.620 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/1-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.494.864 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/1-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.495.342 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/0-LLamaDecodeLayer/attention-LLamaAttention/wq-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.495.594 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/0-LLamaDecodeLayer/attention-LLamaAttention/wk-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.495.843 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/0-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.496.579 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/1-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.497.326 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/2-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.498.075 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/3-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.498.830 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/4-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.499.580 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/5-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.500.326 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/6-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.501.070 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/7-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.501.815 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/8-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.502.572 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/9-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.503.315 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/10-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.504.056 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/11-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.504.790 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/12-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.505.528 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/13-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.506.305 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/14-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.507.045 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/15-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.507.764 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/16-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.508.474 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/17-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.509.191 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/18-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.509.895 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/19-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.510.643 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/20-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.511.362 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/21-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.512.094 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/22-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.512.802 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/23-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.513.505 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/24-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.514.219 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/25-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.514.916 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/26-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.515.619 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/27-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.516.311 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/28-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.517.007 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/29-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.517.715 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/30-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:26.518.384 [mindspore/ccsrc/frontend/parallel/step_parallel.cc:1929] ReshapeInit] FindNextLayout for recompute_Default/network-MFTrainOneStepCell/network-_VirtualDatasetCell/_backbone-PetModel/pet_model-LoraModel/lora_model-LlamaForCausalLM/model-LlamaModel/layers-CellList/31-LLamaDecodeLayer/attention-LLamaAttention/wv-LoRADense/Reshape-op0 return nullptr, and is_next_reshape is 1. If reshape is not the last primitive, there must be some error.
[WARNING] PARALLEL(73857,ffff9337e010,python):2024-07-06-20:57:31.418.270 [mindspore/ccsrc/frontend/parallel/graph_util/graph_utils.cc:68] GetTensorRedistributionFromCNode] Default/network-MFTrainOneStepCell/clip_grad_norm-ClipGradNorm/Sqrt-op0 has no OperatorInfo.
-\|/-\|2024-07-06 21:09:44,257 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[    2/  234], loss: 1.292, per_step_time: 383209ms, lr: 2.8571428e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 21:09:44,273 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    0.2% |                                                  | 0.04175 samples/s/p  5 days, 4:19:49 }
2024-07-06 21:09:51,331 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[    4/  234], loss: 1.361, per_step_time: 3256ms, lr: 8.571428e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 21:09:51,335 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    0.3% |                                                  | 4.91346 samples/s/p  1:03:16 }
2024-07-06 21:09:57,697 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[    6/  234], loss: 1.340, per_step_time: 3177ms, lr: 1.4285714e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:09:57,698 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    0.5% |                                                  | 5.03566 samples/s/p  1:01:38 }
2024-07-06 21:10:04,060 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[    8/  234], loss: 1.158, per_step_time: 3177ms, lr: 2e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:04,060 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    0.7% |                                                  | 5.03556 samples/s/p  1:01:32 }
2024-07-06 21:10:10,429 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   10/  234], loss: 1.632, per_step_time: 3177ms, lr: 2.5714287e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:10,429 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    0.9% |                                                  | 5.03553 samples/s/p  1:01:25 }
2024-07-06 21:10:16,797 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   12/  234], loss: 1.231, per_step_time: 3178ms, lr: 3.142857e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:16,797 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    1.0% |                                                  | 5.03442 samples/s/p  1:01:20 }
2024-07-06 21:10:23,160 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   14/  234], loss: 1.441, per_step_time: 3178ms, lr: 3.7142858e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:23,160 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    1.2% |                                                  | 5.03458 samples/s/p  1:01:13 }
2024-07-06 21:10:29,525 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   16/  234], loss: 1.293, per_step_time: 3178ms, lr: 4.2857144e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:29,525 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    1.4% |                                                  | 5.03364 samples/s/p  1:01:08 }
2024-07-06 21:10:35,893 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   18/  234], loss: 1.329, per_step_time: 3180ms, lr: 4.8571426e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:35,894 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    1.5% |                                                  | 5.03016 samples/s/p  1:01:04 }
2024-07-06 21:10:42,261 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   20/  234], loss: 1.316, per_step_time: 3180ms, lr: 5.4285716e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:42,262 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    1.7% |                                                  | 5.03089 samples/s/p  1:00:57 }
2024-07-06 21:10:48,623 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   22/  234], loss: 1.029, per_step_time: 3177ms, lr: 6.0000002e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:48,624 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    1.9% |                                                  | 5.03526 samples/s/p  1:00:47 }
2024-07-06 21:10:54,991 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   24/  234], loss: 1.103, per_step_time: 3180ms, lr: 6.571429e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:10:54,991 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    2.1% |                                                 | 5.03124 samples/s/p  1:00:44 }
2024-07-06 21:11:01,355 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   26/  234], loss: 1.378, per_step_time: 3178ms, lr: 7.142857e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:01,355 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    2.2% |                                                 | 5.03402 samples/s/p  1:00:36 }
2024-07-06 21:11:07,721 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   28/  234], loss: 0.996, per_step_time: 3179ms, lr: 7.714285e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:07,722 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    2.4% |                                                 | 5.03197 samples/s/p  1:00:31 }
2024-07-06 21:11:14,084 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   30/  234], loss: 1.262, per_step_time: 3177ms, lr: 8.285714e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:14,084 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    2.6% |                                                 | 5.03604 samples/s/p  1:00:21 }
2024-07-06 21:11:20,467 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   32/  234], loss: 0.928, per_step_time: 3188ms, lr: 8.8571425e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:20,468 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    2.7% |                                                 | 5.01875 samples/s/p  1:00:27 }
2024-07-06 21:11:26,824 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   34/  234], loss: 0.651, per_step_time: 3174ms, lr: 9.4285715e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:26,824 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    2.9% |                                                 | 5.04038 samples/s/p  1:00:06 }
2024-07-06 21:11:33,181 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   36/  234], loss: 0.822, per_step_time: 3175ms, lr: 1e-04, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:33,182 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    3.1% |                                                 | 5.03936 samples/s/p  1:00:00 }
2024-07-06 21:11:39,547 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   38/  234], loss: 1.215, per_step_time: 3179ms, lr: 9.999923e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:39,547 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    3.2% |                                                 | 5.03302 samples/s/p  0:59:58 }
2024-07-06 21:11:45,908 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   40/  234], loss: 1.089, per_step_time: 3176ms, lr: 9.9996934e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:45,908 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    3.4% |                                                 | 5.03637 samples/s/p  0:59:49 }
2024-07-06 21:11:52,270 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   42/  234], loss: 1.028, per_step_time: 3177ms, lr: 9.999311e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:52,270 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    3.6% |                                                 | 5.03559 samples/s/p  0:59:44 }
2024-07-06 21:11:58,632 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   44/  234], loss: 1.065, per_step_time: 3177ms, lr: 9.9987745e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:11:58,633 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    3.8% |                                                 | 5.03517 samples/s/p  0:59:38 }
2024-07-06 21:12:04,993 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   46/  234], loss: 1.025, per_step_time: 3176ms, lr: 9.998085e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:04,994 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    3.9% |                                                 | 5.03643 samples/s/p  0:59:30 }
2024-07-06 21:12:11,352 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   48/  234], loss: 0.987, per_step_time: 3175ms, lr: 9.997242e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:11,352 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    4.1% |                                                | 5.03849 samples/s/p  0:59:22 }
2024-07-06 21:12:17,716 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   50/  234], loss: 0.936, per_step_time: 3178ms, lr: 9.996246e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:17,716 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    4.3% |                                                | 5.03443 samples/s/p  0:59:19 }
2024-07-06 21:12:24,074 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   52/  234], loss: 1.009, per_step_time: 3175ms, lr: 9.995097e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:24,074 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    4.4% |                                                | 5.03907 samples/s/p  0:59:09 }
2024-07-06 21:12:30,431 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   54/  234], loss: 1.089, per_step_time: 3175ms, lr: 9.993795e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:30,432 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    4.6% |                                                | 5.03892 samples/s/p  0:59:03 }
2024-07-06 21:12:36,789 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   56/  234], loss: 0.810, per_step_time: 3174ms, lr: 9.9923396e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:36,789 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    4.8% |                                                | 5.03956 samples/s/p  0:58:56 }
2024-07-06 21:12:43,142 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   58/  234], loss: 0.806, per_step_time: 3173ms, lr: 9.990732e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:43,143 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    5.0% |                                                | 5.04240 samples/s/p  0:58:48 }
2024-07-06 21:12:49,496 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   60/  234], loss: 1.046, per_step_time: 3173ms, lr: 9.9889716e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:49,497 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    5.1% |                                                | 5.04187 samples/s/p  0:58:42 }
2024-07-06 21:12:55,856 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   62/  234], loss: 0.783, per_step_time: 3175ms, lr: 9.987058e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:12:55,856 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    5.3% |                                                | 5.03797 samples/s/p  0:58:38 }
2024-07-06 21:13:02,210 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   64/  234], loss: 1.044, per_step_time: 3173ms, lr: 9.984991e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:02,210 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    5.5% |                                                | 5.04229 samples/s/p  0:58:29 }
2024-07-06 21:13:08,566 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   66/  234], loss: 1.165, per_step_time: 3174ms, lr: 9.982772e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:08,566 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    5.6% |                                                | 5.04019 samples/s/p  0:58:24 }
2024-07-06 21:13:14,922 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   68/  234], loss: 0.789, per_step_time: 3174ms, lr: 9.980399e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:14,922 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    5.8% |                                                | 5.04030 samples/s/p  0:58:18 }
2024-07-06 21:13:21,279 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   70/  234], loss: 0.925, per_step_time: 3174ms, lr: 9.977874e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:21,279 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    6.0% |                                                | 5.03943 samples/s/p  0:58:12 }
2024-07-06 21:13:27,636 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   72/  234], loss: 1.040, per_step_time: 3174ms, lr: 9.975197e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:27,636 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    6.2% |                                               | 5.03939 samples/s/p  0:58:06 }
2024-07-06 21:13:33,996 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   74/  234], loss: 0.840, per_step_time: 3176ms, lr: 9.972367e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:33,996 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    6.3% |                                               | 5.03745 samples/s/p  0:58:01 }
2024-07-06 21:13:40,350 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   76/  234], loss: 0.874, per_step_time: 3173ms, lr: 9.9693854e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:40,351 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    6.5% |                                               | 5.04130 samples/s/p  0:57:52 }
2024-07-06 21:13:46,707 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   78/  234], loss: 0.930, per_step_time: 3174ms, lr: 9.96625e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:46,707 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    6.7% |                                               | 5.03982 samples/s/p  0:57:46 }
2024-07-06 21:13:53,071 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   80/  234], loss: 0.968, per_step_time: 3178ms, lr: 9.9629644e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:53,072 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    6.8% |                                               | 5.03313 samples/s/p  0:57:45 }
2024-07-06 21:13:59,430 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   82/  234], loss: 0.762, per_step_time: 3175ms, lr: 9.959526e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:13:59,430 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    7.0% |                                               | 5.03831 samples/s/p  0:57:35 }
2024-07-06 21:14:05,786 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   84/  234], loss: 0.921, per_step_time: 3174ms, lr: 9.955935e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:05,787 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    7.2% |                                               | 5.03984 samples/s/p  0:57:27 }
2024-07-06 21:14:12,138 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   86/  234], loss: 0.870, per_step_time: 3172ms, lr: 9.952192e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:12,138 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    7.4% |                                               | 5.04370 samples/s/p  0:57:18 }
2024-07-06 21:14:18,490 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   88/  234], loss: 0.947, per_step_time: 3172ms, lr: 9.9482975e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:18,491 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    7.5% |                                               | 5.04297 samples/s/p  0:57:12 }
2024-07-06 21:14:24,845 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   90/  234], loss: 0.742, per_step_time: 3173ms, lr: 9.944251e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:24,845 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    7.7% |                                               | 5.04167 samples/s/p  0:57:07 }
2024-07-06 21:14:31,201 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   92/  234], loss: 0.881, per_step_time: 3174ms, lr: 9.940054e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:31,201 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    7.9% |                                               | 5.04057 samples/s/p  0:57:01 }
2024-07-06 21:14:37,556 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   94/  234], loss: 0.986, per_step_time: 3174ms, lr: 9.935706e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:37,557 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    8.0% |                                              | 5.04068 samples/s/p  0:56:55 }
2024-07-06 21:14:43,910 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   96/  234], loss: 0.832, per_step_time: 3173ms, lr: 9.9312056e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:43,911 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    8.2% |                                              | 5.04192 samples/s/p  0:56:48 }
2024-07-06 21:14:50,266 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[   98/  234], loss: 0.901, per_step_time: 3174ms, lr: 9.926554e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:50,266 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    8.4% |                                              | 5.04094 samples/s/p  0:56:42 }
2024-07-06 21:14:56,621 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  100/  234], loss: 0.825, per_step_time: 3174ms, lr: 9.921752e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:14:56,622 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    8.5% |                                              | 5.04070 samples/s/p  0:56:36 }
2024-07-06 21:15:02,978 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  102/  234], loss: 0.830, per_step_time: 3174ms, lr: 9.916799e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:02,980 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    8.7% |                                              | 5.03961 samples/s/p  0:56:30 }
2024-07-06 21:15:09,346 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  104/  234], loss: 0.897, per_step_time: 3179ms, lr: 9.911696e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:09,346 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    8.9% |                                              | 5.03233 samples/s/p  0:56:29 }
2024-07-06 21:15:15,699 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  106/  234], loss: 0.874, per_step_time: 3173ms, lr: 9.9064404e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:15,700 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    9.1% |                                              | 5.04227 samples/s/p  0:56:16 }
2024-07-06 21:15:22,055 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  108/  234], loss: 0.922, per_step_time: 3174ms, lr: 9.901036e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:22,055 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    9.2% |                                              | 5.04040 samples/s/p  0:56:11 }
2024-07-06 21:15:28,413 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  110/  234], loss: 0.777, per_step_time: 3175ms, lr: 9.8954806e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:28,414 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    9.4% |                                              | 5.03828 samples/s/p  0:56:06 }
2024-07-06 21:15:34,793 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  112/  234], loss: 1.066, per_step_time: 3186ms, lr: 9.889776e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:34,793 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    9.6% |                                              | 5.02167 samples/s/p  0:56:10 }
2024-07-06 21:15:41,149 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  114/  234], loss: 1.079, per_step_time: 3174ms, lr: 9.883922e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:41,149 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    9.7% |                                              | 5.04041 samples/s/p  0:55:52 }
2024-07-06 21:15:47,501 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  116/  234], loss: 0.743, per_step_time: 3172ms, lr: 9.877917e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:47,501 - mindformers[mindformers/core/callback/callback.py:326] - INFO -    9.9% |                                              | 5.04345 samples/s/p  0:55:43 }
2024-07-06 21:15:53,852 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  118/  234], loss: 0.566, per_step_time: 3172ms, lr: 9.871764e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:15:53,852 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   10.1% |                                             | 5.04408 samples/s/p  0:55:36 }
2024-07-06 21:16:00,208 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  120/  234], loss: 0.754, per_step_time: 3174ms, lr: 9.86546e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:00,209 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   10.3% |                                             | 5.04013 samples/s/p  0:55:33 }
2024-07-06 21:16:06,562 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  122/  234], loss: 1.000, per_step_time: 3173ms, lr: 9.859008e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:06,563 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   10.4% |                                             | 5.04196 samples/s/p  0:55:25 }
2024-07-06 21:16:12,918 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  124/  234], loss: 0.963, per_step_time: 3174ms, lr: 9.852407e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:12,918 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   10.6% |                                             | 5.04043 samples/s/p  0:55:20 }
2024-07-06 21:16:19,274 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  126/  234], loss: 0.926, per_step_time: 3174ms, lr: 9.845657e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:19,274 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   10.8% |                                             | 5.04044 samples/s/p  0:55:13 }
2024-07-06 21:16:25,628 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  128/  234], loss: 1.115, per_step_time: 3173ms, lr: 9.838759e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:25,628 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   10.9% |                                             | 5.04161 samples/s/p  0:55:06 }
2024-07-06 21:16:31,983 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  130/  234], loss: 0.915, per_step_time: 3173ms, lr: 9.831712e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:31,983 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   11.1% |                                             | 5.04160 samples/s/p  0:55:00 }
2024-07-06 21:16:38,334 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  132/  234], loss: 1.027, per_step_time: 3172ms, lr: 9.8245175e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:38,335 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   11.3% |                                             | 5.04409 samples/s/p  0:54:52 }
2024-07-06 21:16:44,696 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  134/  234], loss: 0.999, per_step_time: 3176ms, lr: 9.8171746e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:44,696 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   11.5% |                                             | 5.03623 samples/s/p  0:54:51 }
2024-07-06 21:16:51,050 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  136/  234], loss: 0.970, per_step_time: 3173ms, lr: 9.809685e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:51,051 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   11.6% |                                             | 5.04135 samples/s/p  0:54:41 }
2024-07-06 21:16:57,407 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  138/  234], loss: 1.120, per_step_time: 3174ms, lr: 9.8020464e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:16:57,408 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   11.8% |                                             | 5.03957 samples/s/p  0:54:36 }
2024-07-06 21:17:03,764 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  140/  234], loss: 0.857, per_step_time: 3174ms, lr: 9.7942626e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:03,764 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   12.0% |                                             | 5.03965 samples/s/p  0:54:30 }
2024-07-06 21:17:10,126 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  142/  234], loss: 0.877, per_step_time: 3177ms, lr: 9.78633e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:10,126 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   12.1% |                                            | 5.03575 samples/s/p  0:54:26 }
2024-07-06 21:17:16,487 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  144/  234], loss: 1.033, per_step_time: 3177ms, lr: 9.778252e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:16,488 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   12.3% |                                            | 5.03603 samples/s/p  0:54:19 }
2024-07-06 21:17:22,865 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  146/  234], loss: 0.989, per_step_time: 3185ms, lr: 9.770027e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:22,865 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   12.5% |                                            | 5.02323 samples/s/p  0:54:21 }
2024-07-06 21:17:29,217 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  148/  234], loss: 0.797, per_step_time: 3172ms, lr: 9.761656e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:29,218 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   12.6% |                                            | 5.04318 samples/s/p  0:54:02 }
2024-07-06 21:17:35,570 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  150/  234], loss: 1.080, per_step_time: 3172ms, lr: 9.7531396e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:35,571 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   12.8% |                                            | 5.04286 samples/s/p  0:53:56 }
2024-07-06 21:17:41,925 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  152/  234], loss: 0.984, per_step_time: 3173ms, lr: 9.744477e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:41,926 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   13.0% |                                            | 5.04104 samples/s/p  0:53:51 }
2024-07-06 21:17:48,276 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  154/  234], loss: 0.756, per_step_time: 3171ms, lr: 9.7356686e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:48,276 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   13.2% |                                            | 5.04438 samples/s/p  0:53:42 }
2024-07-06 21:17:54,634 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  156/  234], loss: 1.104, per_step_time: 3175ms, lr: 9.726716e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:17:54,634 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   13.3% |                                            | 5.03867 samples/s/p  0:53:39 }
2024-07-06 21:18:00,992 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  158/  234], loss: 1.097, per_step_time: 3175ms, lr: 9.717618e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:00,992 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   13.5% |                                            | 5.03855 samples/s/p  0:53:33 }
2024-07-06 21:18:07,344 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  160/  234], loss: 0.947, per_step_time: 3172ms, lr: 9.708376e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:07,345 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   13.7% |                                            | 5.04317 samples/s/p  0:53:24 }
2024-07-06 21:18:13,700 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  162/  234], loss: 1.072, per_step_time: 3174ms, lr: 9.698988e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:13,700 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   13.8% |                                            | 5.04060 samples/s/p  0:53:19 }
2024-07-06 21:18:20,055 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  164/  234], loss: 0.906, per_step_time: 3174ms, lr: 9.6894575e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:20,056 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   14.0% |                                           | 5.04054 samples/s/p  0:53:13 }
2024-07-06 21:18:26,410 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  166/  234], loss: 0.900, per_step_time: 3173ms, lr: 9.6797834e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:26,411 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   14.2% |                                           | 5.04108 samples/s/p  0:53:06 }
2024-07-06 21:18:32,769 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  168/  234], loss: 0.952, per_step_time: 3175ms, lr: 9.669965e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:32,770 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   14.4% |                                           | 5.03781 samples/s/p  0:53:02 }
2024-07-06 21:18:39,122 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  170/  234], loss: 0.959, per_step_time: 3172ms, lr: 9.6600044e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:39,123 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   14.5% |                                           | 5.04263 samples/s/p  0:52:52 }
2024-07-06 21:18:45,479 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  172/  234], loss: 0.889, per_step_time: 3174ms, lr: 9.6499e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:45,480 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   14.7% |                                           | 5.03944 samples/s/p  0:52:48 }
2024-07-06 21:18:51,837 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  174/  234], loss: 1.047, per_step_time: 3175ms, lr: 9.6396536e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:51,838 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   14.9% |                                           | 5.03866 samples/s/p  0:52:42 }
2024-07-06 21:18:58,194 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  176/  234], loss: 0.928, per_step_time: 3174ms, lr: 9.629265e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:18:58,194 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   15.0% |                                           | 5.03963 samples/s/p  0:52:35 }
2024-07-06 21:19:04,547 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  178/  234], loss: 1.119, per_step_time: 3172ms, lr: 9.618735e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:04,548 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   15.2% |                                           | 5.04278 samples/s/p  0:52:27 }
2024-07-06 21:19:10,901 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  180/  234], loss: 1.062, per_step_time: 3173ms, lr: 9.608063e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:10,901 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   15.4% |                                           | 5.04223 samples/s/p  0:52:21 }
2024-07-06 21:19:17,255 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  182/  234], loss: 0.966, per_step_time: 3173ms, lr: 9.59725e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:17,255 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   15.6% |                                           | 5.04176 samples/s/p  0:52:15 }
2024-07-06 21:19:23,607 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  184/  234], loss: 1.161, per_step_time: 3172ms, lr: 9.586296e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:23,608 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   15.7% |                                           | 5.04306 samples/s/p  0:52:08 }
2024-07-06 21:19:29,964 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  186/  234], loss: 1.088, per_step_time: 3174ms, lr: 9.575201e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:29,964 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   15.9% |                                           | 5.04005 samples/s/p  0:52:03 }
2024-07-06 21:19:36,317 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  188/  234], loss: 0.911, per_step_time: 3172ms, lr: 9.563966e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:36,317 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   16.1% |                                          | 5.04270 samples/s/p  0:51:55 }
2024-07-06 21:19:42,670 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  190/  234], loss: 1.000, per_step_time: 3173ms, lr: 9.552592e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:42,671 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   16.2% |                                          | 5.04224 samples/s/p  0:51:49 }
2024-07-06 21:19:49,023 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  192/  234], loss: 0.871, per_step_time: 3172ms, lr: 9.5410775e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:49,023 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   16.4% |                                          | 5.04321 samples/s/p  0:51:42 }
2024-07-06 21:19:55,380 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  194/  234], loss: 1.011, per_step_time: 3175ms, lr: 9.529425e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:19:55,380 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   16.6% |                                          | 5.03921 samples/s/p  0:51:38 }
2024-07-06 21:20:01,735 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  196/  234], loss: 1.131, per_step_time: 3173ms, lr: 9.517632e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:01,735 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   16.8% |                                          | 5.04138 samples/s/p  0:51:31 }
2024-07-06 21:20:08,091 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  198/  234], loss: 0.961, per_step_time: 3174ms, lr: 9.505702e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:08,092 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   16.9% |                                          | 5.03978 samples/s/p  0:51:25 }
2024-07-06 21:20:14,457 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  200/  234], loss: 0.906, per_step_time: 3173ms, lr: 9.4936324e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:14,457 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   17.1% |                                          | 5.04204 samples/s/p  0:51:18 }
2024-07-06 21:20:20,813 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  202/  234], loss: 1.063, per_step_time: 3174ms, lr: 9.481426e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:20,814 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   17.3% |                                          | 5.04002 samples/s/p  0:51:13 }
2024-07-06 21:20:27,170 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  204/  234], loss: 0.946, per_step_time: 3174ms, lr: 9.469082e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:27,170 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   17.4% |                                          | 5.03994 samples/s/p  0:51:06 }
2024-07-06 21:20:33,524 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  206/  234], loss: 1.058, per_step_time: 3173ms, lr: 9.456602e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:33,525 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   17.6% |                                          | 5.04209 samples/s/p  0:50:59 }
2024-07-06 21:20:39,884 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  208/  234], loss: 0.958, per_step_time: 3176ms, lr: 9.443985e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:39,884 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   17.8% |                                          | 5.03768 samples/s/p  0:50:55 }
2024-07-06 21:20:46,240 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  210/  234], loss: 0.747, per_step_time: 3174ms, lr: 9.431231e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:46,241 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   17.9% |                                          | 5.03991 samples/s/p  0:50:47 }
2024-07-06 21:20:52,595 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  212/  234], loss: 1.053, per_step_time: 3173ms, lr: 9.418342e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:52,595 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   18.1% |                                         | 5.04126 samples/s/p  0:50:40 }
2024-07-06 21:20:58,954 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  214/  234], loss: 1.044, per_step_time: 3175ms, lr: 9.405317e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:20:58,954 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   18.3% |                                         | 5.03794 samples/s/p  0:50:36 }
2024-07-06 21:21:05,310 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  216/  234], loss: 1.067, per_step_time: 3174ms, lr: 9.3921575e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:05,311 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   18.5% |                                         | 5.03969 samples/s/p  0:50:28 }
2024-07-06 21:21:11,664 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  218/  234], loss: 0.692, per_step_time: 3173ms, lr: 9.3788636e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:11,665 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   18.6% |                                         | 5.04198 samples/s/p  0:50:21 }
2024-07-06 21:21:18,017 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  220/  234], loss: 0.867, per_step_time: 3172ms, lr: 9.365435e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:18,017 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   18.8% |                                         | 5.04278 samples/s/p  0:50:14 }
2024-07-06 21:21:24,371 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  222/  234], loss: 1.081, per_step_time: 3173ms, lr: 9.3518735e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:24,372 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   19.0% |                                         | 5.04143 samples/s/p  0:50:08 }
2024-07-06 21:21:30,725 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  224/  234], loss: 1.187, per_step_time: 3173ms, lr: 9.338177e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:30,726 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   19.1% |                                         | 5.04204 samples/s/p  0:50:01 }
2024-07-06 21:21:37,080 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  226/  234], loss: 0.751, per_step_time: 3173ms, lr: 9.324349e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:37,081 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   19.3% |                                         | 5.04098 samples/s/p  0:49:56 }
2024-07-06 21:21:43,432 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  228/  234], loss: 0.952, per_step_time: 3172ms, lr: 9.3103874e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:43,433 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   19.5% |                                         | 5.04341 samples/s/p  0:49:48 }
2024-07-06 21:21:49,788 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  230/  234], loss: 0.887, per_step_time: 3173ms, lr: 9.296294e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:49,788 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   19.7% |                                         | 5.04140 samples/s/p  0:49:43 }
2024-07-06 21:21:56,144 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  232/  234], loss: 0.951, per_step_time: 3174ms, lr: 9.2820694e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:21:56,144 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   19.8% |                                         | 5.04007 samples/s/p  0:49:37 }
2024-07-06 21:22:02,499 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  1/  5], step:[  234/  234], loss: 1.001, per_step_time: 3173ms, lr: 9.267714e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:02,499 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   20.0% |                                        | 5.04128 samples/s/p  0:49:30 }
2024-07-06 21:22:08,857 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[    2/  234], loss: 0.853, per_step_time: 3175ms, lr: 9.253226e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:08,858 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   20.2% |                                        | 5.03850 samples/s/p  0:49:25 }
2024-07-06 21:22:15,207 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[    4/  234], loss: 0.778, per_step_time: 3171ms, lr: 9.238609e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:15,207 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   20.3% |                                        | 5.04531 samples/s/p  0:49:15 }
2024-07-06 21:22:21,563 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[    6/  234], loss: 0.961, per_step_time: 3174ms, lr: 9.2238624e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:21,563 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   20.5% |                                        | 5.04064 samples/s/p  0:49:12 }
2024-07-06 21:22:27,923 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[    8/  234], loss: 1.159, per_step_time: 3176ms, lr: 9.208986e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:27,923 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   20.7% |                                        | 5.03707 samples/s/p  0:49:07 }
2024-07-06 21:22:34,278 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   10/  234], loss: 0.960, per_step_time: 3174ms, lr: 9.193981e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:34,279 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   20.9% |                                        | 5.04090 samples/s/p  0:48:59 }
2024-07-06 21:22:40,631 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   12/  234], loss: 1.232, per_step_time: 3173ms, lr: 9.178847e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:40,632 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   21.0% |                                        | 5.04244 samples/s/p  0:48:51 }
2024-07-06 21:22:46,983 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   14/  234], loss: 0.808, per_step_time: 3172ms, lr: 9.163585e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:46,983 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   21.2% |                                        | 5.04395 samples/s/p  0:48:44 }
2024-07-06 21:22:53,335 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   16/  234], loss: 1.021, per_step_time: 3172ms, lr: 9.1481954e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:53,336 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   21.4% |                                        | 5.04295 samples/s/p  0:48:38 }
2024-07-06 21:22:59,687 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   18/  234], loss: 0.905, per_step_time: 3172ms, lr: 9.132678e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:22:59,688 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   21.5% |                                        | 5.04327 samples/s/p  0:48:32 }
2024-07-06 21:23:06,041 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   20/  234], loss: 0.789, per_step_time: 3173ms, lr: 9.117035e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:06,041 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   21.7% |                                        | 5.04234 samples/s/p  0:48:26 }
2024-07-06 21:23:12,393 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   22/  234], loss: 0.741, per_step_time: 3172ms, lr: 9.1012655e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:12,393 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   21.9% |                                        | 5.04318 samples/s/p  0:48:19 }
2024-07-06 21:23:18,744 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   24/  234], loss: 0.844, per_step_time: 3172ms, lr: 9.085369e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:18,745 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   22.1% |                                       | 5.04400 samples/s/p  0:48:12 }
2024-07-06 21:23:25,099 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   26/  234], loss: 1.085, per_step_time: 3173ms, lr: 9.0693495e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:25,100 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   22.2% |                                       | 5.04098 samples/s/p  0:48:08 }
2024-07-06 21:23:31,456 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   28/  234], loss: 0.863, per_step_time: 3174ms, lr: 9.053204e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:31,456 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   22.4% |                                       | 5.03991 samples/s/p  0:48:02 }
2024-07-06 21:23:37,811 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   30/  234], loss: 1.027, per_step_time: 3173ms, lr: 9.036935e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:37,811 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   22.6% |                                       | 5.04189 samples/s/p  0:47:55 }
2024-07-06 21:23:44,168 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   32/  234], loss: 1.151, per_step_time: 3175ms, lr: 9.020541e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:44,169 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   22.7% |                                       | 5.03920 samples/s/p  0:47:50 }
2024-07-06 21:23:50,521 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   34/  234], loss: 1.065, per_step_time: 3172ms, lr: 9.004025e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:50,522 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   22.9% |                                       | 5.04276 samples/s/p  0:47:41 }
2024-07-06 21:23:56,873 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   36/  234], loss: 0.846, per_step_time: 3172ms, lr: 8.987386e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:23:56,874 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   23.1% |                                       | 5.04375 samples/s/p  0:47:35 }
2024-07-06 21:24:03,227 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   38/  234], loss: 0.969, per_step_time: 3173ms, lr: 8.970625e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:03,227 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   23.2% |                                       | 5.04194 samples/s/p  0:47:29 }
2024-07-06 21:24:09,585 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   40/  234], loss: 0.922, per_step_time: 3175ms, lr: 8.953742e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:09,585 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   23.4% |                                       | 5.03917 samples/s/p  0:47:24 }
2024-07-06 21:24:15,942 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   42/  234], loss: 0.796, per_step_time: 3174ms, lr: 8.936738e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:15,942 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   23.6% |                                       | 5.03963 samples/s/p  0:47:18 }
2024-07-06 21:24:22,295 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   44/  234], loss: 0.852, per_step_time: 3173ms, lr: 8.9196124e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:22,296 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   23.8% |                                       | 5.04221 samples/s/p  0:47:10 }
2024-07-06 21:24:28,648 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   46/  234], loss: 0.944, per_step_time: 3172ms, lr: 8.902368e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:28,649 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   23.9% |                                       | 5.04271 samples/s/p  0:47:03 }
2024-07-06 21:24:35,007 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   48/  234], loss: 1.124, per_step_time: 3175ms, lr: 8.8850036e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:35,007 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   24.1% |                                      | 5.03842 samples/s/p  0:46:59 }
2024-07-06 21:24:41,364 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   50/  234], loss: 0.856, per_step_time: 3175ms, lr: 8.86752e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:41,365 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   24.3% |                                      | 5.03890 samples/s/p  0:46:53 }
2024-07-06 21:24:47,718 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   52/  234], loss: 1.077, per_step_time: 3173ms, lr: 8.849918e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:47,719 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   24.4% |                                      | 5.04174 samples/s/p  0:46:45 }
2024-07-06 21:24:54,072 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   54/  234], loss: 0.925, per_step_time: 3173ms, lr: 8.8321984e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:24:54,072 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   24.6% |                                      | 5.04229 samples/s/p  0:46:38 }
2024-07-06 21:25:00,426 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   56/  234], loss: 0.912, per_step_time: 3172ms, lr: 8.8143606e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:00,426 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   24.8% |                                      | 5.04256 samples/s/p  0:46:32 }
2024-07-06 21:25:06,778 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   58/  234], loss: 0.948, per_step_time: 3172ms, lr: 8.7964065e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:06,778 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   25.0% |                                      | 5.04343 samples/s/p  0:46:25 }
2024-07-06 21:25:13,132 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   60/  234], loss: 0.977, per_step_time: 3173ms, lr: 8.778336e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:13,133 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   25.1% |                                      | 5.04176 samples/s/p  0:46:19 }
2024-07-06 21:25:19,486 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   62/  234], loss: 0.892, per_step_time: 3173ms, lr: 8.760149e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:19,486 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   25.3% |                                      | 5.04244 samples/s/p  0:46:13 }
2024-07-06 21:25:25,841 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   64/  234], loss: 0.846, per_step_time: 3173ms, lr: 8.741847e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:25,841 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   25.5% |                                      | 5.04098 samples/s/p  0:46:07 }
2024-07-06 21:25:32,194 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   66/  234], loss: 0.813, per_step_time: 3172ms, lr: 8.723431e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:32,194 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   25.6% |                                      | 5.04277 samples/s/p  0:46:00 }
2024-07-06 21:25:38,548 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   68/  234], loss: 0.876, per_step_time: 3173ms, lr: 8.7049004e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:38,548 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   25.8% |                                      | 5.04253 samples/s/p  0:45:54 }
2024-07-06 21:25:44,899 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   70/  234], loss: 0.839, per_step_time: 3172ms, lr: 8.6862565e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:44,899 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   26.0% |                                      | 5.04414 samples/s/p  0:45:46 }
2024-07-06 21:25:51,252 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   72/  234], loss: 0.850, per_step_time: 3172ms, lr: 8.6675e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:51,252 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   26.2% |                                     | 5.04269 samples/s/p  0:45:41 }
2024-07-06 21:25:57,613 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   74/  234], loss: 0.927, per_step_time: 3176ms, lr: 8.64863e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:25:57,613 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   26.3% |                                     | 5.03637 samples/s/p  0:45:38 }
2024-07-06 21:26:03,970 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   76/  234], loss: 0.948, per_step_time: 3175ms, lr: 8.6296495e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:03,971 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   26.5% |                                     | 5.03886 samples/s/p  0:45:30 }
2024-07-06 21:26:10,327 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   78/  234], loss: 0.880, per_step_time: 3174ms, lr: 8.6105574e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:10,328 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   26.7% |                                     | 5.03969 samples/s/p  0:45:23 }
2024-07-06 21:26:16,681 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   80/  234], loss: 0.818, per_step_time: 3173ms, lr: 8.591354e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:16,681 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   26.8% |                                     | 5.04212 samples/s/p  0:45:16 }
2024-07-06 21:26:23,034 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   82/  234], loss: 0.912, per_step_time: 3172ms, lr: 8.572041e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:23,034 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   27.0% |                                     | 5.04278 samples/s/p  0:45:09 }
2024-07-06 21:26:29,387 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   84/  234], loss: 0.820, per_step_time: 3173ms, lr: 8.5526175e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:29,388 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   27.2% |                                     | 5.04204 samples/s/p  0:45:03 }
2024-07-06 21:26:35,742 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   86/  234], loss: 0.949, per_step_time: 3173ms, lr: 8.533087e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:35,742 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   27.4% |                                     | 5.04143 samples/s/p  0:44:57 }
2024-07-06 21:26:42,092 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   88/  234], loss: 0.737, per_step_time: 3171ms, lr: 8.513447e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:42,092 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   27.5% |                                     | 5.04487 samples/s/p  0:44:49 }
2024-07-06 21:26:48,454 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   90/  234], loss: 1.064, per_step_time: 3177ms, lr: 8.493698e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:48,455 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   27.7% |                                     | 5.03499 samples/s/p  0:44:48 }
2024-07-06 21:26:54,812 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   92/  234], loss: 1.168, per_step_time: 3175ms, lr: 8.473845e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:26:54,813 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   27.9% |                                     | 5.03865 samples/s/p  0:44:40 }
2024-07-06 21:27:01,166 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   94/  234], loss: 0.642, per_step_time: 3172ms, lr: 8.453884e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:01,167 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   28.0% |                                    | 5.04295 samples/s/p  0:44:31 }
2024-07-06 21:27:07,521 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   96/  234], loss: 0.998, per_step_time: 3174ms, lr: 8.433817e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:07,522 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   28.2% |                                    | 5.04093 samples/s/p  0:44:26 }
2024-07-06 21:27:13,876 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[   98/  234], loss: 0.749, per_step_time: 3173ms, lr: 8.4136445e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:13,876 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   28.4% |                                    | 5.04142 samples/s/p  0:44:19 }
2024-07-06 21:27:20,231 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  100/  234], loss: 0.728, per_step_time: 3173ms, lr: 8.3933686e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:20,232 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   28.5% |                                    | 5.04100 samples/s/p  0:44:13 }
2024-07-06 21:27:26,583 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  102/  234], loss: 0.855, per_step_time: 3172ms, lr: 8.372988e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:26,584 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   28.7% |                                    | 5.04365 samples/s/p  0:44:05 }
2024-07-06 21:27:32,933 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  104/  234], loss: 0.902, per_step_time: 3171ms, lr: 8.352504e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:32,934 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   28.9% |                                    | 5.04488 samples/s/p  0:43:58 }
2024-07-06 21:27:39,285 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  106/  234], loss: 0.717, per_step_time: 3172ms, lr: 8.331916e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:39,286 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   29.1% |                                    | 5.04360 samples/s/p  0:43:53 }
2024-07-06 21:27:45,642 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  108/  234], loss: 1.075, per_step_time: 3174ms, lr: 8.311229e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:45,642 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   29.2% |                                    | 5.03985 samples/s/p  0:43:48 }
2024-07-06 21:27:51,999 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  110/  234], loss: 0.932, per_step_time: 3175ms, lr: 8.290437e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:52,000 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   29.4% |                                    | 5.03902 samples/s/p  0:43:42 }
2024-07-06 21:27:58,353 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  112/  234], loss: 0.919, per_step_time: 3173ms, lr: 8.269547e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:27:58,353 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   29.6% |                                    | 5.04197 samples/s/p  0:43:34 }
2024-07-06 21:28:04,703 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  114/  234], loss: 0.914, per_step_time: 3171ms, lr: 8.2485545e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:04,704 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   29.7% |                                    | 5.04482 samples/s/p  0:43:27 }
2024-07-06 21:28:11,058 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  116/  234], loss: 0.803, per_step_time: 3174ms, lr: 8.227465e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:11,059 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   29.9% |                                    | 5.04075 samples/s/p  0:43:22 }
2024-07-06 21:28:17,413 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  118/  234], loss: 0.783, per_step_time: 3173ms, lr: 8.206275e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:17,414 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   30.1% |                                   | 5.04111 samples/s/p  0:43:16 }
2024-07-06 21:28:23,767 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  120/  234], loss: 0.928, per_step_time: 3173ms, lr: 8.184986e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:23,767 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   30.3% |                                   | 5.04224 samples/s/p  0:43:09 }
2024-07-06 21:28:30,119 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  122/  234], loss: 1.173, per_step_time: 3172ms, lr: 8.1636004e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:30,119 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   30.4% |                                   | 5.04385 samples/s/p  0:43:02 }
2024-07-06 21:28:36,469 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  124/  234], loss: 0.745, per_step_time: 3171ms, lr: 8.142119e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:36,470 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   30.6% |                                   | 5.04483 samples/s/p  0:42:55 }
2024-07-06 21:28:42,823 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  126/  234], loss: 0.675, per_step_time: 3173ms, lr: 8.120539e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:42,824 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   30.8% |                                   | 5.04190 samples/s/p  0:42:50 }
2024-07-06 21:28:49,181 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  128/  234], loss: 0.994, per_step_time: 3175ms, lr: 8.098864e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:49,182 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   30.9% |                                   | 5.03854 samples/s/p  0:42:45 }
2024-07-06 21:28:55,539 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  130/  234], loss: 1.164, per_step_time: 3174ms, lr: 8.077095e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:28:55,539 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   31.1% |                                   | 5.03953 samples/s/p  0:42:38 }
2024-07-06 21:29:01,896 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  132/  234], loss: 0.987, per_step_time: 3175ms, lr: 8.055232e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:01,896 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   31.3% |                                   | 5.03936 samples/s/p  0:42:32 }
2024-07-06 21:29:08,247 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  134/  234], loss: 0.982, per_step_time: 3171ms, lr: 8.033274e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:08,247 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   31.5% |                                   | 5.04471 samples/s/p  0:42:23 }
2024-07-06 21:29:14,601 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  136/  234], loss: 0.810, per_step_time: 3173ms, lr: 8.011224e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:14,601 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   31.6% |                                   | 5.04182 samples/s/p  0:42:18 }
2024-07-06 21:29:20,959 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  138/  234], loss: 1.200, per_step_time: 3175ms, lr: 7.989081e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:20,959 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   31.8% |                                   | 5.03870 samples/s/p  0:42:13 }
2024-07-06 21:29:27,309 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  140/  234], loss: 0.856, per_step_time: 3171ms, lr: 7.966846e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:27,310 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   32.0% |                                   | 5.04469 samples/s/p  0:42:04 }
2024-07-06 21:29:33,667 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  142/  234], loss: 0.987, per_step_time: 3175ms, lr: 7.944521e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:33,667 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   32.1% |                                  | 5.03883 samples/s/p  0:42:01 }
2024-07-06 21:29:40,046 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  144/  234], loss: 0.970, per_step_time: 3185ms, lr: 7.9221056e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:40,046 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   32.3% |                                  | 5.02212 samples/s/p  0:42:03 }
2024-07-06 21:29:46,395 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  146/  234], loss: 0.946, per_step_time: 3170ms, lr: 7.899601e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:46,395 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   32.5% |                                  | 5.04591 samples/s/p  0:41:44 }
2024-07-06 21:29:52,751 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  148/  234], loss: 0.960, per_step_time: 3174ms, lr: 7.877008e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:52,751 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   32.6% |                                  | 5.04011 samples/s/p  0:41:41 }
2024-07-06 21:29:59,102 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  150/  234], loss: 1.189, per_step_time: 3172ms, lr: 7.854326e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:29:59,102 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   32.8% |                                  | 5.04396 samples/s/p  0:41:33 }
2024-07-06 21:30:05,459 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  152/  234], loss: 0.969, per_step_time: 3174ms, lr: 7.8315556e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:05,459 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   33.0% |                                  | 5.03998 samples/s/p  0:41:28 }
2024-07-06 21:30:11,814 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  154/  234], loss: 1.062, per_step_time: 3174ms, lr: 7.8086996e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:11,815 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   33.2% |                                  | 5.04091 samples/s/p  0:41:22 }
2024-07-06 21:30:18,169 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  156/  234], loss: 0.932, per_step_time: 3173ms, lr: 7.785758e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:18,170 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   33.3% |                                  | 5.04106 samples/s/p  0:41:15 }
2024-07-06 21:30:24,525 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  158/  234], loss: 0.858, per_step_time: 3174ms, lr: 7.76273e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:24,526 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   33.5% |                                  | 5.04052 samples/s/p  0:41:09 }
2024-07-06 21:30:30,882 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  160/  234], loss: 0.884, per_step_time: 3174ms, lr: 7.739617e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:30,882 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   33.7% |                                  | 5.04050 samples/s/p  0:41:03 }
2024-07-06 21:30:37,234 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  162/  234], loss: 0.748, per_step_time: 3172ms, lr: 7.716421e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:37,235 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   33.8% |                                  | 5.04340 samples/s/p  0:40:55 }
2024-07-06 21:30:43,590 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  164/  234], loss: 0.994, per_step_time: 3174ms, lr: 7.6931414e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:43,590 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   34.0% |                                 | 5.04050 samples/s/p  0:40:50 }
2024-07-06 21:30:49,947 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  166/  234], loss: 0.844, per_step_time: 3174ms, lr: 7.669779e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:49,947 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   34.2% |                                 | 5.03989 samples/s/p  0:40:44 }
2024-07-06 21:30:56,299 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  168/  234], loss: 0.901, per_step_time: 3172ms, lr: 7.646335e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:30:56,299 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   34.4% |                                 | 5.04368 samples/s/p  0:40:36 }
2024-07-06 21:31:02,654 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  170/  234], loss: 0.851, per_step_time: 3173ms, lr: 7.6228105e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:02,654 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   34.5% |                                 | 5.04119 samples/s/p  0:40:31 }
2024-07-06 21:31:09,013 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  172/  234], loss: 0.920, per_step_time: 3175ms, lr: 7.5992044e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:09,013 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   34.7% |                                 | 5.03815 samples/s/p  0:40:26 }
2024-07-06 21:31:15,371 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  174/  234], loss: 1.192, per_step_time: 3175ms, lr: 7.5755204e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:15,372 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   34.9% |                                 | 5.03815 samples/s/p  0:40:19 }
2024-07-06 21:31:21,726 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  176/  234], loss: 0.906, per_step_time: 3173ms, lr: 7.551756e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:21,726 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   35.0% |                                 | 5.04149 samples/s/p  0:40:11 }
2024-07-06 21:31:28,078 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  178/  234], loss: 0.969, per_step_time: 3172ms, lr: 7.527914e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:28,078 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   35.2% |                                 | 5.04335 samples/s/p  0:40:04 }
2024-07-06 21:31:34,427 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  180/  234], loss: 1.015, per_step_time: 3170ms, lr: 7.503995e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:34,427 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   35.4% |                                 | 5.04576 samples/s/p  0:39:57 }
2024-07-06 21:31:40,778 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  182/  234], loss: 0.604, per_step_time: 3172ms, lr: 7.479998e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:40,779 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   35.6% |                                 | 5.04376 samples/s/p  0:39:51 }
2024-07-06 21:31:47,131 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  184/  234], loss: 0.936, per_step_time: 3172ms, lr: 7.455925e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:47,132 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   35.7% |                                 | 5.04288 samples/s/p  0:39:45 }
2024-07-06 21:31:53,486 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  186/  234], loss: 0.971, per_step_time: 3173ms, lr: 7.431777e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:53,486 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   35.9% |                                 | 5.04227 samples/s/p  0:39:39 }
2024-07-06 21:31:59,843 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  188/  234], loss: 1.115, per_step_time: 3175ms, lr: 7.407555e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:31:59,844 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   36.1% |                                | 5.03919 samples/s/p  0:39:34 }
2024-07-06 21:32:06,198 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  190/  234], loss: 0.925, per_step_time: 3173ms, lr: 7.383259e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:06,198 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   36.2% |                                | 5.04164 samples/s/p  0:39:27 }
2024-07-06 21:32:12,550 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  192/  234], loss: 1.100, per_step_time: 3172ms, lr: 7.35889e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:12,551 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   36.4% |                                | 5.04278 samples/s/p  0:39:20 }
2024-07-06 21:32:18,909 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  194/  234], loss: 1.054, per_step_time: 3175ms, lr: 7.334449e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:18,909 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   36.6% |                                | 5.03878 samples/s/p  0:39:16 }
2024-07-06 21:32:25,266 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  196/  234], loss: 0.717, per_step_time: 3174ms, lr: 7.3099356e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:25,266 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   36.8% |                                | 5.03975 samples/s/p  0:39:09 }
2024-07-06 21:32:31,621 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  198/  234], loss: 0.888, per_step_time: 3174ms, lr: 7.285353e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:31,621 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   36.9% |                                | 5.04090 samples/s/p  0:39:02 }
2024-07-06 21:32:37,978 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  200/  234], loss: 0.801, per_step_time: 3175ms, lr: 7.260698e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:37,979 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   37.1% |                                | 5.03908 samples/s/p  0:38:56 }
2024-07-06 21:32:44,344 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  202/  234], loss: 0.841, per_step_time: 3179ms, lr: 7.235976e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:44,345 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   37.3% |                                | 5.03258 samples/s/p  0:38:53 }
2024-07-06 21:32:50,705 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  204/  234], loss: 0.860, per_step_time: 3176ms, lr: 7.2111834e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:50,706 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   37.4% |                                | 5.03621 samples/s/p  0:38:45 }
2024-07-06 21:32:57,058 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  206/  234], loss: 0.803, per_step_time: 3172ms, lr: 7.186326e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:32:57,058 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   37.6% |                                | 5.04306 samples/s/p  0:38:36 }
2024-07-06 21:33:03,411 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  208/  234], loss: 0.952, per_step_time: 3172ms, lr: 7.1613984e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:03,411 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   37.8% |                                | 5.04269 samples/s/p  0:38:29 }
2024-07-06 21:33:09,765 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  210/  234], loss: 0.765, per_step_time: 3173ms, lr: 7.136408e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:09,765 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   37.9% |                                | 5.04177 samples/s/p  0:38:23 }
2024-07-06 21:33:16,118 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  212/  234], loss: 0.855, per_step_time: 3172ms, lr: 7.1113485e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:16,118 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   38.1% |                               | 5.04265 samples/s/p  0:38:17 }
2024-07-06 21:33:22,471 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  214/  234], loss: 1.022, per_step_time: 3172ms, lr: 7.086227e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:22,471 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   38.3% |                               | 5.04320 samples/s/p  0:38:10 }
2024-07-06 21:33:28,823 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  216/  234], loss: 1.070, per_step_time: 3172ms, lr: 7.06104e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:28,824 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   38.5% |                               | 5.04334 samples/s/p  0:38:04 }
2024-07-06 21:33:35,176 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  218/  234], loss: 0.956, per_step_time: 3172ms, lr: 7.035789e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:35,177 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   38.6% |                               | 5.04291 samples/s/p  0:37:58 }
2024-07-06 21:33:41,530 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  220/  234], loss: 0.698, per_step_time: 3173ms, lr: 7.0104776e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:41,531 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   38.8% |                               | 5.04189 samples/s/p  0:37:52 }
2024-07-06 21:33:47,885 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  222/  234], loss: 0.954, per_step_time: 3173ms, lr: 6.985104e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:47,885 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   39.0% |                               | 5.04167 samples/s/p  0:37:45 }
2024-07-06 21:33:54,240 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  224/  234], loss: 0.857, per_step_time: 3173ms, lr: 6.9596696e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:33:54,240 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   39.1% |                               | 5.04162 samples/s/p  0:37:39 }
2024-07-06 21:34:00,593 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  226/  234], loss: 1.057, per_step_time: 3173ms, lr: 6.934174e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:00,594 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   39.3% |                               | 5.04245 samples/s/p  0:37:32 }
2024-07-06 21:34:06,948 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  228/  234], loss: 0.973, per_step_time: 3174ms, lr: 6.908621e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:06,949 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   39.5% |                               | 5.04087 samples/s/p  0:37:27 }
2024-07-06 21:34:13,306 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  230/  234], loss: 1.152, per_step_time: 3175ms, lr: 6.8830086e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:13,306 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   39.7% |                               | 5.03934 samples/s/p  0:37:21 }
2024-07-06 21:34:19,663 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  232/  234], loss: 1.018, per_step_time: 3175ms, lr: 6.857338e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:19,664 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   39.8% |                               | 5.03885 samples/s/p  0:37:15 }
2024-07-06 21:34:26,021 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  2/  5], step:[  234/  234], loss: 0.969, per_step_time: 3175ms, lr: 6.831611e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:26,021 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   40.0% |                              | 5.03893 samples/s/p  0:37:09 }
2024-07-06 21:34:32,376 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[    2/  234], loss: 0.798, per_step_time: 3173ms, lr: 6.805828e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:32,376 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   40.2% |                              | 5.04106 samples/s/p  0:37:01 }
2024-07-06 21:34:38,730 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[    4/  234], loss: 0.950, per_step_time: 3173ms, lr: 6.77999e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:38,731 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   40.3% |                              | 5.04156 samples/s/p  0:36:55 }
2024-07-06 21:34:45,082 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[    6/  234], loss: 1.010, per_step_time: 3172ms, lr: 6.754097e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:45,083 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   40.5% |                              | 5.04350 samples/s/p  0:36:47 }
2024-07-06 21:34:51,438 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[    8/  234], loss: 1.156, per_step_time: 3174ms, lr: 6.72815e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:51,438 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   40.7% |                              | 5.04071 samples/s/p  0:36:42 }
2024-07-06 21:34:57,789 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   10/  234], loss: 0.790, per_step_time: 3171ms, lr: 6.70215e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:34:57,789 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   40.9% |                              | 5.04423 samples/s/p  0:36:34 }
2024-07-06 21:35:04,141 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   12/  234], loss: 0.899, per_step_time: 3172ms, lr: 6.6760986e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:04,142 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   41.0% |                              | 5.04305 samples/s/p  0:36:29 }
2024-07-06 21:35:10,495 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   14/  234], loss: 0.813, per_step_time: 3173ms, lr: 6.649995e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:10,496 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   41.2% |                              | 5.04209 samples/s/p  0:36:23 }
2024-07-06 21:35:16,846 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   16/  234], loss: 1.112, per_step_time: 3171ms, lr: 6.623841e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:16,846 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   41.4% |                              | 5.04535 samples/s/p  0:36:15 }
2024-07-06 21:35:23,200 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   18/  234], loss: 1.019, per_step_time: 3173ms, lr: 6.597638e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:23,201 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   41.5% |                              | 5.04177 samples/s/p  0:36:10 }
2024-07-06 21:35:29,554 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   20/  234], loss: 1.268, per_step_time: 3173ms, lr: 6.571385e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:29,554 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   41.7% |                              | 5.04217 samples/s/p  0:36:04 }
2024-07-06 21:35:35,910 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   22/  234], loss: 0.965, per_step_time: 3174ms, lr: 6.545085e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:35,910 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   41.9% |                              | 5.04076 samples/s/p  0:35:58 }
2024-07-06 21:35:42,268 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   24/  234], loss: 1.162, per_step_time: 3175ms, lr: 6.518737e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:42,268 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   42.1% |                             | 5.03894 samples/s/p  0:35:52 }
2024-07-06 21:35:48,624 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   26/  234], loss: 1.085, per_step_time: 3174ms, lr: 6.492341e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:48,624 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   42.2% |                             | 5.04034 samples/s/p  0:35:45 }
2024-07-06 21:35:54,975 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   28/  234], loss: 0.915, per_step_time: 3172ms, lr: 6.465901e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:35:54,976 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   42.4% |                             | 5.04356 samples/s/p  0:35:38 }
2024-07-06 21:36:01,328 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   30/  234], loss: 0.943, per_step_time: 3172ms, lr: 6.439416e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:01,328 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   42.6% |                             | 5.04296 samples/s/p  0:35:32 }
2024-07-06 21:36:07,682 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   32/  234], loss: 0.975, per_step_time: 3173ms, lr: 6.4128864e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:07,683 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   42.7% |                             | 5.04173 samples/s/p  0:35:26 }
2024-07-06 21:36:14,036 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   34/  234], loss: 0.868, per_step_time: 3173ms, lr: 6.386314e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:14,036 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   42.9% |                             | 5.04232 samples/s/p  0:35:19 }
2024-07-06 21:36:20,392 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   36/  234], loss: 0.992, per_step_time: 3174ms, lr: 6.359699e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:20,392 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   43.1% |                             | 5.03993 samples/s/p  0:35:14 }
2024-07-06 21:36:26,748 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   38/  234], loss: 0.739, per_step_time: 3174ms, lr: 6.333042e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:26,749 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   43.2% |                             | 5.04080 samples/s/p  0:35:07 }
2024-07-06 21:36:33,104 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   40/  234], loss: 0.877, per_step_time: 3174ms, lr: 6.306344e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:33,105 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   43.4% |                             | 5.04027 samples/s/p  0:35:01 }
2024-07-06 21:36:39,456 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   42/  234], loss: 0.928, per_step_time: 3172ms, lr: 6.279607e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:39,457 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   43.6% |                             | 5.04369 samples/s/p  0:34:53 }
2024-07-06 21:36:45,807 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   44/  234], loss: 0.675, per_step_time: 3171ms, lr: 6.252831e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:45,807 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   43.8% |                             | 5.04520 samples/s/p  0:34:46 }
2024-07-06 21:36:52,167 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   46/  234], loss: 0.767, per_step_time: 3176ms, lr: 6.2260144e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:52,167 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   43.9% |                             | 5.03707 samples/s/p  0:34:43 }
2024-07-06 21:36:58,525 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   48/  234], loss: 0.901, per_step_time: 3175ms, lr: 6.199162e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:36:58,525 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   44.1% |                            | 5.03888 samples/s/p  0:34:36 }
2024-07-06 21:37:04,880 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   50/  234], loss: 0.733, per_step_time: 3174ms, lr: 6.172272e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:04,881 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   44.3% |                            | 5.04055 samples/s/p  0:34:29 }
2024-07-06 21:37:11,233 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   52/  234], loss: 0.924, per_step_time: 3172ms, lr: 6.1453466e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:11,233 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   44.4% |                            | 5.04317 samples/s/p  0:34:22 }
2024-07-06 21:37:17,591 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   54/  234], loss: 1.064, per_step_time: 3175ms, lr: 6.1183855e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:17,591 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   44.6% |                            | 5.03888 samples/s/p  0:34:17 }
2024-07-06 21:37:23,943 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   56/  234], loss: 1.014, per_step_time: 3172ms, lr: 6.0913906e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:23,944 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   44.8% |                            | 5.04281 samples/s/p  0:34:09 }
2024-07-06 21:37:30,305 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   58/  234], loss: 0.778, per_step_time: 3176ms, lr: 6.0643622e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:30,305 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   45.0% |                            | 5.03625 samples/s/p  0:34:05 }
2024-07-06 21:37:36,658 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   60/  234], loss: 0.824, per_step_time: 3172ms, lr: 6.037302e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:36,659 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   45.1% |                            | 5.04284 samples/s/p  0:33:56 }
2024-07-06 21:37:43,011 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   62/  234], loss: 0.931, per_step_time: 3172ms, lr: 6.010209e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:43,012 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   45.3% |                            | 5.04267 samples/s/p  0:33:50 }
2024-07-06 21:37:49,373 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   64/  234], loss: 1.055, per_step_time: 3177ms, lr: 5.9830854e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:49,373 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   45.5% |                            | 5.03612 samples/s/p  0:33:46 }
2024-07-06 21:37:55,728 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   66/  234], loss: 0.975, per_step_time: 3173ms, lr: 5.9559316e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:37:55,728 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   45.6% |                            | 5.04110 samples/s/p  0:33:38 }
2024-07-06 21:38:02,082 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   68/  234], loss: 0.881, per_step_time: 3173ms, lr: 5.9287482e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:38:02,083 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   45.8% |                            | 5.04189 samples/s/p  0:33:31 }
2024-07-06 21:38:02,084 - mindformers[mindformers/core/callback/callback.py:562] - INFO - ......Saving ckpt......
2024-07-06 21:38:36,912 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   70/  234], loss: 0.830, per_step_time: 3399ms, lr: 5.901537e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:38:36,913 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   46.0% |                            | 4.70656 samples/s/p  0:35:48 }
2024-07-06 21:38:43,266 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   72/  234], loss: 0.693, per_step_time: 3173ms, lr: 5.8742975e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:38:43,267 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   46.2% |                           | 5.04193 samples/s/p  0:33:19 }
2024-07-06 21:38:49,623 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   74/  234], loss: 0.732, per_step_time: 3174ms, lr: 5.847031e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:38:49,623 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   46.3% |                           | 5.04085 samples/s/p  0:33:13 }
2024-07-06 21:38:55,974 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   76/  234], loss: 0.706, per_step_time: 3171ms, lr: 5.8197395e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:38:55,974 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   46.5% |                           | 5.04435 samples/s/p  0:33:05 }
2024-07-06 21:39:02,327 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   78/  234], loss: 0.899, per_step_time: 3172ms, lr: 5.7924222e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:02,328 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   46.7% |                           | 5.04277 samples/s/p  0:32:59 }
2024-07-06 21:39:08,678 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   80/  234], loss: 0.675, per_step_time: 3171ms, lr: 5.7650803e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:08,678 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   46.8% |                           | 5.04458 samples/s/p  0:32:52 }
2024-07-06 21:39:15,036 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   82/  234], loss: 0.851, per_step_time: 3175ms, lr: 5.7377158e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:15,036 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   47.0% |                           | 5.03881 samples/s/p  0:32:48 }
2024-07-06 21:39:21,393 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   84/  234], loss: 0.953, per_step_time: 3174ms, lr: 5.710328e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:21,393 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   47.2% |                           | 5.03970 samples/s/p  0:32:42 }
2024-07-06 21:39:27,756 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   86/  234], loss: 0.967, per_step_time: 3177ms, lr: 5.6829187e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:27,756 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   47.4% |                           | 5.03471 samples/s/p  0:32:37 }
2024-07-06 21:39:34,116 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   88/  234], loss: 0.983, per_step_time: 3176ms, lr: 5.6554887e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:34,116 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   47.5% |                           | 5.03732 samples/s/p  0:32:30 }
2024-07-06 21:39:40,471 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   90/  234], loss: 0.910, per_step_time: 3173ms, lr: 5.6280383e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:40,471 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   47.7% |                           | 5.04113 samples/s/p  0:32:22 }
2024-07-06 21:39:46,824 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   92/  234], loss: 1.057, per_step_time: 3173ms, lr: 5.600569e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:46,825 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   47.9% |                           | 5.04223 samples/s/p  0:32:15 }
2024-07-06 21:39:53,177 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   94/  234], loss: 0.779, per_step_time: 3172ms, lr: 5.5730812e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:53,177 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   48.0% |                          | 5.04318 samples/s/p  0:32:08 }
2024-07-06 21:39:59,524 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   96/  234], loss: 0.938, per_step_time: 3170ms, lr: 5.5455756e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:39:59,525 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   48.2% |                          | 5.04676 samples/s/p  0:32:01 }
2024-07-06 21:40:05,876 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[   98/  234], loss: 0.845, per_step_time: 3172ms, lr: 5.5180535e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:05,876 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   48.4% |                          | 5.04396 samples/s/p  0:31:55 }
2024-07-06 21:40:12,229 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  100/  234], loss: 0.700, per_step_time: 3173ms, lr: 5.490516e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:12,230 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   48.5% |                          | 5.04225 samples/s/p  0:31:50 }
2024-07-06 21:40:18,956 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  102/  234], loss: 0.940, per_step_time: 3175ms, lr: 5.462962e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:18,956 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   48.7% |                          | 5.03783 samples/s/p  0:31:45 }
2024-07-06 21:40:25,308 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  104/  234], loss: 0.944, per_step_time: 3172ms, lr: 5.435395e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:25,308 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   48.9% |                          | 5.04328 samples/s/p  0:31:37 }
2024-07-06 21:40:31,663 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  106/  234], loss: 0.808, per_step_time: 3173ms, lr: 5.4078148e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:31,663 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   49.1% |                          | 5.04130 samples/s/p  0:31:31 }
2024-07-06 21:40:38,020 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  108/  234], loss: 1.038, per_step_time: 3174ms, lr: 5.380221e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:38,021 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   49.2% |                          | 5.03965 samples/s/p  0:31:25 }
2024-07-06 21:40:44,379 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  110/  234], loss: 0.857, per_step_time: 3175ms, lr: 5.3526168e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:44,380 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   49.4% |                          | 5.03844 samples/s/p  0:31:19 }
2024-07-06 21:40:50,738 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  112/  234], loss: 0.866, per_step_time: 3175ms, lr: 5.3250013e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:50,739 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   49.6% |                          | 5.03794 samples/s/p  0:31:13 }
2024-07-06 21:40:57,096 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  114/  234], loss: 0.727, per_step_time: 3175ms, lr: 5.297376e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:40:57,096 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   49.7% |                          | 5.03878 samples/s/p  0:31:07 }
2024-07-06 21:41:03,450 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  116/  234], loss: 1.057, per_step_time: 3173ms, lr: 5.2697407e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:03,451 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   49.9% |                          | 5.04225 samples/s/p  0:30:59 }
2024-07-06 21:41:09,809 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  118/  234], loss: 1.044, per_step_time: 3175ms, lr: 5.2420983e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:09,810 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   50.1% |                         | 5.03794 samples/s/p  0:30:54 }
2024-07-06 21:41:16,161 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  120/  234], loss: 0.680, per_step_time: 3172ms, lr: 5.2144478e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:16,162 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   50.3% |                         | 5.04374 samples/s/p  0:30:46 }
2024-07-06 21:41:22,517 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  122/  234], loss: 1.081, per_step_time: 3174ms, lr: 5.1867908e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:22,517 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   50.4% |                         | 5.04059 samples/s/p  0:30:41 }
2024-07-06 21:41:28,870 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  124/  234], loss: 0.835, per_step_time: 3173ms, lr: 5.1591283e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:28,871 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   50.6% |                         | 5.04247 samples/s/p  0:30:34 }
2024-07-06 21:41:35,229 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  126/  234], loss: 1.063, per_step_time: 3175ms, lr: 5.1314608e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:35,230 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   50.8% |                         | 5.03783 samples/s/p  0:30:29 }
2024-07-06 21:41:41,586 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  128/  234], loss: 0.877, per_step_time: 3174ms, lr: 5.103789e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:41,586 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   50.9% |                         | 5.04021 samples/s/p  0:30:22 }
2024-07-06 21:41:47,939 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  130/  234], loss: 1.103, per_step_time: 3173ms, lr: 5.0761144e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:47,940 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   51.1% |                         | 5.04229 samples/s/p  0:30:15 }
2024-07-06 21:41:54,300 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  132/  234], loss: 0.743, per_step_time: 3176ms, lr: 5.048437e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:41:54,300 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   51.3% |                         | 5.03740 samples/s/p  0:30:10 }
2024-07-06 21:42:00,653 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  134/  234], loss: 0.833, per_step_time: 3172ms, lr: 5.020759e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:00,653 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   51.5% |                         | 5.04288 samples/s/p  0:30:02 }
2024-07-06 21:42:07,004 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  136/  234], loss: 0.788, per_step_time: 3172ms, lr: 4.9930793e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:07,005 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   51.6% |                         | 5.04391 samples/s/p  0:29:55 }
2024-07-06 21:42:13,358 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  138/  234], loss: 0.798, per_step_time: 3173ms, lr: 4.9654012e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:13,359 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   51.8% |                         | 5.04218 samples/s/p  0:29:49 }
2024-07-06 21:42:19,720 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  140/  234], loss: 1.010, per_step_time: 3177ms, lr: 4.9377228e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:19,720 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   52.0% |                         | 5.03608 samples/s/p  0:29:45 }
2024-07-06 21:42:26,073 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  142/  234], loss: 0.762, per_step_time: 3172ms, lr: 4.910047e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:26,073 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   52.1% |                        | 5.04257 samples/s/p  0:29:36 }
2024-07-06 21:42:32,428 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  144/  234], loss: 0.968, per_step_time: 3174ms, lr: 4.882373e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:32,429 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   52.3% |                        | 5.04083 samples/s/p  0:29:31 }
2024-07-06 21:42:38,783 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  146/  234], loss: 0.977, per_step_time: 3173ms, lr: 4.854704e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:38,784 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   52.5% |                        | 5.04134 samples/s/p  0:29:24 }
2024-07-06 21:42:45,142 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  148/  234], loss: 1.053, per_step_time: 3175ms, lr: 4.8270387e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:45,142 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   52.6% |                        | 5.03831 samples/s/p  0:29:19 }
2024-07-06 21:42:51,498 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  150/  234], loss: 1.101, per_step_time: 3174ms, lr: 4.7993784e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:51,499 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   52.8% |                        | 5.03970 samples/s/p  0:29:12 }
2024-07-06 21:42:57,851 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  152/  234], loss: 0.943, per_step_time: 3173ms, lr: 4.7717254e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:42:57,852 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   53.0% |                        | 5.04248 samples/s/p  0:29:05 }
2024-07-06 21:43:04,204 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  154/  234], loss: 1.036, per_step_time: 3172ms, lr: 4.7440786e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:04,205 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   53.2% |                        | 5.04351 samples/s/p  0:28:58 }
2024-07-06 21:43:10,558 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  156/  234], loss: 0.704, per_step_time: 3173ms, lr: 4.716439e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:10,558 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   53.3% |                        | 5.04224 samples/s/p  0:28:52 }
2024-07-06 21:43:16,920 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  158/  234], loss: 1.027, per_step_time: 3177ms, lr: 4.6888097e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:16,921 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   53.5% |                        | 5.03526 samples/s/p  0:28:48 }
2024-07-06 21:43:23,277 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  160/  234], loss: 0.894, per_step_time: 3173ms, lr: 4.6611884e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:23,277 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   53.7% |                        | 5.04237 samples/s/p  0:28:39 }
2024-07-06 21:43:29,634 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  162/  234], loss: 1.024, per_step_time: 3174ms, lr: 4.633578e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:29,634 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   53.8% |                        | 5.03953 samples/s/p  0:28:34 }
2024-07-06 21:43:35,994 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  164/  234], loss: 0.816, per_step_time: 3176ms, lr: 4.60598e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:35,994 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   54.0% |                       | 5.03715 samples/s/p  0:28:28 }
2024-07-06 21:43:42,349 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  166/  234], loss: 0.717, per_step_time: 3173ms, lr: 4.578392e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:42,350 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   54.2% |                       | 5.04098 samples/s/p  0:28:21 }
2024-07-06 21:43:48,714 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  168/  234], loss: 0.676, per_step_time: 3178ms, lr: 4.5508186e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:48,715 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   54.4% |                       | 5.03368 samples/s/p  0:28:17 }
2024-07-06 21:43:55,070 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  170/  234], loss: 0.944, per_step_time: 3174ms, lr: 4.5232584e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:43:55,070 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   54.5% |                       | 5.04086 samples/s/p  0:28:08 }
2024-07-06 21:44:01,423 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  172/  234], loss: 1.139, per_step_time: 3172ms, lr: 4.495712e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:01,423 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   54.7% |                       | 5.04262 samples/s/p  0:28:01 }
2024-07-06 21:44:07,776 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  174/  234], loss: 0.952, per_step_time: 3173ms, lr: 4.4681827e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:07,776 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   54.9% |                       | 5.04254 samples/s/p  0:27:55 }
2024-07-06 21:44:14,130 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  176/  234], loss: 0.955, per_step_time: 3173ms, lr: 4.4406683e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:14,131 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   55.0% |                       | 5.04154 samples/s/p  0:27:49 }
2024-07-06 21:44:20,485 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  178/  234], loss: 0.749, per_step_time: 3173ms, lr: 4.4131713e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:20,485 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   55.2% |                       | 5.04131 samples/s/p  0:27:43 }
2024-07-06 21:44:26,838 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  180/  234], loss: 0.851, per_step_time: 3172ms, lr: 4.385693e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:26,838 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   55.4% |                       | 5.04303 samples/s/p  0:27:36 }
2024-07-06 21:44:33,193 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  182/  234], loss: 1.007, per_step_time: 3173ms, lr: 4.3582324e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:33,193 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   55.6% |                       | 5.04129 samples/s/p  0:27:30 }
2024-07-06 21:44:39,546 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  184/  234], loss: 0.945, per_step_time: 3173ms, lr: 4.3307922e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:39,546 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   55.7% |                       | 5.04242 samples/s/p  0:27:23 }
2024-07-06 21:44:45,897 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  186/  234], loss: 0.914, per_step_time: 3171ms, lr: 4.303373e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:45,897 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   55.9% |                       | 5.04428 samples/s/p  0:27:16 }
2024-07-06 21:44:52,257 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  188/  234], loss: 0.816, per_step_time: 3175ms, lr: 4.275974e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:52,257 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   56.1% |                      | 5.03790 samples/s/p  0:27:12 }
2024-07-06 21:44:58,612 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  190/  234], loss: 1.055, per_step_time: 3173ms, lr: 4.248598e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:44:58,612 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   56.2% |                      | 5.04099 samples/s/p  0:27:05 }
2024-07-06 21:45:04,964 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  192/  234], loss: 0.853, per_step_time: 3172ms, lr: 4.221244e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:04,964 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   56.4% |                      | 5.04349 samples/s/p  0:26:57 }
2024-07-06 21:45:11,323 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  194/  234], loss: 1.008, per_step_time: 3175ms, lr: 4.1939154e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:11,324 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   56.6% |                      | 5.03795 samples/s/p  0:26:53 }
2024-07-06 21:45:17,681 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  196/  234], loss: 0.851, per_step_time: 3174ms, lr: 4.1666106e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:17,681 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   56.8% |                      | 5.03944 samples/s/p  0:26:46 }
2024-07-06 21:45:24,038 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  198/  234], loss: 0.996, per_step_time: 3174ms, lr: 4.139331e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:24,038 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   56.9% |                      | 5.03953 samples/s/p  0:26:40 }
2024-07-06 21:45:30,395 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  200/  234], loss: 0.968, per_step_time: 3175ms, lr: 4.1120788e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:30,396 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   57.1% |                      | 5.03932 samples/s/p  0:26:33 }
2024-07-06 21:45:36,745 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  202/  234], loss: 0.852, per_step_time: 3171ms, lr: 4.0848536e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:36,746 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   57.3% |                      | 5.04483 samples/s/p  0:26:25 }
2024-07-06 21:45:43,102 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  204/  234], loss: 0.892, per_step_time: 3174ms, lr: 4.0576553e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:43,102 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   57.4% |                      | 5.03987 samples/s/p  0:26:20 }
2024-07-06 21:45:49,457 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  206/  234], loss: 0.810, per_step_time: 3174ms, lr: 4.0304876e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:49,457 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   57.6% |                      | 5.04091 samples/s/p  0:26:14 }
2024-07-06 21:45:55,809 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  208/  234], loss: 0.790, per_step_time: 3172ms, lr: 4.0033476e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:45:55,809 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   57.8% |                      | 5.04344 samples/s/p  0:26:07 }
2024-07-06 21:46:02,159 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  210/  234], loss: 0.837, per_step_time: 3171ms, lr: 3.9762395e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:02,160 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   57.9% |                      | 5.04472 samples/s/p  0:26:00 }
2024-07-06 21:46:08,511 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  212/  234], loss: 0.925, per_step_time: 3172ms, lr: 3.949163e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:08,511 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   58.1% |                     | 5.04368 samples/s/p  0:25:54 }
2024-07-06 21:46:14,870 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  214/  234], loss: 1.059, per_step_time: 3175ms, lr: 3.9221177e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:14,870 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   58.3% |                     | 5.03792 samples/s/p  0:25:49 }
2024-07-06 21:46:21,221 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  216/  234], loss: 0.734, per_step_time: 3172ms, lr: 3.8951064e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:21,222 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   58.5% |                     | 5.04406 samples/s/p  0:25:41 }
2024-07-06 21:46:27,573 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  218/  234], loss: 0.874, per_step_time: 3171ms, lr: 3.868128e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:27,573 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   58.6% |                     | 5.04430 samples/s/p  0:25:35 }
2024-07-06 21:46:33,925 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  220/  234], loss: 0.859, per_step_time: 3172ms, lr: 3.841185e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:33,925 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   58.8% |                     | 5.04305 samples/s/p  0:25:29 }
2024-07-06 21:46:40,277 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  222/  234], loss: 0.684, per_step_time: 3172ms, lr: 3.8142778e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:40,278 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   59.0% |                     | 5.04322 samples/s/p  0:25:22 }
2024-07-06 21:46:46,631 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  224/  234], loss: 0.745, per_step_time: 3173ms, lr: 3.787406e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:46,632 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   59.1% |                     | 5.04210 samples/s/p  0:25:16 }
2024-07-06 21:46:52,986 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  226/  234], loss: 0.877, per_step_time: 3173ms, lr: 3.7605718e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:52,986 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   59.3% |                     | 5.04169 samples/s/p  0:25:10 }
2024-07-06 21:46:59,343 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  228/  234], loss: 0.929, per_step_time: 3174ms, lr: 3.7337762e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:46:59,343 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   59.5% |                     | 5.03993 samples/s/p  0:25:04 }
2024-07-06 21:47:05,696 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  230/  234], loss: 0.870, per_step_time: 3173ms, lr: 3.7070182e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:05,697 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   59.7% |                     | 5.04214 samples/s/p  0:24:57 }
2024-07-06 21:47:12,056 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  232/  234], loss: 0.938, per_step_time: 3176ms, lr: 3.6803e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:12,057 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   59.8% |                     | 5.03729 samples/s/p  0:24:52 }
2024-07-06 21:47:18,410 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  3/  5], step:[  234/  234], loss: 1.002, per_step_time: 3173ms, lr: 3.6536225e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:18,410 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   60.0% |                    | 5.04225 samples/s/p  0:24:45 }
2024-07-06 21:47:24,760 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[    2/  234], loss: 0.900, per_step_time: 3171ms, lr: 3.626987e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:24,760 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   60.2% |                    | 5.04495 samples/s/p  0:24:37 }
2024-07-06 21:47:31,113 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[    4/  234], loss: 0.922, per_step_time: 3173ms, lr: 3.6003927e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:31,113 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   60.3% |                    | 5.04249 samples/s/p  0:24:32 }
2024-07-06 21:47:37,466 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[    6/  234], loss: 0.870, per_step_time: 3173ms, lr: 3.5738412e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:37,467 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   60.5% |                    | 5.04234 samples/s/p  0:24:25 }
2024-07-06 21:47:43,816 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[    8/  234], loss: 0.799, per_step_time: 3171ms, lr: 3.5473346e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:43,816 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   60.7% |                    | 5.04525 samples/s/p  0:24:18 }
2024-07-06 21:47:50,166 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   10/  234], loss: 0.911, per_step_time: 3171ms, lr: 3.520872e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:50,167 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   60.9% |                    | 5.04488 samples/s/p  0:24:12 }
2024-07-06 21:47:56,524 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   12/  234], loss: 1.019, per_step_time: 3175ms, lr: 3.4944533e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:47:56,524 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   61.0% |                    | 5.03892 samples/s/p  0:24:07 }
2024-07-06 21:48:02,878 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   14/  234], loss: 0.923, per_step_time: 3173ms, lr: 3.4680827e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:02,879 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   61.2% |                    | 5.04206 samples/s/p  0:24:00 }
2024-07-06 21:48:09,233 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   16/  234], loss: 0.956, per_step_time: 3173ms, lr: 3.441758e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:09,233 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   61.4% |                    | 5.04164 samples/s/p  0:23:54 }
2024-07-06 21:48:15,584 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   18/  234], loss: 1.060, per_step_time: 3172ms, lr: 3.415481e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:15,584 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   61.5% |                    | 5.04402 samples/s/p  0:23:47 }
2024-07-06 21:48:21,936 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   20/  234], loss: 1.067, per_step_time: 3172ms, lr: 3.3892535e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:21,937 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   61.7% |                    | 5.04321 samples/s/p  0:23:41 }
2024-07-06 21:48:28,294 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   22/  234], loss: 0.988, per_step_time: 3175ms, lr: 3.363074e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:28,294 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   61.9% |                    | 5.03909 samples/s/p  0:23:36 }
2024-07-06 21:48:34,653 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   24/  234], loss: 1.052, per_step_time: 3175ms, lr: 3.3369455e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:34,653 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   62.1% |                   | 5.03804 samples/s/p  0:23:30 }
2024-07-06 21:48:41,006 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   26/  234], loss: 0.616, per_step_time: 3173ms, lr: 3.3108685e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:41,007 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   62.2% |                   | 5.04251 samples/s/p  0:23:22 }
2024-07-06 21:48:47,365 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   28/  234], loss: 0.917, per_step_time: 3175ms, lr: 3.284842e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:47,365 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   62.4% |                   | 5.03828 samples/s/p  0:23:17 }
2024-07-06 21:48:53,721 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   30/  234], loss: 1.226, per_step_time: 3174ms, lr: 3.258869e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:48:53,721 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   62.6% |                   | 5.04040 samples/s/p  0:23:10 }
2024-07-06 21:49:00,075 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   32/  234], loss: 0.915, per_step_time: 3173ms, lr: 3.2329488e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:00,076 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   62.7% |                   | 5.04129 samples/s/p  0:23:03 }
2024-07-06 21:49:06,427 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   34/  234], loss: 0.598, per_step_time: 3172ms, lr: 3.2070828e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:06,428 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   62.9% |                   | 5.04325 samples/s/p  0:22:56 }
2024-07-06 21:49:12,778 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   36/  234], loss: 0.788, per_step_time: 3171ms, lr: 3.1812728e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:12,779 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   63.1% |                   | 5.04426 samples/s/p  0:22:50 }
2024-07-06 21:49:19,133 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   38/  234], loss: 0.826, per_step_time: 3173ms, lr: 3.155517e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:19,133 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   63.2% |                   | 5.04136 samples/s/p  0:22:44 }
2024-07-06 21:49:25,484 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   40/  234], loss: 0.949, per_step_time: 3172ms, lr: 3.1298183e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:25,485 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   63.4% |                   | 5.04389 samples/s/p  0:22:37 }
2024-07-06 21:49:31,839 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   42/  234], loss: 0.874, per_step_time: 3173ms, lr: 3.1041778e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:31,839 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   63.6% |                   | 5.04150 samples/s/p  0:22:31 }
2024-07-06 21:49:38,195 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   44/  234], loss: 0.964, per_step_time: 3174ms, lr: 3.0785937e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:38,196 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   63.8% |                   | 5.04063 samples/s/p  0:22:25 }
2024-07-06 21:49:44,548 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   46/  234], loss: 0.793, per_step_time: 3172ms, lr: 3.0530697e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:44,549 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   63.9% |                   | 5.04255 samples/s/p  0:22:19 }
2024-07-06 21:49:50,900 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   48/  234], loss: 0.687, per_step_time: 3172ms, lr: 3.0276042e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:50,901 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   64.1% |                  | 5.04362 samples/s/p  0:22:12 }
2024-07-06 21:49:57,256 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   50/  234], loss: 1.145, per_step_time: 3174ms, lr: 3.0022007e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:49:57,256 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   64.3% |                  | 5.04077 samples/s/p  0:22:06 }
2024-07-06 21:50:03,610 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   52/  234], loss: 1.089, per_step_time: 3173ms, lr: 2.9768576e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:03,610 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   64.4% |                  | 5.04191 samples/s/p  0:22:00 }
2024-07-06 21:50:09,967 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   54/  234], loss: 1.233, per_step_time: 3174ms, lr: 2.951576e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:09,968 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   64.6% |                  | 5.03962 samples/s/p  0:21:54 }
2024-07-06 21:50:16,321 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   56/  234], loss: 0.894, per_step_time: 3173ms, lr: 2.9263585e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:16,321 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   64.8% |                  | 5.04249 samples/s/p  0:21:47 }
2024-07-06 21:50:22,675 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   58/  234], loss: 0.837, per_step_time: 3173ms, lr: 2.9012035e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:22,675 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   65.0% |                  | 5.04178 samples/s/p  0:21:41 }
2024-07-06 21:50:29,030 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   60/  234], loss: 0.996, per_step_time: 3174ms, lr: 2.8761136e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:29,031 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   65.1% |                  | 5.04062 samples/s/p  0:21:35 }
2024-07-06 21:50:35,384 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   62/  234], loss: 0.879, per_step_time: 3173ms, lr: 2.851088e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:35,384 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   65.3% |                  | 5.04209 samples/s/p  0:21:28 }
2024-07-06 21:50:41,739 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   64/  234], loss: 0.673, per_step_time: 3173ms, lr: 2.8261276e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:41,739 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   65.5% |                  | 5.04142 samples/s/p  0:21:22 }
2024-07-06 21:50:48,090 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   66/  234], loss: 0.644, per_step_time: 3171ms, lr: 2.8012359e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:48,090 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   65.6% |                  | 5.04420 samples/s/p  0:21:15 }
2024-07-06 21:50:54,441 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   68/  234], loss: 0.947, per_step_time: 3172ms, lr: 2.7764105e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:50:54,441 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   65.8% |                  | 5.04384 samples/s/p  0:21:08 }
2024-07-06 21:51:00,796 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   70/  234], loss: 1.055, per_step_time: 3173ms, lr: 2.7516526e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:00,796 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   66.0% |                  | 5.04131 samples/s/p  0:21:03 }
2024-07-06 21:51:07,149 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   72/  234], loss: 0.905, per_step_time: 3173ms, lr: 2.7269649e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:07,149 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   66.2% |                 | 5.04246 samples/s/p  0:20:56 }
2024-07-06 21:51:13,506 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   74/  234], loss: 1.047, per_step_time: 3175ms, lr: 2.7023463e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:13,506 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   66.3% |                 | 5.03936 samples/s/p  0:20:50 }
2024-07-06 21:51:19,859 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   76/  234], loss: 0.939, per_step_time: 3172ms, lr: 2.677797e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:19,859 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   66.5% |                 | 5.04285 samples/s/p  0:20:43 }
2024-07-06 21:51:26,210 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   78/  234], loss: 0.925, per_step_time: 3171ms, lr: 2.653321e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:26,210 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   66.7% |                 | 5.04426 samples/s/p  0:20:37 }
2024-07-06 21:51:32,563 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   80/  234], loss: 1.037, per_step_time: 3173ms, lr: 2.6289146e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:32,564 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   66.8% |                 | 5.04208 samples/s/p  0:20:31 }
2024-07-06 21:51:38,916 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   82/  234], loss: 0.915, per_step_time: 3172ms, lr: 2.6045831e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:38,916 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   67.0% |                 | 5.04372 samples/s/p  0:20:24 }
2024-07-06 21:51:45,271 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   84/  234], loss: 0.850, per_step_time: 3173ms, lr: 2.5803243e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:45,271 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   67.2% |                 | 5.04103 samples/s/p  0:20:18 }
2024-07-06 21:51:51,626 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   86/  234], loss: 0.962, per_step_time: 3173ms, lr: 2.5561383e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:51,626 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   67.4% |                 | 5.04116 samples/s/p  0:20:12 }
2024-07-06 21:51:57,980 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   88/  234], loss: 1.009, per_step_time: 3173ms, lr: 2.532029e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:51:57,981 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   67.5% |                 | 5.04161 samples/s/p  0:20:05 }
2024-07-06 21:52:04,339 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   90/  234], loss: 0.772, per_step_time: 3175ms, lr: 2.5079948e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:04,339 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   67.7% |                 | 5.03829 samples/s/p  0:20:00 }
2024-07-06 21:52:10,696 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   92/  234], loss: 1.108, per_step_time: 3174ms, lr: 2.4840354e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:10,697 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   67.9% |                 | 5.03973 samples/s/p  0:19:53 }
2024-07-06 21:52:17,055 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   94/  234], loss: 0.805, per_step_time: 3175ms, lr: 2.4601557e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:17,055 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   68.0% |                | 5.03817 samples/s/p  0:19:47 }
2024-07-06 21:52:23,411 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   96/  234], loss: 0.972, per_step_time: 3174ms, lr: 2.4363515e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:23,412 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   68.2% |                | 5.03979 samples/s/p  0:19:41 }
2024-07-06 21:52:29,763 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[   98/  234], loss: 0.769, per_step_time: 3172ms, lr: 2.412627e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:29,764 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   68.4% |                | 5.04343 samples/s/p  0:19:33 }
2024-07-06 21:52:36,122 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  100/  234], loss: 0.849, per_step_time: 3175ms, lr: 2.3889826e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:36,123 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   68.5% |                | 5.03790 samples/s/p  0:19:28 }
2024-07-06 21:52:42,480 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  102/  234], loss: 1.138, per_step_time: 3175ms, lr: 2.3654165e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:42,481 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   68.7% |                | 5.03840 samples/s/p  0:19:22 }
2024-07-06 21:52:48,833 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  104/  234], loss: 0.766, per_step_time: 3172ms, lr: 2.3419318e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:48,833 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   68.9% |                | 5.04328 samples/s/p  0:19:14 }
2024-07-06 21:52:55,188 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  106/  234], loss: 0.981, per_step_time: 3174ms, lr: 2.3185285e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:52:55,189 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   69.1% |                | 5.04046 samples/s/p  0:19:09 }
2024-07-06 21:53:01,540 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  108/  234], loss: 0.953, per_step_time: 3172ms, lr: 2.295208e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:01,541 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   69.2% |                | 5.04375 samples/s/p  0:19:02 }
2024-07-06 21:53:07,892 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  110/  234], loss: 0.716, per_step_time: 3172ms, lr: 2.2719698e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:07,892 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   69.4% |                | 5.04352 samples/s/p  0:18:55 }
2024-07-06 21:53:14,245 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  112/  234], loss: 1.120, per_step_time: 3172ms, lr: 2.2488155e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:14,245 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   69.6% |                | 5.04283 samples/s/p  0:18:49 }
2024-07-06 21:53:20,597 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  114/  234], loss: 1.029, per_step_time: 3172ms, lr: 2.2257453e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:20,597 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   69.7% |                | 5.04351 samples/s/p  0:18:43 }
2024-07-06 21:53:26,952 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  116/  234], loss: 0.861, per_step_time: 3173ms, lr: 2.2027602e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:26,953 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   69.9% |                | 5.04118 samples/s/p  0:18:37 }
2024-07-06 21:53:33,306 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  118/  234], loss: 0.870, per_step_time: 3173ms, lr: 2.179861e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:33,307 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   70.1% |               | 5.04202 samples/s/p  0:18:30 }
2024-07-06 21:53:39,664 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  120/  234], loss: 0.828, per_step_time: 3175ms, lr: 2.1570479e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:39,664 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   70.3% |               | 5.03922 samples/s/p  0:18:24 }
2024-07-06 21:53:46,018 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  122/  234], loss: 0.908, per_step_time: 3173ms, lr: 2.1343221e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:46,019 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   70.4% |               | 5.04127 samples/s/p  0:18:18 }
2024-07-06 21:53:52,375 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  124/  234], loss: 1.013, per_step_time: 3174ms, lr: 2.1116843e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:52,375 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   70.6% |               | 5.04011 samples/s/p  0:18:12 }
2024-07-06 21:53:58,726 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  126/  234], loss: 0.861, per_step_time: 3172ms, lr: 2.0891346e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:53:58,727 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   70.8% |               | 5.04400 samples/s/p  0:18:04 }
2024-07-06 21:54:05,080 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  128/  234], loss: 0.901, per_step_time: 3173ms, lr: 2.0666745e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:05,080 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   70.9% |               | 5.04242 samples/s/p  0:17:58 }
2024-07-06 21:54:11,441 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  130/  234], loss: 0.954, per_step_time: 3176ms, lr: 2.044304e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:11,441 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   71.1% |               | 5.03624 samples/s/p  0:17:53 }
2024-07-06 21:54:17,795 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  132/  234], loss: 0.818, per_step_time: 3173ms, lr: 2.0220243e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:17,795 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   71.3% |               | 5.04234 samples/s/p  0:17:46 }
2024-07-06 21:54:24,151 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  134/  234], loss: 0.704, per_step_time: 3174ms, lr: 1.9998357e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:24,151 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   71.5% |               | 5.04023 samples/s/p  0:17:40 }
2024-07-06 21:54:30,506 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  136/  234], loss: 0.811, per_step_time: 3174ms, lr: 1.977739e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:30,507 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   71.6% |               | 5.04058 samples/s/p  0:17:33 }
2024-07-06 21:54:36,863 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  138/  234], loss: 0.893, per_step_time: 3174ms, lr: 1.955735e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:36,863 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   71.8% |               | 5.03968 samples/s/p  0:17:27 }
2024-07-06 21:54:43,216 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  140/  234], loss: 0.876, per_step_time: 3172ms, lr: 1.9338244e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:43,216 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   72.0% |               | 5.04292 samples/s/p  0:17:20 }
2024-07-06 21:54:49,567 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  142/  234], loss: 0.961, per_step_time: 3172ms, lr: 1.9120078e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:49,568 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   72.1% |              | 5.04367 samples/s/p  0:17:14 }
2024-07-06 21:54:55,923 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  144/  234], loss: 1.071, per_step_time: 3174ms, lr: 1.8902856e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:54:55,924 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   72.3% |              | 5.04005 samples/s/p  0:17:08 }
2024-07-06 21:55:02,276 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  146/  234], loss: 0.699, per_step_time: 3172ms, lr: 1.868659e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:02,277 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   72.5% |              | 5.04296 samples/s/p  0:17:01 }
2024-07-06 21:55:08,628 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  148/  234], loss: 0.942, per_step_time: 3172ms, lr: 1.8471279e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:08,628 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   72.6% |              | 5.04392 samples/s/p  0:16:55 }
2024-07-06 21:55:14,995 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  150/  234], loss: 0.974, per_step_time: 3180ms, lr: 1.8256938e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:14,996 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   72.8% |              | 5.03123 samples/s/p  0:16:51 }
2024-07-06 21:55:21,348 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  152/  234], loss: 1.011, per_step_time: 3172ms, lr: 1.8043569e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:21,348 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   73.0% |              | 5.04285 samples/s/p  0:16:42 }
2024-07-06 21:55:27,702 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  154/  234], loss: 0.770, per_step_time: 3173ms, lr: 1.783117e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:27,703 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   73.2% |              | 5.04150 samples/s/p  0:16:36 }
2024-07-06 21:55:34,056 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  156/  234], loss: 0.963, per_step_time: 3172ms, lr: 1.7619774e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:34,057 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   73.3% |              | 5.04258 samples/s/p  0:16:29 }
2024-07-06 21:55:40,409 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  158/  234], loss: 0.862, per_step_time: 3173ms, lr: 1.7409362e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:40,410 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   73.5% |              | 5.04231 samples/s/p  0:16:23 }
2024-07-06 21:55:46,762 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  160/  234], loss: 0.920, per_step_time: 3172ms, lr: 1.7199942e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:46,762 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   73.7% |              | 5.04367 samples/s/p  0:16:17 }
2024-07-06 21:55:53,120 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  162/  234], loss: 0.909, per_step_time: 3175ms, lr: 1.6991544e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:53,121 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   73.8% |              | 5.03877 samples/s/p  0:16:11 }
2024-07-06 21:55:59,473 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  164/  234], loss: 1.074, per_step_time: 3172ms, lr: 1.6784146e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:55:59,473 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   74.0% |             | 5.04296 samples/s/p  0:16:04 }
2024-07-06 21:56:05,827 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  166/  234], loss: 0.658, per_step_time: 3173ms, lr: 1.6577758e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:05,828 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   74.2% |             | 5.04148 samples/s/p  0:15:58 }
2024-07-06 21:56:12,185 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  168/  234], loss: 0.885, per_step_time: 3175ms, lr: 1.6372413e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:12,185 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   74.4% |             | 5.03916 samples/s/p  0:15:52 }
2024-07-06 21:56:18,541 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  170/  234], loss: 0.950, per_step_time: 3174ms, lr: 1.6168082e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:18,541 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   74.5% |             | 5.04052 samples/s/p  0:15:45 }
2024-07-06 21:56:24,893 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  172/  234], loss: 0.967, per_step_time: 3172ms, lr: 1.5964793e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:24,893 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   74.7% |             | 5.04353 samples/s/p  0:15:39 }
2024-07-06 21:56:31,251 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  174/  234], loss: 0.909, per_step_time: 3175ms, lr: 1.5762558e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:31,251 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   74.9% |             | 5.03878 samples/s/p  0:15:33 }
2024-07-06 21:56:37,603 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  176/  234], loss: 0.880, per_step_time: 3172ms, lr: 1.5561356e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:37,604 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   75.0% |             | 5.04295 samples/s/p  0:15:26 }
2024-07-06 21:56:43,956 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  178/  234], loss: 0.933, per_step_time: 3172ms, lr: 1.5361225e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:43,956 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   75.2% |             | 5.04286 samples/s/p  0:15:20 }
2024-07-06 21:56:50,308 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  180/  234], loss: 0.849, per_step_time: 3172ms, lr: 1.5162149e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:50,309 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   75.4% |             | 5.04326 samples/s/p  0:15:13 }
2024-07-06 21:56:56,662 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  182/  234], loss: 0.930, per_step_time: 3173ms, lr: 1.4964127e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:56:56,663 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   75.6% |             | 5.04177 samples/s/p  0:15:07 }
2024-07-06 21:57:03,019 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  184/  234], loss: 0.821, per_step_time: 3174ms, lr: 1.4767199e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:03,019 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   75.7% |             | 5.03997 samples/s/p  0:15:01 }
2024-07-06 21:57:09,373 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  186/  234], loss: 1.138, per_step_time: 3173ms, lr: 1.4571336e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:09,373 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   75.9% |             | 5.04163 samples/s/p  0:14:54 }
2024-07-06 21:57:15,730 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  188/  234], loss: 0.846, per_step_time: 3174ms, lr: 1.4376566e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:15,730 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   76.1% |            | 5.03947 samples/s/p  0:14:48 }
2024-07-06 21:57:22,083 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  190/  234], loss: 0.893, per_step_time: 3172ms, lr: 1.4182895e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:22,083 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   76.2% |            | 5.04256 samples/s/p  0:14:42 }
2024-07-06 21:57:28,436 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  192/  234], loss: 0.908, per_step_time: 3172ms, lr: 1.3990303e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:28,436 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   76.4% |            | 5.04263 samples/s/p  0:14:35 }
2024-07-06 21:57:34,789 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  194/  234], loss: 0.745, per_step_time: 3173ms, lr: 1.3798826e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:34,790 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   76.6% |            | 5.04230 samples/s/p  0:14:29 }
2024-07-06 21:57:41,146 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  196/  234], loss: 0.819, per_step_time: 3174ms, lr: 1.36084645e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:41,147 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   76.8% |            | 5.03938 samples/s/p  0:14:23 }
2024-07-06 21:57:47,502 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  198/  234], loss: 1.064, per_step_time: 3174ms, lr: 1.3419205e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:47,503 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   76.9% |            | 5.04035 samples/s/p  0:14:17 }
2024-07-06 21:57:53,857 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  200/  234], loss: 1.060, per_step_time: 3174ms, lr: 1.3231069e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:57:53,858 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   77.1% |            | 5.04078 samples/s/p  0:14:10 }
2024-07-06 21:58:00,217 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  202/  234], loss: 0.854, per_step_time: 3176ms, lr: 1.3044065e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:00,217 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   77.3% |            | 5.03747 samples/s/p  0:14:04 }
2024-07-06 21:58:06,569 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  204/  234], loss: 1.033, per_step_time: 3172ms, lr: 1.2858194e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:06,570 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   77.4% |            | 5.04361 samples/s/p  0:13:57 }
2024-07-06 21:58:12,924 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  206/  234], loss: 0.960, per_step_time: 3173ms, lr: 1.26734585e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:12,924 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   77.6% |            | 5.04164 samples/s/p  0:13:51 }
2024-07-06 21:58:19,277 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  208/  234], loss: 1.026, per_step_time: 3173ms, lr: 1.2489867e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:19,277 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   77.8% |            | 5.04240 samples/s/p  0:13:45 }
2024-07-06 21:58:25,630 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  210/  234], loss: 0.761, per_step_time: 3172ms, lr: 1.2307426e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:25,630 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   77.9% |            | 5.04282 samples/s/p  0:13:38 }
2024-07-06 21:58:31,982 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  212/  234], loss: 0.855, per_step_time: 3172ms, lr: 1.2126138e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:31,982 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   78.1% |           | 5.04397 samples/s/p  0:13:32 }
2024-07-06 21:58:38,339 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  214/  234], loss: 1.036, per_step_time: 3175ms, lr: 1.1946013e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:38,340 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   78.3% |           | 5.03905 samples/s/p  0:13:26 }
2024-07-06 21:58:44,692 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  216/  234], loss: 0.968, per_step_time: 3173ms, lr: 1.1767053e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:44,693 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   78.5% |           | 5.04255 samples/s/p  0:13:19 }
2024-07-06 21:58:51,048 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  218/  234], loss: 0.909, per_step_time: 3174ms, lr: 1.1589264e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:51,048 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   78.6% |           | 5.04094 samples/s/p  0:13:13 }
2024-07-06 21:58:57,402 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  220/  234], loss: 0.819, per_step_time: 3173ms, lr: 1.1412656e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:58:57,403 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   78.8% |           | 5.04182 samples/s/p  0:13:07 }
2024-07-06 21:59:03,758 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  222/  234], loss: 0.887, per_step_time: 3174ms, lr: 1.1237227e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:03,758 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   79.0% |           | 5.04074 samples/s/p  0:13:00 }
2024-07-06 21:59:10,113 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  224/  234], loss: 0.853, per_step_time: 3174ms, lr: 1.1062988e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:10,113 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   79.1% |           | 5.04079 samples/s/p  0:12:54 }
2024-07-06 21:59:16,468 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  226/  234], loss: 0.771, per_step_time: 3173ms, lr: 1.0889941e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:16,468 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   79.3% |           | 5.04119 samples/s/p  0:12:48 }
2024-07-06 21:59:22,823 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  228/  234], loss: 0.748, per_step_time: 3174ms, lr: 1.0718092e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:22,823 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   79.5% |           | 5.04093 samples/s/p  0:12:41 }
2024-07-06 21:59:29,175 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  230/  234], loss: 0.842, per_step_time: 3172ms, lr: 1.0547447e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:29,175 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   79.7% |           | 5.04364 samples/s/p  0:12:35 }
2024-07-06 21:59:35,547 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  232/  234], loss: 0.883, per_step_time: 3182ms, lr: 1.0378012e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:35,548 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   79.8% |           | 5.02686 samples/s/p  0:12:31 }
2024-07-06 21:59:41,901 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  4/  5], step:[  234/  234], loss: 0.515, per_step_time: 3173ms, lr: 1.0209793e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:41,902 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   80.0% |          | 5.04191 samples/s/p  0:12:22 }
2024-07-06 21:59:48,257 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[    2/  234], loss: 1.115, per_step_time: 3174ms, lr: 1.0042786e-05, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:48,257 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   80.2% |          | 5.04065 samples/s/p  0:12:16 }
2024-07-06 21:59:54,615 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[    4/  234], loss: 0.960, per_step_time: 3175ms, lr: 9.877011e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 21:59:54,616 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   80.3% |          | 5.03818 samples/s/p  0:12:10 }
2024-07-06 22:00:00,974 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[    6/  234], loss: 0.957, per_step_time: 3175ms, lr: 9.712463e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:00,975 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   80.5% |          | 5.03786 samples/s/p  0:12:04 }
2024-07-06 22:00:07,326 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[    8/  234], loss: 0.908, per_step_time: 3172ms, lr: 9.54915e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:07,327 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   80.7% |          | 5.04383 samples/s/p  0:11:56 }
2024-07-06 22:00:13,682 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   10/  234], loss: 1.002, per_step_time: 3173ms, lr: 9.387069e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:13,682 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   80.9% |          | 5.04096 samples/s/p  0:11:50 }
2024-07-06 22:00:20,034 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   12/  234], loss: 0.620, per_step_time: 3172ms, lr: 9.226247e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:20,035 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   81.0% |          | 5.04312 samples/s/p  0:11:44 }
2024-07-06 22:00:26,387 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   14/  234], loss: 0.894, per_step_time: 3172ms, lr: 9.066668e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:26,388 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   81.2% |          | 5.04368 samples/s/p  0:11:37 }
2024-07-06 22:00:32,738 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   16/  234], loss: 0.715, per_step_time: 3171ms, lr: 8.908334e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:32,739 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   81.4% |          | 5.04442 samples/s/p  0:11:31 }
2024-07-06 22:00:39,090 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   18/  234], loss: 0.864, per_step_time: 3172ms, lr: 8.751279e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:39,091 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   81.5% |          | 5.04343 samples/s/p  0:11:25 }
2024-07-06 22:00:45,443 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   20/  234], loss: 1.030, per_step_time: 3172ms, lr: 8.595475e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:45,443 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   81.7% |          | 5.04310 samples/s/p  0:11:18 }
2024-07-06 22:00:51,792 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   22/  234], loss: 0.778, per_step_time: 3170ms, lr: 8.440936e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:51,792 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   81.9% |          | 5.04624 samples/s/p  0:11:12 }
2024-07-06 22:00:58,145 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   24/  234], loss: 0.941, per_step_time: 3172ms, lr: 8.2876795e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:00:58,145 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   82.1% |         | 5.04288 samples/s/p  0:11:06 }
2024-07-06 22:01:04,503 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   26/  234], loss: 0.945, per_step_time: 3175ms, lr: 8.135697e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:04,503 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   82.2% |         | 5.03843 samples/s/p  0:11:00 }
2024-07-06 22:01:10,862 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   28/  234], loss: 0.884, per_step_time: 3175ms, lr: 7.984996e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:10,862 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   82.4% |         | 5.03787 samples/s/p  0:10:54 }
2024-07-06 22:01:17,214 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   30/  234], loss: 0.966, per_step_time: 3172ms, lr: 7.835588e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:17,215 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   82.6% |         | 5.04310 samples/s/p  0:10:47 }
2024-07-06 22:01:23,571 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   32/  234], loss: 1.092, per_step_time: 3174ms, lr: 7.687468e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:23,571 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   82.7% |         | 5.03988 samples/s/p  0:10:41 }
2024-07-06 22:01:29,925 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   34/  234], loss: 0.713, per_step_time: 3173ms, lr: 7.540646e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:29,926 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   82.9% |         | 5.04151 samples/s/p  0:10:34 }
2024-07-06 22:01:36,279 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   36/  234], loss: 1.065, per_step_time: 3173ms, lr: 7.3951333e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:36,280 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   83.1% |         | 5.04159 samples/s/p  0:10:28 }
2024-07-06 22:01:42,633 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   38/  234], loss: 0.886, per_step_time: 3173ms, lr: 7.2509047e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:42,633 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   83.2% |         | 5.04235 samples/s/p  0:10:21 }
2024-07-06 22:01:48,989 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   40/  234], loss: 0.981, per_step_time: 3174ms, lr: 7.1080085e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:48,989 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   83.4% |         | 5.04016 samples/s/p  0:10:15 }
2024-07-06 22:01:55,345 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   42/  234], loss: 0.992, per_step_time: 3174ms, lr: 6.9664147e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:01:55,345 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   83.6% |         | 5.04006 samples/s/p  0:10:09 }
2024-07-06 22:02:01,700 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   44/  234], loss: 1.095, per_step_time: 3174ms, lr: 6.826141e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:01,701 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   83.8% |         | 5.04059 samples/s/p  0:10:03 }
2024-07-06 22:02:08,049 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   46/  234], loss: 0.891, per_step_time: 3170ms, lr: 6.687203e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:08,049 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   83.9% |         | 5.04629 samples/s/p  0:09:56 }
2024-07-06 22:02:14,400 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   48/  234], loss: 0.952, per_step_time: 3171ms, lr: 6.549573e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:14,400 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   84.1% |        | 5.04427 samples/s/p  0:09:49 }
2024-07-06 22:02:20,751 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   50/  234], loss: 0.688, per_step_time: 3171ms, lr: 6.4132837e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:20,751 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   84.3% |        | 5.04422 samples/s/p  0:09:43 }
2024-07-06 22:02:27,105 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   52/  234], loss: 0.707, per_step_time: 3173ms, lr: 6.278336e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:27,105 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   84.4% |        | 5.04199 samples/s/p  0:09:37 }
2024-07-06 22:02:33,456 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   54/  234], loss: 1.044, per_step_time: 3172ms, lr: 6.144714e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:33,457 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   84.6% |        | 5.04411 samples/s/p  0:09:30 }
2024-07-06 22:02:39,812 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   56/  234], loss: 1.043, per_step_time: 3174ms, lr: 6.0124457e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:39,812 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   84.8% |        | 5.04076 samples/s/p  0:09:24 }
2024-07-06 22:02:46,167 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   58/  234], loss: 0.810, per_step_time: 3173ms, lr: 5.881524e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:46,167 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   85.0% |        | 5.04135 samples/s/p  0:09:18 }
2024-07-06 22:02:52,519 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   60/  234], loss: 1.109, per_step_time: 3172ms, lr: 5.7519524e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:52,520 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   85.1% |        | 5.04279 samples/s/p  0:09:12 }
2024-07-06 22:02:58,872 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   62/  234], loss: 1.101, per_step_time: 3172ms, lr: 5.6237427e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:02:58,873 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   85.3% |        | 5.04335 samples/s/p  0:09:05 }
2024-07-06 22:03:05,230 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   64/  234], loss: 0.903, per_step_time: 3175ms, lr: 5.4968864e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:05,231 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   85.5% |        | 5.03845 samples/s/p  0:08:59 }
2024-07-06 22:03:11,582 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   66/  234], loss: 0.810, per_step_time: 3172ms, lr: 5.3713975e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:11,582 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   85.6% |        | 5.04389 samples/s/p  0:08:52 }
2024-07-06 22:03:17,937 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   68/  234], loss: 1.070, per_step_time: 3174ms, lr: 5.2472737e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:17,938 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   85.8% |        | 5.04077 samples/s/p  0:08:46 }
2024-07-06 22:03:24,296 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   70/  234], loss: 0.923, per_step_time: 3175ms, lr: 5.124521e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:24,297 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   86.0% |        | 5.03788 samples/s/p  0:08:40 }
2024-07-06 22:03:30,653 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   72/  234], loss: 0.959, per_step_time: 3174ms, lr: 5.003148e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:30,653 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   86.2% |       | 5.04012 samples/s/p  0:08:34 }
2024-07-06 22:03:37,011 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   74/  234], loss: 0.865, per_step_time: 3175ms, lr: 4.8831494e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:37,012 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   86.3% |       | 5.03825 samples/s/p  0:08:28 }
2024-07-06 22:03:43,366 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   76/  234], loss: 0.773, per_step_time: 3173ms, lr: 4.764536e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:43,366 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   86.5% |       | 5.04151 samples/s/p  0:08:21 }
2024-07-06 22:03:49,721 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   78/  234], loss: 1.048, per_step_time: 3173ms, lr: 4.6473083e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:49,721 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   86.7% |       | 5.04107 samples/s/p  0:08:15 }
2024-07-06 22:03:56,074 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   80/  234], loss: 0.919, per_step_time: 3172ms, lr: 4.5314728e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:03:56,074 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   86.8% |       | 5.04285 samples/s/p  0:08:08 }
2024-07-06 22:04:02,427 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   82/  234], loss: 0.865, per_step_time: 3172ms, lr: 4.417026e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:02,428 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   87.0% |       | 5.04286 samples/s/p  0:08:02 }
2024-07-06 22:04:08,782 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   84/  234], loss: 0.719, per_step_time: 3173ms, lr: 4.30398e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:08,782 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   87.2% |       | 5.04152 samples/s/p  0:07:56 }
2024-07-06 22:04:15,135 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   86/  234], loss: 0.661, per_step_time: 3173ms, lr: 4.1923345e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:15,136 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   87.4% |       | 5.04196 samples/s/p  0:07:49 }
2024-07-06 22:04:21,496 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   88/  234], loss: 1.108, per_step_time: 3176ms, lr: 4.0820864e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:21,496 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   87.5% |       | 5.03666 samples/s/p  0:07:43 }
2024-07-06 22:04:27,848 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   90/  234], loss: 0.921, per_step_time: 3172ms, lr: 3.9732486e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:27,849 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   87.7% |       | 5.04325 samples/s/p  0:07:36 }
2024-07-06 22:04:34,206 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   92/  234], loss: 1.077, per_step_time: 3175ms, lr: 3.865826e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:34,207 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   87.9% |       | 5.03912 samples/s/p  0:07:30 }
2024-07-06 22:04:40,558 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   94/  234], loss: 0.780, per_step_time: 3172ms, lr: 3.7598131e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:40,558 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   88.0% |      | 5.04393 samples/s/p  0:07:24 }
2024-07-06 22:04:46,912 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   96/  234], loss: 1.105, per_step_time: 3173ms, lr: 3.655219e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:46,913 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   88.2% |      | 5.04103 samples/s/p  0:07:18 }
2024-07-06 22:04:53,268 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[   98/  234], loss: 0.882, per_step_time: 3174ms, lr: 3.5520434e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:53,268 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   88.4% |      | 5.04091 samples/s/p  0:07:11 }
2024-07-06 22:04:59,622 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  100/  234], loss: 0.779, per_step_time: 3173ms, lr: 3.4502923e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:04:59,622 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   88.5% |      | 5.04168 samples/s/p  0:07:05 }
2024-07-06 22:05:05,977 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  102/  234], loss: 0.899, per_step_time: 3174ms, lr: 3.3499687e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:05,978 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   88.7% |      | 5.04080 samples/s/p  0:06:58 }
2024-07-06 22:05:12,334 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  104/  234], loss: 1.128, per_step_time: 3174ms, lr: 3.2510727e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:12,334 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   88.9% |      | 5.04011 samples/s/p  0:06:52 }
2024-07-06 22:05:18,687 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  106/  234], loss: 1.116, per_step_time: 3173ms, lr: 3.1536072e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:18,687 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   89.1% |      | 5.04225 samples/s/p  0:06:46 }
2024-07-06 22:05:25,041 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  108/  234], loss: 1.068, per_step_time: 3173ms, lr: 3.057584e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:25,042 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   89.2% |      | 5.04137 samples/s/p  0:06:39 }
2024-07-06 22:05:31,396 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  110/  234], loss: 0.899, per_step_time: 3173ms, lr: 2.9629946e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:31,397 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   89.4% |      | 5.04108 samples/s/p  0:06:33 }
2024-07-06 22:05:37,748 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  112/  234], loss: 1.070, per_step_time: 3172ms, lr: 2.8698444e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:37,749 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   89.6% |      | 5.04349 samples/s/p  0:06:27 }
2024-07-06 22:05:44,105 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  114/  234], loss: 0.868, per_step_time: 3174ms, lr: 2.7781487e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:44,105 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   89.7% |      | 5.03970 samples/s/p  0:06:20 }
2024-07-06 22:05:50,460 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  116/  234], loss: 0.865, per_step_time: 3173ms, lr: 2.6878893e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:50,460 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   89.9% |      | 5.04126 samples/s/p  0:06:14 }
2024-07-06 22:05:56,811 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  118/  234], loss: 0.680, per_step_time: 3172ms, lr: 2.5990814e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:05:56,812 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   90.1% |     | 5.04347 samples/s/p  0:06:08 }
2024-07-06 22:06:03,168 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  120/  234], loss: 0.887, per_step_time: 3174ms, lr: 2.5117338e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:03,169 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   90.3% |     | 5.03947 samples/s/p  0:06:01 }
2024-07-06 22:06:09,523 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  122/  234], loss: 0.778, per_step_time: 3173ms, lr: 2.4258316e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:09,524 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   90.4% |     | 5.04175 samples/s/p  0:05:55 }
2024-07-06 22:06:15,878 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  124/  234], loss: 0.623, per_step_time: 3173ms, lr: 2.3413925e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:15,878 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   90.6% |     | 5.04160 samples/s/p  0:05:49 }
2024-07-06 22:06:22,231 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  126/  234], loss: 0.926, per_step_time: 3172ms, lr: 2.258417e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:22,231 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   90.8% |     | 5.04289 samples/s/p  0:05:42 }
2024-07-06 22:06:28,584 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  128/  234], loss: 0.694, per_step_time: 3173ms, lr: 2.1768956e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:28,584 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   90.9% |     | 5.04224 samples/s/p  0:05:36 }
2024-07-06 22:06:34,936 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  130/  234], loss: 0.864, per_step_time: 3172ms, lr: 2.0968496e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:34,937 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   91.1% |     | 5.04325 samples/s/p  0:05:29 }
2024-07-06 22:06:41,294 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  132/  234], loss: 0.979, per_step_time: 3175ms, lr: 2.0182638e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:41,294 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   91.3% |     | 5.03910 samples/s/p  0:05:23 }
2024-07-06 22:06:47,647 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  134/  234], loss: 0.818, per_step_time: 3172ms, lr: 1.9411505e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:47,647 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   91.5% |     | 5.04288 samples/s/p  0:05:17 }
2024-07-06 22:06:54,002 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  136/  234], loss: 0.973, per_step_time: 3173ms, lr: 1.8655121e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:06:54,002 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   91.6% |     | 5.04100 samples/s/p  0:05:11 }
2024-07-06 22:06:54,003 - mindformers[mindformers/core/callback/callback.py:562] - INFO - ......Saving ckpt......
2024-07-06 22:07:29,151 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  138/  234], loss: 0.846, per_step_time: 3281ms, lr: 1.7913491e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:07:29,151 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   91.8% |     | 4.87641 samples/s/p  0:05:14 }
2024-07-06 22:07:35,506 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  140/  234], loss: 0.817, per_step_time: 3173ms, lr: 1.7186611e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:07:35,506 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   92.0% |     | 5.04104 samples/s/p  0:04:58 }
2024-07-06 22:07:41,859 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  142/  234], loss: 1.098, per_step_time: 3172ms, lr: 1.6474545e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:07:41,859 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   92.1% |    | 5.04274 samples/s/p  0:04:51 }
2024-07-06 22:07:48,211 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  144/  234], loss: 0.941, per_step_time: 3172ms, lr: 1.577729e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:07:48,211 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   92.3% |    | 5.04321 samples/s/p  0:04:45 }
2024-07-06 22:07:54,568 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  146/  234], loss: 0.866, per_step_time: 3174ms, lr: 1.5094876e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:07:54,568 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   92.5% |    | 5.03960 samples/s/p  0:04:39 }
2024-07-06 22:08:00,929 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  148/  234], loss: 0.929, per_step_time: 3177ms, lr: 1.4427304e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:00,929 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   92.6% |    | 5.03599 samples/s/p  0:04:33 }
2024-07-06 22:08:07,282 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  150/  234], loss: 0.869, per_step_time: 3172ms, lr: 1.3774663e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:07,282 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   92.8% |    | 5.04288 samples/s/p  0:04:26 }
2024-07-06 22:08:13,636 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  152/  234], loss: 0.791, per_step_time: 3173ms, lr: 1.3136863e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:13,637 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   93.0% |    | 5.04153 samples/s/p  0:04:20 }
2024-07-06 22:08:19,988 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  154/  234], loss: 1.003, per_step_time: 3172ms, lr: 1.2513995e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:19,988 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   93.2% |    | 5.04376 samples/s/p  0:04:13 }
2024-07-06 22:08:26,342 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  156/  234], loss: 0.876, per_step_time: 3173ms, lr: 1.1906146e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:26,343 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   93.3% |    | 5.04126 samples/s/p  0:04:07 }
2024-07-06 22:08:32,696 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  158/  234], loss: 0.905, per_step_time: 3173ms, lr: 1.1313169e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:32,697 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   93.5% |    | 5.04199 samples/s/p  0:04:01 }
2024-07-06 22:08:39,052 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  160/  234], loss: 1.000, per_step_time: 3174ms, lr: 1.0735214e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:39,053 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   93.7% |    | 5.04020 samples/s/p  0:03:54 }
2024-07-06 22:08:45,405 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  162/  234], loss: 1.092, per_step_time: 3172ms, lr: 1.0172217e-06, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:45,405 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   93.8% |    | 5.04286 samples/s/p  0:03:48 }
2024-07-06 22:08:51,761 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  164/  234], loss: 0.881, per_step_time: 3174ms, lr: 9.624183e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:51,762 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   94.0% |   | 5.04001 samples/s/p  0:03:42 }
2024-07-06 22:08:58,113 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  166/  234], loss: 1.051, per_step_time: 3172ms, lr: 9.091228e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:08:58,113 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   94.2% |   | 5.04381 samples/s/p  0:03:35 }
2024-07-06 22:09:04,466 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  168/  234], loss: 0.899, per_step_time: 3173ms, lr: 8.573383e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:04,467 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   94.4% |   | 5.04198 samples/s/p  0:03:29 }
2024-07-06 22:09:10,821 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  170/  234], loss: 0.989, per_step_time: 3173ms, lr: 8.0704984e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:10,821 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   94.5% |   | 5.04158 samples/s/p  0:03:23 }
2024-07-06 22:09:17,178 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  172/  234], loss: 0.833, per_step_time: 3175ms, lr: 7.582724e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:17,179 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   94.7% |   | 5.03897 samples/s/p  0:03:16 }
2024-07-06 22:09:23,535 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  174/  234], loss: 1.039, per_step_time: 3174ms, lr: 7.110059e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:23,535 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   94.9% |   | 5.04001 samples/s/p  0:03:10 }
2024-07-06 22:09:29,887 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  176/  234], loss: 0.864, per_step_time: 3172ms, lr: 6.6525337e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:29,887 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   95.0% |   | 5.04318 samples/s/p  0:03:04 }
2024-07-06 22:09:36,236 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  178/  234], loss: 0.825, per_step_time: 3171ms, lr: 6.2100287e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:36,236 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   95.2% |   | 5.04565 samples/s/p  0:02:57 }
2024-07-06 22:09:42,587 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  180/  234], loss: 0.874, per_step_time: 3171ms, lr: 5.782753e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:42,587 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   95.4% |   | 5.04498 samples/s/p  0:02:51 }
2024-07-06 22:09:48,938 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  182/  234], loss: 0.861, per_step_time: 3172ms, lr: 5.370557e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:48,939 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   95.6% |   | 5.04375 samples/s/p  0:02:44 }
2024-07-06 22:09:55,294 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  184/  234], loss: 1.013, per_step_time: 3174ms, lr: 4.973531e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:09:55,295 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   95.7% |   | 5.04058 samples/s/p  0:02:38 }
2024-07-06 22:10:01,673 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  186/  234], loss: 1.064, per_step_time: 3185ms, lr: 4.5916735e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:01,673 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   95.9% |   | 5.02257 samples/s/p  0:02:32 }
2024-07-06 22:10:08,027 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  188/  234], loss: 0.720, per_step_time: 3173ms, lr: 4.2250454e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:08,028 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   96.1% |  | 5.04135 samples/s/p  0:02:25 }
2024-07-06 22:10:14,378 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  190/  234], loss: 0.731, per_step_time: 3171ms, lr: 3.8735567e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:14,379 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   96.2% |  | 5.04425 samples/s/p  0:02:19 }
2024-07-06 22:10:20,742 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  192/  234], loss: 1.045, per_step_time: 3173ms, lr: 3.537327e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:20,742 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   96.4% |  | 5.04233 samples/s/p  0:02:13 }
2024-07-06 22:10:27,096 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  194/  234], loss: 0.971, per_step_time: 3173ms, lr: 3.2162367e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:27,097 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   96.6% |  | 5.04154 samples/s/p  0:02:06 }
2024-07-06 22:10:33,448 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  196/  234], loss: 0.772, per_step_time: 3172ms, lr: 2.9104052e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:33,448 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   96.8% |  | 5.04362 samples/s/p  0:02:00 }
2024-07-06 22:10:39,808 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  198/  234], loss: 0.931, per_step_time: 3174ms, lr: 2.619803e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:39,808 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   96.9% |  | 5.03999 samples/s/p  0:01:54 }
2024-07-06 22:10:46,164 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  200/  234], loss: 0.738, per_step_time: 3172ms, lr: 2.3444592e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:46,164 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   97.1% |  | 5.04293 samples/s/p  0:01:47 }
2024-07-06 22:10:52,540 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  202/  234], loss: 0.960, per_step_time: 3172ms, lr: 2.0843147e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:52,540 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   97.3% |  | 5.04301 samples/s/p  0:01:41 }
2024-07-06 22:10:58,900 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  204/  234], loss: 0.757, per_step_time: 3176ms, lr: 1.8395781e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:10:58,900 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   97.4% |  | 5.03718 samples/s/p  0:01:35 }
2024-07-06 22:11:05,260 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  206/  234], loss: 0.961, per_step_time: 3176ms, lr: 1.6098916e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:05,260 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   97.6% |  | 5.03722 samples/s/p  0:01:28 }
2024-07-06 22:11:11,616 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  208/  234], loss: 0.837, per_step_time: 3174ms, lr: 1.3956428e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:11,616 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   97.8% |  | 5.04020 samples/s/p  0:01:22 }
2024-07-06 22:11:17,977 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  210/  234], loss: 0.768, per_step_time: 3176ms, lr: 1.196593e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:17,977 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   97.9% |  | 5.03650 samples/s/p  0:01:16 }
2024-07-06 22:11:24,334 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  212/  234], loss: 0.993, per_step_time: 3175ms, lr: 1.0128617e-07, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:24,335 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   98.1% | | 5.03899 samples/s/p  0:01:09 }
2024-07-06 22:11:30,690 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  214/  234], loss: 0.919, per_step_time: 3174ms, lr: 8.444488e-08, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:30,690 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   98.3% | | 5.04079 samples/s/p  0:01:03 }
2024-07-06 22:11:37,044 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  216/  234], loss: 0.932, per_step_time: 3173ms, lr: 6.9126486e-08, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:37,045 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   98.5% | | 5.04148 samples/s/p  0:00:57 }
2024-07-06 22:11:43,396 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  218/  234], loss: 1.037, per_step_time: 3172ms, lr: 5.533993e-08, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:43,397 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   98.6% | | 5.04395 samples/s/p  0:00:50 }
2024-07-06 22:11:49,748 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  220/  234], loss: 0.554, per_step_time: 3172ms, lr: 4.3088196e-08, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:49,749 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   98.8% | | 5.04323 samples/s/p  0:00:44 }
2024-07-06 22:11:56,121 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  222/  234], loss: 1.074, per_step_time: 3182ms, lr: 3.236234e-08, overflow cond: False, loss_scale: 1.0
2024-07-06 22:11:56,121 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   99.0% | | 5.02707 samples/s/p  0:00:38 }
2024-07-06 22:12:02,478 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  224/  234], loss: 1.054, per_step_time: 3174ms, lr: 2.3174286e-08, overflow cond: False, loss_scale: 1.0
2024-07-06 22:12:02,478 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   99.1% | | 5.03965 samples/s/p  0:00:31 }
2024-07-06 22:12:08,828 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  226/  234], loss: 0.863, per_step_time: 3171ms, lr: 1.5509128e-08, overflow cond: False, loss_scale: 1.0
2024-07-06 22:12:08,829 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   99.3% | | 5.04455 samples/s/p  0:00:25 }
2024-07-06 22:12:15,183 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  228/  234], loss: 1.066, per_step_time: 3173ms, lr: 9.381771e-09, overflow cond: False, loss_scale: 1.0
2024-07-06 22:12:15,184 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   99.5% | | 5.04124 samples/s/p  0:00:19 }
2024-07-06 22:12:21,537 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  230/  234], loss: 1.055, per_step_time: 3173ms, lr: 4.789233e-09, overflow cond: False, loss_scale: 1.0
2024-07-06 22:12:21,538 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   99.7% | | 5.04175 samples/s/p  0:00:12 }
2024-07-06 22:12:27,890 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  232/  234], loss: 0.785, per_step_time: 3172ms, lr: 1.7195939e-09, overflow cond: False, loss_scale: 1.0
2024-07-06 22:12:27,890 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   99.8% | | 5.04348 samples/s/p  0:00:06 }
2024-07-06 22:12:34,241 - mindformers[mindformers/core/callback/callback.py:316] - INFO - { Epoch:[  5/  5], step:[  234/  234], loss: 1.086, per_step_time: 3171ms, lr: 1.9371509e-10, overflow cond: False, loss_scale: 1.0
2024-07-06 22:12:34,241 - mindformers[mindformers/core/callback/callback.py:326] - INFO -   100.0% || 5.04426 samples/s/p  0:00:00 }
2024-07-06 22:12:34,249 - mindformers[mindformers/core/callback/callback.py:562] - INFO - ......Saving ckpt......
2024-07-06 22:13:01,972 - mindformers[mindformers/trainer/base_trainer.py:779] - INFO - .........Training Over!.............
