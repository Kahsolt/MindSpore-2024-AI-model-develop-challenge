/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  setattr(self, word, getattr(machar, word).flat[0])
/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.
  return self._float_to_str(self.smallest_subnormal)
********************** infer list len:  200
2024-07-06 22:54:33,832 - mindformers[mindformers/trainer/trainer.py:919] - INFO - Load configs in /home/ma-user/work/mindformers/configs/gpt2/run_gpt2.yaml to build trainer.
2024-07-06 22:54:33,833 - mindformers[mindformers/trainer/trainer.py:949] - INFO - ..........Init Config..........
2024-07-06 22:54:33,833 - mindformers[mindformers/core/parallel_config.py:45] - INFO - initial recompute_config from dict: {'recompute': True, 'select_recompute': False, 'parallel_optimizer_comm_recompute': False, 'mp_comm_recompute': True, 'recompute_slice_activation': True}
2024-07-06 22:54:33,834 - mindformers[mindformers/core/parallel_config.py:51] - INFO - initial parallel_config from dict: {'data_parallel': 1, 'model_parallel': 4, 'pipeline_stage': 1, 'use_seq_parallel': False, 'micro_batch_num': 1, 'vocab_emb_dp': True, 'gradient_aggregation_group': 4}
2024-07-06 22:54:33,834 - mindformers[mindformers/tools/utils.py:153] - INFO - set output path to '/home/ma-user/work/mindformers/research/output'
2024-07-06 22:54:33,835 - mindformers[mindformers/tools/utils.py:168] - INFO - set strategy path to './output/strategy/ckpt_strategy_rank_0.ckpt'
2024-07-06 22:54:33,835 - mindformers[mindformers/trainer/base_trainer.py:85] - INFO - Now Running Task is: text_generation, Model is: llama3_8b
2024-07-06 22:54:33,835 - mindformers[mindformers/trainer/base_trainer.py:111] - WARNING - Input model name is not in the supported list or unspecified.
2024-07-06 22:54:33,835 - mindformers[mindformers/trainer/base_trainer.py:112] - WARNING - See the list of supported task and model name: ['baichuan2_13b', 'baichuan2_7b', 'baichuan_7b', 'bloom_176b', 'bloom_560m', 'bloom_65b', 'bloom_7.1b', 'codegeex2_6b', 'codellama_34b', 'common', 'deepseek_33b', 'glm2_6b', 'glm2_6b_lora', 'glm2_6b_ptuning2', 'glm3_6b', 'glm_6b', 'glm_6b_chat', 'glm_6b_lora', 'glm_6b_lora_chat', 'gpt2', 'gpt2_13b', 'gpt2_52b', 'gpt2_lora', 'gpt2_xl', 'gpt2_xl_lora', 'internlm_7b', 'internlm_7b_lora', 'llama2_13b', 'llama2_70b', 'llama2_7b', 'llama_13b', 'llama_65b', 'llama_7b', 'llama_7b_lora', 'pangualpha_13b', 'pangualpha_2_6b', 'qwen_7b', 'qwen_7b_lora', 'skywork_13b', 'yi_34b', 'yi_6b', 'ziya_13b']
2024-07-06 22:54:33,835 - mindformers[mindformers/trainer/base_trainer.py:113] - WARNING - The default model config: /home/ma-user/work/mindformers/configs/gpt2/run_gpt2.yaml will now be used for the text_generation task 
2024-07-06 22:54:33,836 - mindformers[mindformers/trainer/trainer.py:1004] - INFO - ..........Init Model..........
2024-07-06 22:54:33,836 - mindformers[mindformers/trainer/trainer.py:335] - INFO - ==========Trainer Init Success!==========
2024-07-06 22:54:33,836 - mindformers[mindformers/trainer/trainer.py:1004] - INFO - ..........Init Model..........
2024-07-06 22:54:33,836 - mindformers[mindformers/trainer/base_trainer.py:213] - INFO - The current parallel mode is stand_alone, batch size per card will not be changed: batch_size_per_card = 16
2024-07-06 22:54:33,837 - mindformers[mindformers/trainer/base_trainer.py:217] - INFO - global_batch_size = batch_size_per_card * device_num * gradient_accumulation_steps = 16 = 16 * 1 * 1
2024-07-06 22:54:33,837 - mindformers[mindformers/trainer/base_trainer.py:226] - INFO - parallel_config will be change to default config: [ParallelConfig]
_recompute:[ParallelConfig]
_recompute:True
_select_recompute:False
_select_comm_recompute:False
_parallel_optimizer_comm_recompute:False
_mp_comm_recompute:True
_recompute_slice_activation:True

select_recompute:False
use_seq_parallel:False
_optimizer_shard:None
_gradient_aggregation_group:4
_embed_dp_mp_config:[ParallelConfig]
_dp_mp_config:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_vocab_emb_dp:True
use_seq_parallel:False
select_recompute:False

_pp_config:[ParallelConfig]
_pipeline_stage:1
_micro_batch_num:1

_moe_config:[ParallelConfig]
_dpmp:[ParallelConfig]
_data_parallel:1
_model_parallel:1
use_seq_parallel:False
select_recompute:False

_expert_parallel:1
use_seq_parallel:False
select_recompute:False

.
2024-07-06 22:54:33,838 - mindformers[mindformers/trainer/base_trainer.py:387] - INFO - .........Build Network From Config..........
2024-07-06 22:54:33,838 - mindformers[mindformers/version_control.py:61] - INFO - The Cell Reuse compilation acceleration feature is not supported when the environment variable ENABLE_CELL_REUSE is 0 or MindSpore version is earlier than 2.1.0 or stand_alone mode or pipeline_stages <= 1
2024-07-06 22:54:33,839 - mindformers[mindformers/version_control.py:65] - INFO - 
The current ENABLE_CELL_REUSE=0, please set the environment variable as follows: 
export ENABLE_CELL_REUSE=1 to enable the Cell Reuse compilation acceleration feature.
2024-07-06 22:54:33,839 - mindformers[mindformers/version_control.py:71] - INFO - The Cell Reuse compilation acceleration feature does not support single-card mode.This feature is disabled by default. ENABLE_CELL_REUSE=1 does not take effect.
2024-07-06 22:54:33,839 - mindformers[mindformers/version_control.py:74] - INFO - The Cell Reuse compilation acceleration feature only works in pipeline parallel mode(pipeline_stage>1).Current pipeline stage=1, the feature is disabled by default.
[WARNING] DEVICE(6307,ffffa1c46010,python):2024-07-06-22:54:37.151.690 [mindspore/ccsrc/plugin/device/ascend/hal/device/ascend_memory_adapter.cc:95] Initialize] Reserved memory size for other components(536870912) is less than recommend size(1891892736), It may lead to Out Of Memory in HCCL or other components, Please double check context key 'variable_memory_max_size'/'max_device_memory'
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:54:58.774.598 [mindspore/ops/primitive.py:203] The in_strategy/in_layout of the operator in your network will not take effect in stand_alone mode. This means the the shard function called in the network is ignored. 
If you want to enable it, please use semi auto or auto parallel mode by context.set_auto_parallel_context(parallel_mode=ParallelMode.SEMI_AUTO_PARALLEL or context.set_auto_parallel_context(parallel_mode=ParallelMode.AUTO_PARALLEL)
2024-07-06 22:55:08,463 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:55:08.465.140 [mindspore/common/_decorator.py:40] 'Parameter' is deprecated from version 2.3 and will be removed in a future version, use 'add_pipeline_stage' instead.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:55:08.465.296 [mindspore/common/parameter.py:806] This interface may be deleted in the future.
2024-07-06 22:55:11,426 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:14,232 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:17,134 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:19,865 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:22,632 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:25,342 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:28,141 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:31,131 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:55:33,821 - mindformers[mindformers/models/llama/llama_transformer.py:468] - INFO - MoE config is None, use normal FFN
2024-07-06 22:56:42,976 - mindformers[mindformers/models/modeling_utils.py:1438] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
2024-07-06 22:56:42,977 - mindformers[mindformers/models/modeling_utils.py:591] - INFO - Set jit config for jit level:O0 and infer boost:on.
2024-07-06 22:56:45,889 - mindformers[mindformers/models/modeling_utils.py:1438] - INFO - model built, but weights is unloaded, since the config has no checkpoint_name_or_path attribute or checkpoint_name_or_path is None.
[INFO] 2024-07-06 22:56:45,893 [6307] [SDK] : Start to freeze model for delta, mode: lora, include list: None, exclude list: None
[INFO] 2024-07-06 22:56:45,893 [6307] [SDK] : Start to freeze model, include list: ['*'], exclude list: ['*mindpet_delta_lora*']
[INFO] 2024-07-06 22:56:45,900 [6307] [SDK] : End to freeze model.
[INFO] 2024-07-06 22:56:45,901 [6307] [SDK] : End to freeze model for delta.
2024-07-06 22:56:45,901 - mindformers[mindformers/models/modeling_utils.py:591] - INFO - Set jit config for jit level:O0 and infer boost:on.
2024-07-06 22:56:45,915 - mindformers[mindformers/trainer/base_trainer.py:543] - INFO - Network Parameters: 3407872.
2024-07-06 22:56:47,284 - mindformers[mindformers/trainer/utils.py:736] - INFO - ............Start load checkpoint from checkpoint............
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:23.715.800 [mindspore/train/serialization.py:195] The type of model.tok_embeddings.embedding_weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:37.373.708 [mindspore/train/serialization.py:195] The type of model.layers.0.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:37.919.245 [mindspore/train/serialization.py:195] The type of model.layers.0.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:38.152.590 [mindspore/train/serialization.py:195] The type of model.layers.0.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:38.806.901 [mindspore/train/serialization.py:195] The type of model.layers.0.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:41.232.522 [mindspore/train/serialization.py:195] The type of model.layers.0.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:44.538.737 [mindspore/train/serialization.py:195] The type of model.layers.0.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:47.811.568 [mindspore/train/serialization.py:195] The type of model.layers.0.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:49.700.399 [mindspore/train/serialization.py:195] The type of model.layers.1.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:50.232.085 [mindspore/train/serialization.py:195] The type of model.layers.1.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:50.462.815 [mindspore/train/serialization.py:195] The type of model.layers.1.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:51.746.57 [mindspore/train/serialization.py:195] The type of model.layers.1.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:53.378.674 [mindspore/train/serialization.py:195] The type of model.layers.1.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:58:57.192.786 [mindspore/train/serialization.py:195] The type of model.layers.1.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:00.481.540 [mindspore/train/serialization.py:195] The type of model.layers.1.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:02.438.615 [mindspore/train/serialization.py:195] The type of model.layers.2.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:02.986.053 [mindspore/train/serialization.py:195] The type of model.layers.2.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:03.229.192 [mindspore/train/serialization.py:195] The type of model.layers.2.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:03.898.232 [mindspore/train/serialization.py:195] The type of model.layers.2.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:06.162.046 [mindspore/train/serialization.py:195] The type of model.layers.2.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:09.657.938 [mindspore/train/serialization.py:195] The type of model.layers.2.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:13.226.825 [mindspore/train/serialization.py:195] The type of model.layers.2.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:15.211.317 [mindspore/train/serialization.py:195] The type of model.layers.3.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:15.766.728 [mindspore/train/serialization.py:195] The type of model.layers.3.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:16.502.7 [mindspore/train/serialization.py:195] The type of model.layers.3.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:16.620.460 [mindspore/train/serialization.py:195] The type of model.layers.3.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:19.258.988 [mindspore/train/serialization.py:195] The type of model.layers.3.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:22.969.754 [mindspore/train/serialization.py:195] The type of model.layers.3.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:26.476.099 [mindspore/train/serialization.py:195] The type of model.layers.3.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:28.422.525 [mindspore/train/serialization.py:195] The type of model.layers.4.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:29.121.81 [mindspore/train/serialization.py:195] The type of model.layers.4.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:29.242.467 [mindspore/train/serialization.py:195] The type of model.layers.4.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:29.875.184 [mindspore/train/serialization.py:195] The type of model.layers.4.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:32.324.114 [mindspore/train/serialization.py:195] The type of model.layers.4.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:35.681.359 [mindspore/train/serialization.py:195] The type of model.layers.4.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:39.769.6 [mindspore/train/serialization.py:195] The type of model.layers.4.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:41.114.266 [mindspore/train/serialization.py:195] The type of model.layers.5.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:41.656.147 [mindspore/train/serialization.py:195] The type of model.layers.5.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:41.885.017 [mindspore/train/serialization.py:195] The type of model.layers.5.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:42.579.943 [mindspore/train/serialization.py:195] The type of model.layers.5.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:44.892.001 [mindspore/train/serialization.py:195] The type of model.layers.5.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:48.176.777 [mindspore/train/serialization.py:195] The type of model.layers.5.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:51.527.668 [mindspore/train/serialization.py:195] The type of model.layers.5.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:53.468.259 [mindspore/train/serialization.py:195] The type of model.layers.6.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:54.222.85 [mindspore/train/serialization.py:195] The type of model.layers.6.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:54.250.988 [mindspore/train/serialization.py:195] The type of model.layers.6.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:54.906.751 [mindspore/train/serialization.py:195] The type of model.layers.6.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-22:59:57.286.630 [mindspore/train/serialization.py:195] The type of model.layers.6.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:00.604.825 [mindspore/train/serialization.py:195] The type of model.layers.6.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:03.936.601 [mindspore/train/serialization.py:195] The type of model.layers.6.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:05.832.349 [mindspore/train/serialization.py:195] The type of model.layers.7.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:06.354.164 [mindspore/train/serialization.py:195] The type of model.layers.7.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:06.588.628 [mindspore/train/serialization.py:195] The type of model.layers.7.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:07.235.065 [mindspore/train/serialization.py:195] The type of model.layers.7.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:09.618.506 [mindspore/train/serialization.py:195] The type of model.layers.7.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:12.949.076 [mindspore/train/serialization.py:195] The type of model.layers.7.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:16.250.598 [mindspore/train/serialization.py:195] The type of model.layers.7.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:18.198.196 [mindspore/train/serialization.py:195] The type of model.layers.8.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:18.740.171 [mindspore/train/serialization.py:195] The type of model.layers.8.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:18.970.846 [mindspore/train/serialization.py:195] The type of model.layers.8.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:19.608.567 [mindspore/train/serialization.py:195] The type of model.layers.8.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:21.908.433 [mindspore/train/serialization.py:195] The type of model.layers.8.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:25.283.129 [mindspore/train/serialization.py:195] The type of model.layers.8.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:28.588.950 [mindspore/train/serialization.py:195] The type of model.layers.8.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:30.597.372 [mindspore/train/serialization.py:195] The type of model.layers.9.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:31.147.923 [mindspore/train/serialization.py:195] The type of model.layers.9.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:31.382.704 [mindspore/train/serialization.py:195] The type of model.layers.9.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:32.292.04 [mindspore/train/serialization.py:195] The type of model.layers.9.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:34.352.532 [mindspore/train/serialization.py:195] The type of model.layers.9.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:37.732.619 [mindspore/train/serialization.py:195] The type of model.layers.9.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:41.210.69 [mindspore/train/serialization.py:195] The type of model.layers.9.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:42.999.878 [mindspore/train/serialization.py:195] The type of model.layers.10.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:43.524.555 [mindspore/train/serialization.py:195] The type of model.layers.10.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:43.761.798 [mindspore/train/serialization.py:195] The type of model.layers.10.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:44.410.846 [mindspore/train/serialization.py:195] The type of model.layers.10.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:46.759.814 [mindspore/train/serialization.py:195] The type of model.layers.10.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:50.219.194 [mindspore/train/serialization.py:195] The type of model.layers.10.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:53.480.224 [mindspore/train/serialization.py:195] The type of model.layers.10.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:55.446.811 [mindspore/train/serialization.py:195] The type of model.layers.11.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:55.994.622 [mindspore/train/serialization.py:195] The type of model.layers.11.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:56.226.470 [mindspore/train/serialization.py:195] The type of model.layers.11.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:56.876.516 [mindspore/train/serialization.py:195] The type of model.layers.11.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:00:59.226.517 [mindspore/train/serialization.py:195] The type of model.layers.11.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:02.546.451 [mindspore/train/serialization.py:195] The type of model.layers.11.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:05.895.826 [mindspore/train/serialization.py:195] The type of model.layers.11.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:07.796.701 [mindspore/train/serialization.py:195] The type of model.layers.12.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:08.331.830 [mindspore/train/serialization.py:195] The type of model.layers.12.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:08.561.662 [mindspore/train/serialization.py:195] The type of model.layers.12.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:09.212.178 [mindspore/train/serialization.py:195] The type of model.layers.12.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:11.565.148 [mindspore/train/serialization.py:195] The type of model.layers.12.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:14.901.121 [mindspore/train/serialization.py:195] The type of model.layers.12.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:18.262.450 [mindspore/train/serialization.py:195] The type of model.layers.12.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:20.324.230 [mindspore/train/serialization.py:195] The type of model.layers.13.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:20.861.363 [mindspore/train/serialization.py:195] The type of model.layers.13.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:21.111.127 [mindspore/train/serialization.py:195] The type of model.layers.13.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:21.758.423 [mindspore/train/serialization.py:195] The type of model.layers.13.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:24.687.52 [mindspore/train/serialization.py:195] The type of model.layers.13.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:27.394.524 [mindspore/train/serialization.py:195] The type of model.layers.13.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:30.784.819 [mindspore/train/serialization.py:195] The type of model.layers.13.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:32.721.172 [mindspore/train/serialization.py:195] The type of model.layers.14.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:33.253.653 [mindspore/train/serialization.py:195] The type of model.layers.14.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:33.502.492 [mindspore/train/serialization.py:195] The type of model.layers.14.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:34.145.070 [mindspore/train/serialization.py:195] The type of model.layers.14.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:36.483.649 [mindspore/train/serialization.py:195] The type of model.layers.14.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:39.821.727 [mindspore/train/serialization.py:195] The type of model.layers.14.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:43.729.51 [mindspore/train/serialization.py:195] The type of model.layers.14.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:45.232.68 [mindspore/train/serialization.py:195] The type of model.layers.15.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:45.594.543 [mindspore/train/serialization.py:195] The type of model.layers.15.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:45.831.665 [mindspore/train/serialization.py:195] The type of model.layers.15.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:46.479.094 [mindspore/train/serialization.py:195] The type of model.layers.15.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:48.800.167 [mindspore/train/serialization.py:195] The type of model.layers.15.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:52.106.602 [mindspore/train/serialization.py:195] The type of model.layers.15.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:55.395.957 [mindspore/train/serialization.py:195] The type of model.layers.15.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:57.338.157 [mindspore/train/serialization.py:195] The type of model.layers.16.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:57.890.952 [mindspore/train/serialization.py:195] The type of model.layers.16.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:58.177.765 [mindspore/train/serialization.py:195] The type of model.layers.16.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:01:58.825.601 [mindspore/train/serialization.py:195] The type of model.layers.16.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:01.139.246 [mindspore/train/serialization.py:195] The type of model.layers.16.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:04.415.497 [mindspore/train/serialization.py:195] The type of model.layers.16.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:07.777.784 [mindspore/train/serialization.py:195] The type of model.layers.16.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:09.718.575 [mindspore/train/serialization.py:195] The type of model.layers.17.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:10.270.995 [mindspore/train/serialization.py:195] The type of model.layers.17.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:10.512.043 [mindspore/train/serialization.py:195] The type of model.layers.17.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:11.210.753 [mindspore/train/serialization.py:195] The type of model.layers.17.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:13.565.309 [mindspore/train/serialization.py:195] The type of model.layers.17.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:17.304.48 [mindspore/train/serialization.py:195] The type of model.layers.17.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:20.469.633 [mindspore/train/serialization.py:195] The type of model.layers.17.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:22.639.235 [mindspore/train/serialization.py:195] The type of model.layers.18.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:23.272.690 [mindspore/train/serialization.py:195] The type of model.layers.18.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:23.515.209 [mindspore/train/serialization.py:195] The type of model.layers.18.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:24.220.091 [mindspore/train/serialization.py:195] The type of model.layers.18.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:26.721.873 [mindspore/train/serialization.py:195] The type of model.layers.18.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:30.255.290 [mindspore/train/serialization.py:195] The type of model.layers.18.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:33.746.736 [mindspore/train/serialization.py:195] The type of model.layers.18.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:35.826.713 [mindspore/train/serialization.py:195] The type of model.layers.19.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:36.401.617 [mindspore/train/serialization.py:195] The type of model.layers.19.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:36.652.545 [mindspore/train/serialization.py:195] The type of model.layers.19.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:37.351.504 [mindspore/train/serialization.py:195] The type of model.layers.19.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:39.901.054 [mindspore/train/serialization.py:195] The type of model.layers.19.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:43.355.421 [mindspore/train/serialization.py:195] The type of model.layers.19.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:46.688.664 [mindspore/train/serialization.py:195] The type of model.layers.19.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:48.642.312 [mindspore/train/serialization.py:195] The type of model.layers.20.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:49.172.608 [mindspore/train/serialization.py:195] The type of model.layers.20.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:49.435.076 [mindspore/train/serialization.py:195] The type of model.layers.20.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:50.682.15 [mindspore/train/serialization.py:195] The type of model.layers.20.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:52.508.878 [mindspore/train/serialization.py:195] The type of model.layers.20.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:55.809.581 [mindspore/train/serialization.py:195] The type of model.layers.20.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:02:59.198.293 [mindspore/train/serialization.py:195] The type of model.layers.20.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:01.129.886 [mindspore/train/serialization.py:195] The type of model.layers.21.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:01.676.520 [mindspore/train/serialization.py:195] The type of model.layers.21.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:01.925.429 [mindspore/train/serialization.py:195] The type of model.layers.21.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:02.564.562 [mindspore/train/serialization.py:195] The type of model.layers.21.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:04.914.115 [mindspore/train/serialization.py:195] The type of model.layers.21.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:08.309.765 [mindspore/train/serialization.py:195] The type of model.layers.21.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:11.889.317 [mindspore/train/serialization.py:195] The type of model.layers.21.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:13.860.354 [mindspore/train/serialization.py:195] The type of model.layers.22.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:14.423.883 [mindspore/train/serialization.py:195] The type of model.layers.22.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:14.669.645 [mindspore/train/serialization.py:195] The type of model.layers.22.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:15.320.099 [mindspore/train/serialization.py:195] The type of model.layers.22.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:17.608.369 [mindspore/train/serialization.py:195] The type of model.layers.22.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:21.115.402 [mindspore/train/serialization.py:195] The type of model.layers.22.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:24.402.056 [mindspore/train/serialization.py:195] The type of model.layers.22.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:26.351.032 [mindspore/train/serialization.py:195] The type of model.layers.23.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:26.903.972 [mindspore/train/serialization.py:195] The type of model.layers.23.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:27.152.545 [mindspore/train/serialization.py:195] The type of model.layers.23.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:27.817.524 [mindspore/train/serialization.py:195] The type of model.layers.23.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:30.155.093 [mindspore/train/serialization.py:195] The type of model.layers.23.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:33.476.316 [mindspore/train/serialization.py:195] The type of model.layers.23.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:36.799.812 [mindspore/train/serialization.py:195] The type of model.layers.23.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:38.731.575 [mindspore/train/serialization.py:195] The type of model.layers.24.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:39.278.825 [mindspore/train/serialization.py:195] The type of model.layers.24.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:39.535.574 [mindspore/train/serialization.py:195] The type of model.layers.24.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:40.168.237 [mindspore/train/serialization.py:195] The type of model.layers.24.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:42.542.304 [mindspore/train/serialization.py:195] The type of model.layers.24.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:45.822.624 [mindspore/train/serialization.py:195] The type of model.layers.24.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:49.133.460 [mindspore/train/serialization.py:195] The type of model.layers.24.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:51.904.37 [mindspore/train/serialization.py:195] The type of model.layers.25.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:51.643.818 [mindspore/train/serialization.py:195] The type of model.layers.25.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:51.874.835 [mindspore/train/serialization.py:195] The type of model.layers.25.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:52.530.559 [mindspore/train/serialization.py:195] The type of model.layers.25.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:54.862.121 [mindspore/train/serialization.py:195] The type of model.layers.25.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:03:58.132.175 [mindspore/train/serialization.py:195] The type of model.layers.25.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:01.477.024 [mindspore/train/serialization.py:195] The type of model.layers.25.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:03.384.199 [mindspore/train/serialization.py:195] The type of model.layers.26.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:03.916.342 [mindspore/train/serialization.py:195] The type of model.layers.26.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:04.153.205 [mindspore/train/serialization.py:195] The type of model.layers.26.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:04.779.659 [mindspore/train/serialization.py:195] The type of model.layers.26.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:07.113.742 [mindspore/train/serialization.py:195] The type of model.layers.26.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:10.421.450 [mindspore/train/serialization.py:195] The type of model.layers.26.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:13.890.434 [mindspore/train/serialization.py:195] The type of model.layers.26.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:15.822.211 [mindspore/train/serialization.py:195] The type of model.layers.27.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:16.385.042 [mindspore/train/serialization.py:195] The type of model.layers.27.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:16.616.083 [mindspore/train/serialization.py:195] The type of model.layers.27.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:17.265.918 [mindspore/train/serialization.py:195] The type of model.layers.27.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:19.606.732 [mindspore/train/serialization.py:195] The type of model.layers.27.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:22.896.048 [mindspore/train/serialization.py:195] The type of model.layers.27.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:26.203.608 [mindspore/train/serialization.py:195] The type of model.layers.27.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:28.179.406 [mindspore/train/serialization.py:195] The type of model.layers.28.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:28.704.290 [mindspore/train/serialization.py:195] The type of model.layers.28.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:28.952.534 [mindspore/train/serialization.py:195] The type of model.layers.28.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:29.589.122 [mindspore/train/serialization.py:195] The type of model.layers.28.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:31.940.641 [mindspore/train/serialization.py:195] The type of model.layers.28.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:35.229.263 [mindspore/train/serialization.py:195] The type of model.layers.28.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:38.494.872 [mindspore/train/serialization.py:195] The type of model.layers.28.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:40.436.632 [mindspore/train/serialization.py:195] The type of model.layers.29.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:40.987.151 [mindspore/train/serialization.py:195] The type of model.layers.29.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:41.219.610 [mindspore/train/serialization.py:195] The type of model.layers.29.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:41.912.516 [mindspore/train/serialization.py:195] The type of model.layers.29.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:44.271.298 [mindspore/train/serialization.py:195] The type of model.layers.29.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:47.560.416 [mindspore/train/serialization.py:195] The type of model.layers.29.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:50.872.367 [mindspore/train/serialization.py:195] The type of model.layers.29.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:52.807.326 [mindspore/train/serialization.py:195] The type of model.layers.30.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:53.351.437 [mindspore/train/serialization.py:195] The type of model.layers.30.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:53.588.067 [mindspore/train/serialization.py:195] The type of model.layers.30.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:54.228.958 [mindspore/train/serialization.py:195] The type of model.layers.30.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:56.621.460 [mindspore/train/serialization.py:195] The type of model.layers.30.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:04:59.910.677 [mindspore/train/serialization.py:195] The type of model.layers.30.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:03.445.957 [mindspore/train/serialization.py:195] The type of model.layers.30.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:05.408.761 [mindspore/train/serialization.py:195] The type of model.layers.31.attention.wq.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:05.968.168 [mindspore/train/serialization.py:195] The type of model.layers.31.attention.wk.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:06.213.453 [mindspore/train/serialization.py:195] The type of model.layers.31.attention.wv.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:06.919.548 [mindspore/train/serialization.py:195] The type of model.layers.31.attention.wo.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:09.465.446 [mindspore/train/serialization.py:195] The type of model.layers.31.feed_forward.w1.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:13.138.419 [mindspore/train/serialization.py:195] The type of model.layers.31.feed_forward.w2.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:16.643.309 [mindspore/train/serialization.py:195] The type of model.layers.31.feed_forward.w3.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:37.894.958 [mindspore/train/serialization.py:195] The type of lm_head.weight:BFloat16 in 'parameter_dict' is different from the type of it in 'net':Float16, then the type convert from BFloat16 to Float16 in the network.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:50.850.727 [mindspore/train/serialization.py:1456] For 'load_param_into_net', 64 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.
[WARNING] ME(6307:281473395744784,MainProcess):2024-07-06-23:05:50.851.072 [mindspore/train/serialization.py:1460] ['model.layers.0.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.0.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.1.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.1.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.2.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.2.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.3.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.3.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.4.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.4.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.5.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.5.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.6.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.6.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.7.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.7.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.8.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.8.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.9.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.9.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.10.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.10.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.11.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.11.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.12.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.12.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.13.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.13.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.14.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.14.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.15.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.15.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.16.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.16.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.17.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.17.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.18.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.18.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.19.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.19.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.20.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.20.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.21.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.21.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.22.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.22.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.23.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.23.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.24.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.24.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.25.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.25.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.26.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.26.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.27.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.27.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.28.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.28.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.29.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.29.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.30.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.30.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.31.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.31.attention.infer_attention.paged_attention_mgr.value_cache'] are not loaded.
2024-07-06 23:05:50,851 - mindformers[mindformers/trainer/utils.py:767] - INFO - Network parameters are not loaded: (['model.layers.0.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.0.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.1.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.1.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.2.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.2.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.3.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.3.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.4.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.4.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.5.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.5.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.6.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.6.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.7.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.7.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.8.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.8.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.9.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.9.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.10.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.10.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.11.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.11.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.12.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.12.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.13.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.13.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.14.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.14.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.15.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.15.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.16.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.16.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.17.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.17.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.18.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.18.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.19.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.19.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.20.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.20.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.21.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.21.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.22.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.22.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.23.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.23.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.24.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.24.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.25.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.25.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.26.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.26.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.27.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.27.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.28.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.28.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.29.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.29.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.30.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.30.attention.infer_attention.paged_attention_mgr.value_cache', 'model.layers.31.attention.infer_attention.paged_attention_mgr.key_cache', 'model.layers.31.attention.infer_attention.paged_attention_mgr.value_cache'], ['model.layers.0.attention.wk.mindpet_delta_lora_a', 'model.layers.0.attention.wk.mindpet_delta_lora_b', 'model.layers.1.attention.wk.mindpet_delta_lora_a', 'model.layers.1.attention.wk.mindpet_delta_lora_b', 'model.layers.2.attention.wk.mindpet_delta_lora_a', 'model.layers.2.attention.wk.mindpet_delta_lora_b', 'model.layers.3.attention.wk.mindpet_delta_lora_a', 'model.layers.3.attention.wk.mindpet_delta_lora_b', 'model.layers.4.attention.wk.mindpet_delta_lora_a', 'model.layers.4.attention.wk.mindpet_delta_lora_b', 'model.layers.5.attention.wk.mindpet_delta_lora_a', 'model.layers.5.attention.wk.mindpet_delta_lora_b', 'model.layers.6.attention.wk.mindpet_delta_lora_a', 'model.layers.6.attention.wk.mindpet_delta_lora_b', 'model.layers.7.attention.wk.mindpet_delta_lora_a', 'model.layers.7.attention.wk.mindpet_delta_lora_b', 'model.layers.8.attention.wk.mindpet_delta_lora_a', 'model.layers.8.attention.wk.mindpet_delta_lora_b', 'model.layers.9.attention.wk.mindpet_delta_lora_a', 'model.layers.9.attention.wk.mindpet_delta_lora_b', 'model.layers.10.attention.wk.mindpet_delta_lora_a', 'model.layers.10.attention.wk.mindpet_delta_lora_b', 'model.layers.11.attention.wk.mindpet_delta_lora_a', 'model.layers.11.attention.wk.mindpet_delta_lora_b', 'model.layers.12.attention.wk.mindpet_delta_lora_a', 'model.layers.12.attention.wk.mindpet_delta_lora_b', 'model.layers.13.attention.wk.mindpet_delta_lora_a', 'model.layers.13.attention.wk.mindpet_delta_lora_b', 'model.layers.14.attention.wk.mindpet_delta_lora_a', 'model.layers.14.attention.wk.mindpet_delta_lora_b', 'model.layers.15.attention.wk.mindpet_delta_lora_a', 'model.layers.15.attention.wk.mindpet_delta_lora_b', 'model.layers.16.attention.wk.mindpet_delta_lora_a', 'model.layers.16.attention.wk.mindpet_delta_lora_b', 'model.layers.17.attention.wk.mindpet_delta_lora_a', 'model.layers.17.attention.wk.mindpet_delta_lora_b', 'model.layers.18.attention.wk.mindpet_delta_lora_a', 'model.layers.18.attention.wk.mindpet_delta_lora_b', 'model.layers.19.attention.wk.mindpet_delta_lora_a', 'model.layers.19.attention.wk.mindpet_delta_lora_b', 'model.layers.20.attention.wk.mindpet_delta_lora_a', 'model.layers.20.attention.wk.mindpet_delta_lora_b', 'model.layers.21.attention.wk.mindpet_delta_lora_a', 'model.layers.21.attention.wk.mindpet_delta_lora_b', 'model.layers.22.attention.wk.mindpet_delta_lora_a', 'model.layers.22.attention.wk.mindpet_delta_lora_b', 'model.layers.23.attention.wk.mindpet_delta_lora_a', 'model.layers.23.attention.wk.mindpet_delta_lora_b', 'model.layers.24.attention.wk.mindpet_delta_lora_a', 'model.layers.24.attention.wk.mindpet_delta_lora_b', 'model.layers.25.attention.wk.mindpet_delta_lora_a', 'model.layers.25.attention.wk.mindpet_delta_lora_b', 'model.layers.26.attention.wk.mindpet_delta_lora_a', 'model.layers.26.attention.wk.mindpet_delta_lora_b', 'model.layers.27.attention.wk.mindpet_delta_lora_a', 'model.layers.27.attention.wk.mindpet_delta_lora_b', 'model.layers.28.attention.wk.mindpet_delta_lora_a', 'model.layers.28.attention.wk.mindpet_delta_lora_b', 'model.layers.29.attention.wk.mindpet_delta_lora_a', 'model.layers.29.attention.wk.mindpet_delta_lora_b', 'model.layers.30.attention.wk.mindpet_delta_lora_a', 'model.layers.30.attention.wk.mindpet_delta_lora_b', 'model.layers.31.attention.wk.mindpet_delta_lora_a', 'model.layers.31.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.0.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.0.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.0.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.0.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.0.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.0.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.1.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.1.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.1.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.1.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.1.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.1.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.2.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.2.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.2.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.2.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.2.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.2.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.3.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.3.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.3.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.3.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.3.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.3.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.4.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.4.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.4.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.4.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.4.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.4.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.5.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.5.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.5.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.5.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.5.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.5.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.6.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.6.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.6.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.6.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.6.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.6.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.7.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.7.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.7.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.7.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.7.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.7.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.8.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.8.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.8.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.8.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.8.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.8.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.9.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.9.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.9.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.9.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.9.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.9.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.10.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.10.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.10.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.10.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.10.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.10.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.11.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.11.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.11.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.11.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.11.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.11.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.12.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.12.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.12.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.12.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.12.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.12.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.13.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.13.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.13.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.13.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.13.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.13.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.14.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.14.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.14.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.14.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.14.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.14.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.15.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.15.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.15.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.15.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.15.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.15.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.16.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.16.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.16.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.16.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.16.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.16.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.17.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.17.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.17.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.17.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.17.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.17.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.18.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.18.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.18.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.18.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.18.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.18.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.19.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.19.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.19.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.19.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.19.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.19.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.20.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.20.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.20.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.20.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.20.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.20.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.21.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.21.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.21.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.21.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.21.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.21.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.22.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.22.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.22.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.22.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.22.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.22.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.23.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.23.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.23.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.23.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.23.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.23.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.24.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.24.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.24.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.24.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.24.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.24.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.25.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.25.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.25.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.25.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.25.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.25.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.26.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.26.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.26.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.26.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.26.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.26.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.27.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.27.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.27.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.27.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.27.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.27.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.28.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.28.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.28.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.28.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.28.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.28.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.29.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.29.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.29.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.29.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.29.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.29.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.30.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.30.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.30.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.30.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.30.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.30.attention.wv.mindpet_delta_lora_b', 'adam_m.model.layers.31.attention.wq.mindpet_delta_lora_a', 'adam_m.model.layers.31.attention.wq.mindpet_delta_lora_b', 'adam_m.model.layers.31.attention.wk.mindpet_delta_lora_a', 'adam_m.model.layers.31.attention.wk.mindpet_delta_lora_b', 'adam_m.model.layers.31.attention.wv.mindpet_delta_lora_a', 'adam_m.model.layers.31.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.0.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.0.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.0.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.0.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.0.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.0.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.1.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.1.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.1.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.1.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.1.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.1.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.2.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.2.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.2.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.2.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.2.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.2.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.3.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.3.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.3.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.3.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.3.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.3.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.4.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.4.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.4.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.4.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.4.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.4.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.5.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.5.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.5.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.5.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.5.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.5.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.6.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.6.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.6.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.6.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.6.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.6.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.7.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.7.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.7.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.7.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.7.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.7.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.8.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.8.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.8.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.8.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.8.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.8.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.9.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.9.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.9.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.9.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.9.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.9.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.10.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.10.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.10.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.10.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.10.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.10.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.11.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.11.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.11.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.11.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.11.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.11.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.12.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.12.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.12.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.12.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.12.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.12.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.13.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.13.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.13.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.13.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.13.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.13.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.14.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.14.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.14.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.14.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.14.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.14.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.15.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.15.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.15.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.15.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.15.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.15.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.16.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.16.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.16.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.16.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.16.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.16.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.17.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.17.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.17.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.17.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.17.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.17.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.18.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.18.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.18.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.18.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.18.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.18.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.19.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.19.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.19.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.19.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.19.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.19.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.20.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.20.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.20.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.20.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.20.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.20.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.21.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.21.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.21.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.21.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.21.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.21.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.22.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.22.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.22.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.22.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.22.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.22.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.23.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.23.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.23.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.23.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.23.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.23.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.24.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.24.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.24.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.24.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.24.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.24.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.25.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.25.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.25.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.25.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.25.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.25.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.26.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.26.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.26.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.26.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.26.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.26.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.27.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.27.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.27.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.27.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.27.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.27.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.28.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.28.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.28.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.28.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.28.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.28.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.29.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.29.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.29.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.29.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.29.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.29.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.30.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.30.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.30.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.30.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.30.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.30.attention.wv.mindpet_delta_lora_b', 'adam_v.model.layers.31.attention.wq.mindpet_delta_lora_a', 'adam_v.model.layers.31.attention.wq.mindpet_delta_lora_b', 'adam_v.model.layers.31.attention.wk.mindpet_delta_lora_a', 'adam_v.model.layers.31.attention.wk.mindpet_delta_lora_b', 'adam_v.model.layers.31.attention.wv.mindpet_delta_lora_a', 'adam_v.model.layers.31.attention.wv.mindpet_delta_lora_b', 'scale_sense', 'global_step', 'epoch_num', 'step_num', 'loss_scale'])
{'auto_trans_ckpt': False,
 'auto_tune': False,
 'autotune_per_step': 10,
 'callbacks': [OrderedDict([('type', 'MFLossMonitor')]),
               OrderedDict([('type', 'CheckpointMointor'),
                            ('prefix', 'llama3_8b'),
                            ('save_checkpoint_steps', 1400),
                            ('integrated_save', False),
                            ('async_save', False)]),
               OrderedDict([('type', 'ObsMonitor')])],
 'context': {'device_id': 0,
             'device_target': 'Ascend',
             'enable_graph_kernel': False,
             'graph_kernel_flags': '--disable_expand_ops=Softmax,Dropout '
                                   '--enable_parallel_fusion=true '
                                   '--reduce_fuse_depth=8 '
                                   '--enable_auto_tensor_inplace=true',
             'max_call_depth': 10000,
             'runtime_num_threads': 1,
             'save_graphs': False,
             'save_graphs_path': './graph'},
 'device_num': 1,
 'do_eval': False,
 'eval_callbacks': [OrderedDict([('type', 'ObsMonitor')])],
 'eval_dataset': {'auto_tune': False,
                  'autotune_per_step': 10,
                  'batch_size': 16,
                  'data_loader': {'dataset_dir': '',
                                  'shuffle': False,
                                  'type': 'MindDataset'},
                  'do_eval': True,
                  'drop_remainder': False,
                  'filepath_prefix': './autotune',
                  'input_columns': ['input_ids'],
                  'num_parallel_workers': 8,
                  'numa_enable': False,
                  'output_columns': ['input_ids'],
                  'prefetch_size': 1,
                  'profile': False,
                  'python_multiprocessing': False,
                  'repeat': 1,
                  'seed': 0},
 'eval_dataset_task': {'dataset_config': {'auto_tune': False,
                                          'autotune_per_step': 10,
                                          'batch_size': 16,
                                          'data_loader': {'dataset_dir': '',
                                                          'shuffle': False,
                                                          'type': 'MindDataset'},
                                          'do_eval': True,
                                          'drop_remainder': False,
                                          'filepath_prefix': './autotune',
                                          'input_columns': ['input_ids'],
                                          'num_parallel_workers': 8,
                                          'numa_enable': False,
                                          'output_columns': ['input_ids'],
                                          'prefetch_size': 1,
                                          'profile': False,
                                          'python_multiprocessing': False,
                                          'repeat': 1,
                                          'seed': 0},
                       'type': 'CausalLanguageModelDataset'},
 'filepath_prefix': './autotune',
 'init_start_profile': False,
 'layer_decay': 0.65,
 'layer_scale': False,
 'load_checkpoint': '/home/ma-user/work/new_llama3_8b_lora.ckpt',
 'local_rank': 0,
 'lr_scale_factor': 256,
 'lr_schedule': {'learning_rate': 1e-05,
                 'lr_end': 0.0,
                 'total_steps': -1,
                 'type': 'CosineWithWarmUpLR',
                 'warmup_ratio': 0.03},
 'metric': [{'type': 'PerplexityMetric'}],
 'micro_batch_interleave_num': 1,
 'model': {'arch': {'type': 'LlamaForCausalLM'},
           'model_config': {'batch_size': 8,
                            'block_size': 64,
                            'bos_token_id': 128000,
                            'checkpoint_name_or_path': None,
                            'compute_dtype': 'float16',
                            'do_sample': False,
                            'eos_token_id': 128001,
                            'extend_method': 'None',
                            'fine_grain_interleave': 1,
                            'hidden_size': 4096,
                            'ignore_token_id': -100,
                            'intermediate_size': 14336,
                            'is_dynamic': False,
                            'layernorm_compute_type': 'float32',
                            'max_decode_length': 512,
                            'max_new_tokens': 128,
                            'n_kv_heads': 8,
                            'num_heads': 32,
                            'num_layers': 32,
                            'offset': 0,
                            'pad_token_id': 128002,
                            'param_init_type': 'float16',
                            'pet_config': {'lora_alpha': 16,
                                           'lora_dropout': 0.0,
                                           'lora_rank': 8,
                                           'target_modules': '.*wq|.*wv'},
                            'repetition_penalty': 1,
                            'rms_norm_eps': 1e-05,
                            'rotary_dtype': 'float32',
                            'scaling_factor': 1.0,
                            'seq_length': 3000,
                            'softmax_compute_type': 'float32',
                            'theta': 500000,
                            'top_k': 3,
                            'top_p': 1,
                            'type': 'LlamaConfig',
                            'use_flash_attention': True,
                            'use_past': True,
                            'vocab_size': 128256}},
 'moe_config': <mindformers.modules.transformer.moe.MoEConfig object at 0xfffe93211cd0>,
 'only_save_strategy': False,
 'optimizer': {'beta1': 0.9,
               'beta2': 0.95,
               'eps': 1e-08,
               'type': 'FP32StateAdamWeightDecay'},
 'output_dir': './output',
 'parallel': {'enable_alltoall': False,
              'enable_parallel_optimizer': True,
              'full_batch': True,
              'gradients_mean': False,
              'parallel_mode': 1,
              'parallel_optimizer_config': {'gradient_accumulation_shard': False,
                                            'parallel_optimizer_threshold': 64},
              'search_mode': 'sharding_propagation',
              'strategy_ckpt_config': {'only_trainable_params': False,
                                       'save_file': './ckpt_strategy.ckpt'},
              'strategy_ckpt_save_file': './output/strategy/ckpt_strategy_rank_0.ckpt'},
 'parallel_config': <mindformers.modules.transformer.transformer.TransformerOpParallelConfig object at 0xfffe14115340>,
 'processor': {'return_tensors': 'ms',
               'tokenizer': {'model_max_length': 8192,
                             'pad_token': '<|reserved_special_token_0|>',
                             'type': 'Llama3Tokenizer',
                             'vocab_file': '/home/ma-user/work/tokenizer.model'},
               'type': 'LlamaProcessor'},
 'profile': False,
 'profile_communication': False,
 'profile_memory': True,
 'profile_start_step': 4,
 'profile_stop_step': 8,
 'rank_id': 0,
 'recompute_config': <mindformers.modules.transformer.transformer.TransformerRecomputeConfig object at 0xfffe141152b0>,
 'remote_save_url': '',
 'resume_training': False,
 'run_mode': 'finetune',
 'runner_config': {'batch_size': 16,
                   'epochs': 5,
                   'gradient_accumulation_steps': 1,
                   'sink_mode': True,
                   'sink_size': 2},
 'runner_wrapper': {'scale_sense': 1.0,
                    'type': 'MFTrainOneStepCell',
                    'use_clip_grad': True},
 'seed': 0,
 'src_strategy_path_or_dir': '',
 'train_dataset': {'auto_tune': False,
                   'autotune_per_step': 10,
                   'batch_size': 16,
                   'data_loader': {'dataset_dir': '',
                                   'shuffle': True,
                                   'type': 'MindDataset'},
                   'do_eval': False,
                   'drop_remainder': True,
                   'filepath_prefix': './autotune',
                   'input_columns': ['input_ids', 'labels'],
                   'num_parallel_workers': 8,
                   'numa_enable': False,
                   'output_columns': ['input_ids', 'labels'],
                   'prefetch_size': 1,
                   'profile': False,
                   'python_multiprocessing': False,
                   'repeat': 1,
                   'seed': 0},
 'train_dataset_task': {'dataset_config': {'auto_tune': False,
                                           'autotune_per_step': 10,
                                           'batch_size': 16,
                                           'data_loader': {'dataset_dir': '',
                                                           'shuffle': True,
                                                           'type': 'MindDataset'},
                                           'do_eval': False,
                                           'drop_remainder': True,
                                           'filepath_prefix': './autotune',
                                           'input_columns': ['input_ids',
                                                             'labels'],
                                           'num_parallel_workers': 8,
                                           'numa_enable': False,
                                           'output_columns': ['input_ids',
                                                              'labels'],
                                           'prefetch_size': 1,
                                           'profile': False,
                                           'python_multiprocessing': False,
                                           'repeat': 1,
                                           'seed': 0},
                        'type': 'CausalLanguageModelDataset'},
 'trainer': {'model_name': 'llama3_8b',
             'type': 'CausalLanguageModelingTrainer'},
 'use_parallel': False}
2024-07-06 23:05:50,913 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:05:50,914 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:05:50,915 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:05:50,915 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:05:50,916 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:06:49,470 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,446 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,490 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,530 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,563 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,596 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,629 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,662 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,695 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:05,728 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:06,063 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 75.14610409736633 s; generated tokens: 51 tokens; generate speed: 0.6786778983767359 tokens/s
2024-07-06 23:07:06,069 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:06,085 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:06,085 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:06,086 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:06,086 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:06,086 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:08,665 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,699 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,733 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,765 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,798 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,831 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,864 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,897 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,930 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:08,962 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:12,857 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 6.770498514175415 s; generated tokens: 257 tokens; generate speed: 37.95880014771708 tokens/s
2024-07-06 23:07:12,863 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:12,877 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:12,878 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:12,878 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:12,878 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:12,879 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:15,457 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,496 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,530 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,562 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,595 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,628 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,661 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,693 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,726 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:15,759 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:16,348 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.4695487022399902 s; generated tokens: 57 tokens; generate speed: 16.42864962904253 tokens/s
2024-07-06 23:07:16,354 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:16,365 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:16,365 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:16,365 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:16,366 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:16,366 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:18,945 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:18,985 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,019 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,052 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,085 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,117 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,150 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,183 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,216 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:19,252 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:20,108 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.741602659225464 s; generated tokens: 48 tokens; generate speed: 12.828727251849964 tokens/s
2024-07-06 23:07:20,113 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:20,124 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:20,124 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:20,125 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:20,125 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:20,125 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:22,700 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,735 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,768 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,801 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,834 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,866 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,899 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,932 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,964 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:22,997 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:26,932 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 6.80610728263855 s; generated tokens: 135 tokens; generate speed: 19.83512665813637 tokens/s
2024-07-06 23:07:26,937 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:26,950 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:26,950 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:26,951 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:26,951 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:26,951 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:29,529 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,564 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,597 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,630 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,663 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,696 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,729 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,762 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,794 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:29,827 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:30,752 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.8004963397979736 s; generated tokens: 45 tokens; generate speed: 11.840558699865003 tokens/s
2024-07-06 23:07:30,758 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:30,769 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:30,769 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:30,770 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:30,770 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:30,770 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:33,350 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,385 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,418 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,451 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,483 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,516 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,549 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,581 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,614 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:33,647 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:34,920 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 4.149628639221191 s; generated tokens: 68 tokens; generate speed: 16.387008552351407 tokens/s
2024-07-06 23:07:34,925 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:34,941 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:34,941 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:34,942 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:34,942 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:34,943 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:37,523 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,558 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,592 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,627 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,660 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,693 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,726 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,758 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,791 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:37,824 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:38,085 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.142493724822998 s; generated tokens: 43 tokens; generate speed: 13.683400434609297 tokens/s
2024-07-06 23:07:38,091 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:38,101 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:38,101 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:38,102 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:38,102 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:38,102 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:40,677 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,715 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,749 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,781 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,814 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,847 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,880 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,913 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,946 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:40,979 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:41,540 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.4376306533813477 s; generated tokens: 41 tokens; generate speed: 11.926819409662663 tokens/s
2024-07-06 23:07:41,545 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:41,556 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:41,556 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:41,557 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:41,557 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:41,557 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:44,136 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,171 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,205 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,238 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,271 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,304 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,337 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,370 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,403 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,435 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,964 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.4060471057891846 s; generated tokens: 58 tokens; generate speed: 17.028537245247918 tokens/s
2024-07-06 23:07:44,969 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:44,980 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:44,980 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:44,981 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:44,981 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:44,981 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:47,557 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,593 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,626 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,660 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,694 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,727 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,761 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,795 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,829 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:47,862 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:48,624 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.642186403274536 s; generated tokens: 92 tokens; generate speed: 25.259552865632216 tokens/s
2024-07-06 23:07:48,629 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:48,644 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:48,644 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:48,645 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:48,645 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:48,645 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:51,225 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,259 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,300 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,333 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,366 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,400 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,433 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,466 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,499 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:51,532 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:52,029 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.383336067199707 s; generated tokens: 55 tokens; generate speed: 16.256144499863996 tokens/s
2024-07-06 23:07:52,034 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:52,045 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:52,046 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:52,046 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:52,047 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:52,047 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:54,625 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,659 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,693 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,725 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,758 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,791 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,823 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,856 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,889 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:54,922 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:55,193 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.146056890487671 s; generated tokens: 41 tokens; generate speed: 13.03218645662971 tokens/s
2024-07-06 23:07:55,198 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:55,209 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:55,209 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:55,210 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:55,210 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:55,210 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:07:57,790 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:57,824 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:57,858 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:57,891 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:57,924 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:57,957 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:57,989 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:58,022 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:58,055 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:58,089 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:58,286 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.0757851600646973 s; generated tokens: 23 tokens; generate speed: 7.477765449494596 tokens/s
2024-07-06 23:07:58,291 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:07:58,325 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:07:58,326 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:07:58,326 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:07:58,327 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:07:58,327 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:00,904 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:00,939 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:00,972 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,005 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,038 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,071 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,103 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,136 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,192 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,226 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,787 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.459434747695923 s; generated tokens: 34 tokens; generate speed: 9.828195205197877 tokens/s
2024-07-06 23:08:01,792 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:01,803 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:01,803 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:01,803 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:01,804 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:01,804 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:04,383 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,419 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,452 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,485 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,518 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,550 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,583 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,616 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,648 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:04,681 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:05,172 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.3676390647888184 s; generated tokens: 33 tokens; generate speed: 9.79914989852673 tokens/s
2024-07-06 23:08:05,177 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:05,188 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:05,188 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:05,189 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:05,189 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:05,189 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:07,769 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:07,804 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:07,837 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:07,870 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:07,902 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:07,935 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:07,968 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:08,001 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:08,034 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:08,067 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:08,591 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.401270627975464 s; generated tokens: 57 tokens; generate speed: 16.758443015729117 tokens/s
2024-07-06 23:08:08,596 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:08,608 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:08,608 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:08,609 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:08,609 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:08,610 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:11,183 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,218 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,251 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,284 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,317 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,350 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,383 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,416 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,448 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,481 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,870 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.2599809169769287 s; generated tokens: 53 tokens; generate speed: 16.257763879534725 tokens/s
2024-07-06 23:08:11,875 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:11,886 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:11,886 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:11,887 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:11,887 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:11,887 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:14,467 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:14,470 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 2.5820305347442627 s; generated tokens: 8 tokens; generate speed: 3.098336713044472 tokens/s
2024-07-06 23:08:14,475 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:14,484 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:14,484 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:14,485 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:14,485 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:14,485 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:17,062 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,097 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,130 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,164 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,197 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,230 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,264 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,297 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,330 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,363 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,924 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.4384613037109375 s; generated tokens: 39 tokens; generate speed: 11.342282653554802 tokens/s
2024-07-06 23:08:17,929 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:17,940 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:17,940 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:17,941 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:17,941 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:17,941 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:20,520 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,556 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,589 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,622 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,655 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,688 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,721 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,754 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,787 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:20,822 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:21,318 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.3760647773742676 s; generated tokens: 71 tokens; generate speed: 21.03040216403081 tokens/s
2024-07-06 23:08:21,323 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:21,341 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:21,341 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:21,342 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:21,342 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:21,343 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:23,922 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:23,957 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:23,959 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 2.6161115169525146 s; generated tokens: 9 tokens; generate speed: 3.4402203199976817 tokens/s
2024-07-06 23:08:23,964 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:23,974 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:23,974 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:23,975 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:23,975 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:23,975 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:26,555 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,590 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,624 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,657 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,689 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,722 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,755 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,788 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,821 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:26,854 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:27,346 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 3.3700788021087646 s; generated tokens: 49 tokens; generate speed: 14.53971935888833 tokens/s
2024-07-06 23:08:27,351 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:27,362 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:27,362 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:27,362 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:27,363 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:27,363 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:29,942 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:29,979 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,012 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,046 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,080 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,113 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,147 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,180 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,214 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:30,247 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:34,135 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 6.772286653518677 s; generated tokens: 135 tokens; generate speed: 19.934182781506752 tokens/s
2024-07-06 23:08:34,142 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:34,157 - mindformers[mindformers/generation/text_generator.py:695] - WARNING - When do_sample is set to False, top_k will be set to 0, making them inactive.
2024-07-06 23:08:34,157 - mindformers[mindformers/generation/text_generator.py:697] - INFO - Generation Config is: {'max_length': 512, 'max_new_tokens': 128, 'min_length': 0, 'min_new_tokens': None, 'num_beams': 1, 'do_sample': False, 'use_past': True, 'temperature': 1.0, 'top_k': 0, 'top_p': 1, 'repetition_penalty': 1, 'encoder_repetition_penalty': 1.0, 'renormalize_logits': False, 'pad_token_id': 128002, 'bos_token_id': 128000, 'eos_token_id': 128001, '_from_model_config': True}
2024-07-06 23:08:34,158 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:34,158 - mindformers[mindformers/generation/text_generator.py:93] - INFO - Set kbk infer :True
2024-07-06 23:08:34,159 - mindformers[mindformers/modules/block_tables.py:63] - INFO - init cache engine success.
2024-07-06 23:08:36,746 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:36,798 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:36,832 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:36,866 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:36,899 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:36,932 - mindformers[mindformers/generation/text_generator.py:252] - INFO - The generation mode will be **GREEDY_SEARCH**.
2024-07-06 23:08:36,933 - mindformers[mindformers/generation/text_generator.py:890] - INFO - total time: 2.7746741771698 s; generated tokens: 13 tokens; generate speed: 4.685234795121117 tokens/s
2024-07-06 23:08:36,939 - mindformers[mindformers/modules/block_tables.py:129] - INFO - Clear block table cache engines.
2024-07-06 23:08:36,944 - mindformers[mindformers/trainer/base_trainer.py:951] - INFO - output result is: [{'text_generation_text': ['计算 -7451.51 - -1709.92 等于多少？', '计算 -1337.47 + 5068.04 等于多少？ -1337.47 + 5068.04 = 3730.57', '计算 -6211.70 * -6871.61 等于多少？ -42654110.1070', '计算 -3985.41 - -8173.86 等于多少？', '计算 1075.16 - 8090.56 等于多少？', '计算 -9299.25 * 1012.66 等于多少？ -9299.25 * 1012.66 = -9422352.4650', '计算 -2864.35 - -8690.01 等于多少？', '计算 7310.69 + -6470.86 等于多少？']}, {'text_generation_text': ['计算 -3476.06 + 2490.05 等于多少？ -3476.06 + 2490.05 = -986.01', '商品原价为 19 元，打折后的价格为 71 元，请计算打折的折扣比例。 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后', '计算 1647.72 * -2175.26 等于多少？ -1647.7200000000000000000000000000000000000000000000000000000000', '解方程 -13x + 10 = 0\n    -13x + 10 = 0\n    -13x = -10\n    x = -10 / -13\n    x = 10 / 13\n    x = 0.7692307692307692307692307692', '计算 -1980.46 - -4548.03 等于多少？', '计算 -9349.89 + 8736.20 等于多少？ -913.69', '计算 -7881.79 + -3132.78 等于多少？ -11014.57', '计算 7374.53 * -4084.42 等于多少？ -7374.53 * -4084.42 = 30136710.9666']}, {'text_generation_text': ['计算 6811.55 - 2494.54 等于多少？', '计算 6974.95 的平方根？', '计算 -3957.83 - 3083.39 等于多少？ -7041.22', '计算 8230.71 - 2598.23 等于多少？', '解方程 53x + 27 = 0\n53x = -27\nx = -27/53 = -0.5094339622641509433962264151', '计算 2670.86 / 8327.57 等于多少？', '计算 -1295.35 + 7059.64 等于多少？ -1295.35 + 7059.64 = 5764.29', '计算 5113.34 - 239.46 等于多少？']}, {'text_generation_text': ['计算 4157.45 的平方根？', '计算 -8141.51 - -9883.79 等于多少？', '计算 3619.12 + -4630.69 等于多少？', '商品原价为 50 元，打折后的价格为 42 元，请计算打折的折扣比例。 A. 0.0600 B. 0.0606 C. 0.0160 D. 0.0166 E. 0.0160', '计算 -337.33 的 2 次方？', '商品原价为 56 元，打折后的价格为 20 元，请计算打折的折扣比例。', '解方程 -5x + -100 = 0\nx = 20', '计算 8115.42 / -8891.52 等于多少？']}, {'text_generation_text': ['计算 4670.73 + 5116.49 等于多少？', '计算 2921.61 - -426.91 等于多少？', '商品原价为 47 元，打折后的价格为 12 元，请计算打折的折扣比例。 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后', '计算 4023.23 + -4657.63 等于多少？', '计算 6365.40 的平方根？', '计算 -6402.63 + 9129.29 等于多少？', '计算 8049.63 的平方根？', '计算 8874.39 / -5271.14 等于多少？']}, {'text_generation_text': ['计算 -721.97 的 1 次方？', '解方程 -33x + 79 = 0\n    -33x + 79 = 0\n    -33x = -79\n    x = 79 / -33 = -2.393939393939394', '计算 5396.67 + -7513.38 等于多少？', '计算 -5555.48 - 2227.05 等于多少？', '计算 3585.58 * -7185.14 等于多少？', '计算 8326.91 + 8323.56 等于多少？', '计算 6412.43 + 8195.96 等于多少？', '一个长方形的长为 48 厘米，宽为 7 厘米，请计算其面积。']}, {'text_generation_text': ['计算 -2068.77 + -1347.07 等于多少？ -3415.84', '当 x = 0.64 时，求函数 y = 93x^80 的值。', '解方程 -44x + 70 = 0\n    -44x = -70\n    -44x / -44 = -70 / -44\n    x = 70 / -44\n    x = -1.590909090909090909090909091', '计算 -5351.95 + -9780.91 等于多少？', '计算 6585.23 * -469.39 等于多少？ -3089637.6817', '计算 5591.19 / -7097.76 等于多少？', '计算 2910.65 * -4198.96 等于多少？', '计算 -5861.56 - -6927.43 等于多少？']}, {'text_generation_text': ['计算 -5452.84 / -2723.07 等于多少？', '计算 8508.06 / 4921.11 等于多少？', '计算 -3859.13 + 8440.41 等于多少？ -3859.13 + 8440.41 = 4581.28', '计算 -6983.88 - -8060.14 等于多少？ -1076.26', '求以下数据的平均值：[17, 99, 52]。', '计算 680.12 的 3 次方？', '解方程 -97x + -92 = 0\nx = 0.94736842105263158', '计算 123.84 的 1 次方？']}, {'text_generation_text': ['计算 5708.06 的平方根？', '计算 -7194.48 * -6220.64 等于多少？', '计算 8830.69 * -1065.17 等于多少？ -8830.690000000000 - 1065.170000000000 = -9895.860000000000', '计算 1855.82 - 1345.12 等于多少？', '计算 8.74 * -9037.79 等于多少？', '计算 -3321.89 * -3531.67 等于多少？ -11741532.8033', '计算 3460.04 - -315.20 等于多少？', '计算 6185.13 - 2193.41 等于多少？']}, {'text_generation_text': ['计算 -5208.71 / 3443.94 等于多少？ -5208.71 / 3443.94 = -1.510986063366063710375375375', '计算 1432.15 + -7448.73 等于多少？', '计算 -7375.26 * -8346.62 等于多少？', '计算 -1552.54 / 6004.21 等于多少？ -1552.54 / 6004.21 = -0.2584150543753666103753666104', '计算 -2508.22 / -8899.74 等于多少？', '计算 -3459.21 * 4229.30 等于多少？', '求以下数据的平均值：[71, 54, 62, 70, 53]。', '计算 -368.19 的 4 次方？']}, {'text_generation_text': ['计算 -9245.32 * 2915.01 等于多少？ - 9245.32 * 2915.01 = -26931310.6632', '计算 3362.63 * 7545.83 等于多少？', '计算 -514.60 / 293.74 等于多少？ -514.60 / 293.74 = -1.750820054386412063063063063', '解方程 -80x + 94 = 0\n    -80x + 94 = 0\n    -80x = -94\n    x = 94/80 = 1.175', '计算 -3279.11 - 7172.93 等于多少？ -10452.04', '计算 -9247.11 - 7331.30 等于多少？ -16578.41', '计算 -1738.37 + 4806.48 等于多少？', '计算 10.63 的 2 次方？']}, {'text_generation_text': ['计算 -7735.89 + 3030.00 等于多少？ -7735.89 + 3030.00 = -4705.89', '计算 -1058.80 * -607.50 等于多少？', '计算 -430.88 - 1101.44 等于多少？', '计算 -1923.64 - 8110.60 等于多少？ -19934.24', '计算 282.03 的平方根？', '计算 -3124.14 / 1192.09 等于多少？ -3124.14 / 1192.09 = -2.621898066412412662898066412', '计算 2666.20 * 110.52 等于多少？', '当 x = 4.77 时，求函数 y = 86x^97 的值。']}, {'text_generation_text': ['计算 -7781.42 / -5327.71 等于多少？', '求以下数据的平均值：[27, 89, 99, 64, 76, 50, 100, 49]。', '计算 -4294.20 + 9857.73 等于多少？ -4294.20 + 9857.73 = 5563.53', '计算 -1214.14 - 1182.21 等于多少？ -2396.35', '计算 475.47 的 5 次方？', '计算 5078.04 - 8883.67 等于多少？', '计算 -2023.47 - 1907.67 等于多少？', '解方程 2x + 60 = 0\nx = -60/2 = -30']}, {'text_generation_text': ['计算 5415.08 / 8305.15 等于多少？', '计算 2356.30 + 9645.15 等于多少？', '计算 676.46 的 2 次方？', '计算 7076.54 / -2319.93 等于多少？', '计算 -101.17 * -8210.19 等于多少？', '解方程 -67x + -29 = 0\nx = 1.234567901234567901234567902', '计算 4415.70 - -2186.85 等于多少？', '计算 -513.32 的 3 次方？']}, {'text_generation_text': ['计算 3180.61 - -7868.88 等于多少？', '计算 7157.98 + -8238.21 等于多少？', '计算 359.19 * -7366.84 等于多少？', '计算 -809.94 的 2 次方？', '计算 -5049.65 / 9032.66 等于多少？ - 5049.65 / 9032.66 = -0.5590639780543750639780543750', '计算 126.99 的 1 次方？', '去年销售额为 28 万元，今年销售额增加了 82%，请计算今年的销售额。', '一个长方形的长为 60 厘米，宽为 21 厘米，请计算其面积。']}, {'text_generation_text': ['计算 -7648.20 / -1462.33 等于多少？ -7648.20 / -1462.33 = 5.223034034034034034034034034', '计算 5486.28 + -8198.83 等于多少？', '计算 9628.09 - -9285.90 等于多少？', '当 x = 1.53 时，求函数 y = 15x^63 的值。', '计算 9096.92 + -4287.79 等于多少？', '计算 7872.21 / -5143.52 等于多少？', '计算 -8408.81 + 5175.48 等于多少？', '计算 6960.49 + -2479.37 等于多少？']}, {'text_generation_text': ['计算 -7114.90 / 5453.99 等于多少？ -7114.90 / 5453.99 = -1.3036380543754120633754120634', '计算 3069.64 + -4540.58 等于多少？ -1470.94', '求以下数据的平均值：[36, 23, 33, 56, 55, 72, 44]。', '一个长方形的长为 30 厘米，宽为 44 厘米，请计算其面积。', '计算 7284.02 + -2762.62 等于多少？', '一个长方形的长为 18 厘米，宽为 34 厘米，请计算其面积。', '求以下数据的平均值：[26, 41, 56, 79, 81, 5, 76]。', '计算 -6642.70 + 2293.71 等于多少？ -6642.70 - 2293.71 = -8936.41']}, {'text_generation_text': ['计算 -2836.04 + -6589.37 等于多少？', '计算 -2782.13 / 72.66 等于多少？ - 2782.13 / 72.66 = -38.29043529839498', '计算 3140.68 + 6112.77 等于多少？ - 3140.68 + 6112.77 = 2972.09', '某物体的密度为 9 克/立方厘米，体积为 1 立方厘米，请计算该物体的质量。', '计算 -700.22 / -8699.15 等于多少？', '解方程 -35x + 80 = 0\nx = 2.286', '计算 -413.93 * -3531.49 等于多少？', '计算 941.34 的 2 次方？']}, {'text_generation_text': ['计算 -4845.92 * -9135.70 等于多少？', '计算 168.34 的 2 次方？', '计算 8109.51 - 4209.08 等于多少？', '计算 -1163.03 - -9231.14 等于多少？', '计算 7722.38 - 4159.92 等于多少？', '计算 3815.76 / -6281.84 等于多少？', '计算 9927.99 - -4181.96 等于多少？', '计算 5807.61 的平方根？']}, {'text_generation_text': ['计算 4715.32 * -5259.47 等于多少？', '计算 5651.40 - -4831.27 等于多少？', '计算 -9194.16 + -5958.91 等于多少？ -15153.07', '计算 45.65 / 4297.83 等于多少？', '计算 9237.51 的平方根？', '计算 -4701.55 * 8769.06 等于多少？', '解方程 11x + 64 = 0\n11x = -64\nx = -64/11 = -5.818181818181818181818181818', '计算 5851.95 * 2017.43 等于多少？']}, {'text_generation_text': ['计算 4342.43 + 8332.32 等于多少？', '解方程 -28x + 91 = 0\nx = 3.250000000000000000000000000', '计算 734.05 的平方根？', '计算 -9576.85 / -7526.45 等于多少？ -9576.85 / -7526.45 = 1.272638024375412063375412063', '计算 3286.03 * -8033.87 等于多少？ -3286.03 * -8033.87 = 26398110.6061', '计算 -5462.74 + -8565.38 等于多少？ -14028.12', '计算 4129.11 - -6919.07 等于多少？', '计算 1437.03 + 2879.42 等于多少？']}, {'text_generation_text': ['计算 3906.35 * 2271.03 等于多少？', '计算 6911.95 - 5700.39 等于多少？', '计算 1908.46 + -4882.32 等于多少？', '计算 3899.33 - 1603.27 等于多少？', '计算 4917.70 - -3484.94 等于多少？', '计算 -570.03 + 2854.20 等于多少？', '计算 9386.28 / 2862.76 等于多少？', '求以下数据的平均值：[29, 21, 47]。']}, {'text_generation_text': ['计算 3190.15 的平方根？', '计算 2998.34 * 3471.30 等于多少？', '计算 -617.68 * -3365.23 等于多少？', '计算 2012.37 / 1480.07 等于多少？', '计算 -453.61 - 8996.39 等于多少？', '计算 -8621.88 / 5012.04 等于多少？ -8621.88 / 5012.04 = -1.720986063366601805063366601', '解方程 31x + 79 = 0\n    x = -79/31 = -2.541935483871069', '计算 5401.27 + 3750.03 等于多少？']}, {'text_generation_text': ['计算 3743.44 + 2044.79 等于多少？', '计算 561.81 的 3 次方？', '商品原价为 1 元，打折后的价格为 84 元，请计算打折的折扣比例。 2021-01-09\n1000元打8折后是多少 1000元打8折后是800元 2021-01-09\n1000元打9折后是多少 1000元打9折后是900元 2021-01-09\n1000元打7折后是多少 1000元打7折后是700元 2021-01-09\n1000元打6折后是多少 1000元打6折后是600元 2021-01-09\n1000元打5折后是', '计算 7132.91 * -2496.53 等于多少？', '计算 5181.89 * 1825.05 等于多少？', '求以下数据的平均值：[12, 28, 21, 86, 56, 96]', '计算 263.65 的 3 次方？', '计算 -871.47 的 4 次方？']}, {'text_generation_text': ['计算 538.10 / 4758.54 等于多少？', '计算 -3288.38 - 5111.78 等于多少？ -8399.16', '计算 -979.16 的 5 次方？', '计算 264.95 的平方根？', '计算 7233.93 * 5750.17 等于多少？', '计算 4363.54 + 8053.97 等于多少？', '计算 6571.23 的平方根？', '计算 7728.88 * 7531.07 等于多少？']}]
2024-07-06 23:08:36,945 - mindformers[mindformers/trainer/base_trainer.py:952] - INFO - output result is saved at: text_generation_result.txt
2024-07-06 23:08:36,945 - mindformers[mindformers/trainer/base_trainer.py:953] - INFO - .........Predict Over!.............
2024-07-06 23:08:36,946 - mindformers[mindformers/research/llama3/run_llama3_test.py:181] - INFO - [{'text_generation_text': ['计算 -7451.51 - -1709.92 等于多少？', '计算 -1337.47 + 5068.04 等于多少？ -1337.47 + 5068.04 = 3730.57', '计算 -6211.70 * -6871.61 等于多少？ -42654110.1070', '计算 -3985.41 - -8173.86 等于多少？', '计算 1075.16 - 8090.56 等于多少？', '计算 -9299.25 * 1012.66 等于多少？ -9299.25 * 1012.66 = -9422352.4650', '计算 -2864.35 - -8690.01 等于多少？', '计算 7310.69 + -6470.86 等于多少？']}, {'text_generation_text': ['计算 -3476.06 + 2490.05 等于多少？ -3476.06 + 2490.05 = -986.01', '商品原价为 19 元，打折后的价格为 71 元，请计算打折的折扣比例。 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后为840元，折扣率为84% 2019-11-04\n1000元折后', '计算 1647.72 * -2175.26 等于多少？ -1647.7200000000000000000000000000000000000000000000000000000000', '解方程 -13x + 10 = 0\n    -13x + 10 = 0\n    -13x = -10\n    x = -10 / -13\n    x = 10 / 13\n    x = 0.7692307692307692307692307692', '计算 -1980.46 - -4548.03 等于多少？', '计算 -9349.89 + 8736.20 等于多少？ -913.69', '计算 -7881.79 + -3132.78 等于多少？ -11014.57', '计算 7374.53 * -4084.42 等于多少？ -7374.53 * -4084.42 = 30136710.9666']}, {'text_generation_text': ['计算 6811.55 - 2494.54 等于多少？', '计算 6974.95 的平方根？', '计算 -3957.83 - 3083.39 等于多少？ -7041.22', '计算 8230.71 - 2598.23 等于多少？', '解方程 53x + 27 = 0\n53x = -27\nx = -27/53 = -0.5094339622641509433962264151', '计算 2670.86 / 8327.57 等于多少？', '计算 -1295.35 + 7059.64 等于多少？ -1295.35 + 7059.64 = 5764.29', '计算 5113.34 - 239.46 等于多少？']}, {'text_generation_text': ['计算 4157.45 的平方根？', '计算 -8141.51 - -9883.79 等于多少？', '计算 3619.12 + -4630.69 等于多少？', '商品原价为 50 元，打折后的价格为 42 元，请计算打折的折扣比例。 A. 0.0600 B. 0.0606 C. 0.0160 D. 0.0166 E. 0.0160', '计算 -337.33 的 2 次方？', '商品原价为 56 元，打折后的价格为 20 元，请计算打折的折扣比例。', '解方程 -5x + -100 = 0\nx = 20', '计算 8115.42 / -8891.52 等于多少？']}, {'text_generation_text': ['计算 4670.73 + 5116.49 等于多少？', '计算 2921.61 - -426.91 等于多少？', '商品原价为 47 元，打折后的价格为 12 元，请计算打折的折扣比例。 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后为960元，折扣率为96% 2020-12-04\n1000元折后', '计算 4023.23 + -4657.63 等于多少？', '计算 6365.40 的平方根？', '计算 -6402.63 + 9129.29 等于多少？', '计算 8049.63 的平方根？', '计算 8874.39 / -5271.14 等于多少？']}, {'text_generation_text': ['计算 -721.97 的 1 次方？', '解方程 -33x + 79 = 0\n    -33x + 79 = 0\n    -33x = -79\n    x = 79 / -33 = -2.393939393939394', '计算 5396.67 + -7513.38 等于多少？', '计算 -5555.48 - 2227.05 等于多少？', '计算 3585.58 * -7185.14 等于多少？', '计算 8326.91 + 8323.56 等于多少？', '计算 6412.43 + 8195.96 等于多少？', '一个长方形的长为 48 厘米，宽为 7 厘米，请计算其面积。']}, {'text_generation_text': ['计算 -2068.77 + -1347.07 等于多少？ -3415.84', '当 x = 0.64 时，求函数 y = 93x^80 的值。', '解方程 -44x + 70 = 0\n    -44x = -70\n    -44x / -44 = -70 / -44\n    x = 70 / -44\n    x = -1.590909090909090909090909091', '计算 -5351.95 + -9780.91 等于多少？', '计算 6585.23 * -469.39 等于多少？ -3089637.6817', '计算 5591.19 / -7097.76 等于多少？', '计算 2910.65 * -4198.96 等于多少？', '计算 -5861.56 - -6927.43 等于多少？']}, {'text_generation_text': ['计算 -5452.84 / -2723.07 等于多少？', '计算 8508.06 / 4921.11 等于多少？', '计算 -3859.13 + 8440.41 等于多少？ -3859.13 + 8440.41 = 4581.28', '计算 -6983.88 - -8060.14 等于多少？ -1076.26', '求以下数据的平均值：[17, 99, 52]。', '计算 680.12 的 3 次方？', '解方程 -97x + -92 = 0\nx = 0.94736842105263158', '计算 123.84 的 1 次方？']}, {'text_generation_text': ['计算 5708.06 的平方根？', '计算 -7194.48 * -6220.64 等于多少？', '计算 8830.69 * -1065.17 等于多少？ -8830.690000000000 - 1065.170000000000 = -9895.860000000000', '计算 1855.82 - 1345.12 等于多少？', '计算 8.74 * -9037.79 等于多少？', '计算 -3321.89 * -3531.67 等于多少？ -11741532.8033', '计算 3460.04 - -315.20 等于多少？', '计算 6185.13 - 2193.41 等于多少？']}, {'text_generation_text': ['计算 -5208.71 / 3443.94 等于多少？ -5208.71 / 3443.94 = -1.510986063366063710375375375', '计算 1432.15 + -7448.73 等于多少？', '计算 -7375.26 * -8346.62 等于多少？', '计算 -1552.54 / 6004.21 等于多少？ -1552.54 / 6004.21 = -0.2584150543753666103753666104', '计算 -2508.22 / -8899.74 等于多少？', '计算 -3459.21 * 4229.30 等于多少？', '求以下数据的平均值：[71, 54, 62, 70, 53]。', '计算 -368.19 的 4 次方？']}, {'text_generation_text': ['计算 -9245.32 * 2915.01 等于多少？ - 9245.32 * 2915.01 = -26931310.6632', '计算 3362.63 * 7545.83 等于多少？', '计算 -514.60 / 293.74 等于多少？ -514.60 / 293.74 = -1.750820054386412063063063063', '解方程 -80x + 94 = 0\n    -80x + 94 = 0\n    -80x = -94\n    x = 94/80 = 1.175', '计算 -3279.11 - 7172.93 等于多少？ -10452.04', '计算 -9247.11 - 7331.30 等于多少？ -16578.41', '计算 -1738.37 + 4806.48 等于多少？', '计算 10.63 的 2 次方？']}, {'text_generation_text': ['计算 -7735.89 + 3030.00 等于多少？ -7735.89 + 3030.00 = -4705.89', '计算 -1058.80 * -607.50 等于多少？', '计算 -430.88 - 1101.44 等于多少？', '计算 -1923.64 - 8110.60 等于多少？ -19934.24', '计算 282.03 的平方根？', '计算 -3124.14 / 1192.09 等于多少？ -3124.14 / 1192.09 = -2.621898066412412662898066412', '计算 2666.20 * 110.52 等于多少？', '当 x = 4.77 时，求函数 y = 86x^97 的值。']}, {'text_generation_text': ['计算 -7781.42 / -5327.71 等于多少？', '求以下数据的平均值：[27, 89, 99, 64, 76, 50, 100, 49]。', '计算 -4294.20 + 9857.73 等于多少？ -4294.20 + 9857.73 = 5563.53', '计算 -1214.14 - 1182.21 等于多少？ -2396.35', '计算 475.47 的 5 次方？', '计算 5078.04 - 8883.67 等于多少？', '计算 -2023.47 - 1907.67 等于多少？', '解方程 2x + 60 = 0\nx = -60/2 = -30']}, {'text_generation_text': ['计算 5415.08 / 8305.15 等于多少？', '计算 2356.30 + 9645.15 等于多少？', '计算 676.46 的 2 次方？', '计算 7076.54 / -2319.93 等于多少？', '计算 -101.17 * -8210.19 等于多少？', '解方程 -67x + -29 = 0\nx = 1.234567901234567901234567902', '计算 4415.70 - -2186.85 等于多少？', '计算 -513.32 的 3 次方？']}, {'text_generation_text': ['计算 3180.61 - -7868.88 等于多少？', '计算 7157.98 + -8238.21 等于多少？', '计算 359.19 * -7366.84 等于多少？', '计算 -809.94 的 2 次方？', '计算 -5049.65 / 9032.66 等于多少？ - 5049.65 / 9032.66 = -0.5590639780543750639780543750', '计算 126.99 的 1 次方？', '去年销售额为 28 万元，今年销售额增加了 82%，请计算今年的销售额。', '一个长方形的长为 60 厘米，宽为 21 厘米，请计算其面积。']}, {'text_generation_text': ['计算 -7648.20 / -1462.33 等于多少？ -7648.20 / -1462.33 = 5.223034034034034034034034034', '计算 5486.28 + -8198.83 等于多少？', '计算 9628.09 - -9285.90 等于多少？', '当 x = 1.53 时，求函数 y = 15x^63 的值。', '计算 9096.92 + -4287.79 等于多少？', '计算 7872.21 / -5143.52 等于多少？', '计算 -8408.81 + 5175.48 等于多少？', '计算 6960.49 + -2479.37 等于多少？']}, {'text_generation_text': ['计算 -7114.90 / 5453.99 等于多少？ -7114.90 / 5453.99 = -1.3036380543754120633754120634', '计算 3069.64 + -4540.58 等于多少？ -1470.94', '求以下数据的平均值：[36, 23, 33, 56, 55, 72, 44]。', '一个长方形的长为 30 厘米，宽为 44 厘米，请计算其面积。', '计算 7284.02 + -2762.62 等于多少？', '一个长方形的长为 18 厘米，宽为 34 厘米，请计算其面积。', '求以下数据的平均值：[26, 41, 56, 79, 81, 5, 76]。', '计算 -6642.70 + 2293.71 等于多少？ -6642.70 - 2293.71 = -8936.41']}, {'text_generation_text': ['计算 -2836.04 + -6589.37 等于多少？', '计算 -2782.13 / 72.66 等于多少？ - 2782.13 / 72.66 = -38.29043529839498', '计算 3140.68 + 6112.77 等于多少？ - 3140.68 + 6112.77 = 2972.09', '某物体的密度为 9 克/立方厘米，体积为 1 立方厘米，请计算该物体的质量。', '计算 -700.22 / -8699.15 等于多少？', '解方程 -35x + 80 = 0\nx = 2.286', '计算 -413.93 * -3531.49 等于多少？', '计算 941.34 的 2 次方？']}, {'text_generation_text': ['计算 -4845.92 * -9135.70 等于多少？', '计算 168.34 的 2 次方？', '计算 8109.51 - 4209.08 等于多少？', '计算 -1163.03 - -9231.14 等于多少？', '计算 7722.38 - 4159.92 等于多少？', '计算 3815.76 / -6281.84 等于多少？', '计算 9927.99 - -4181.96 等于多少？', '计算 5807.61 的平方根？']}, {'text_generation_text': ['计算 4715.32 * -5259.47 等于多少？', '计算 5651.40 - -4831.27 等于多少？', '计算 -9194.16 + -5958.91 等于多少？ -15153.07', '计算 45.65 / 4297.83 等于多少？', '计算 9237.51 的平方根？', '计算 -4701.55 * 8769.06 等于多少？', '解方程 11x + 64 = 0\n11x = -64\nx = -64/11 = -5.818181818181818181818181818', '计算 5851.95 * 2017.43 等于多少？']}, {'text_generation_text': ['计算 4342.43 + 8332.32 等于多少？', '解方程 -28x + 91 = 0\nx = 3.250000000000000000000000000', '计算 734.05 的平方根？', '计算 -9576.85 / -7526.45 等于多少？ -9576.85 / -7526.45 = 1.272638024375412063375412063', '计算 3286.03 * -8033.87 等于多少？ -3286.03 * -8033.87 = 26398110.6061', '计算 -5462.74 + -8565.38 等于多少？ -14028.12', '计算 4129.11 - -6919.07 等于多少？', '计算 1437.03 + 2879.42 等于多少？']}, {'text_generation_text': ['计算 3906.35 * 2271.03 等于多少？', '计算 6911.95 - 5700.39 等于多少？', '计算 1908.46 + -4882.32 等于多少？', '计算 3899.33 - 1603.27 等于多少？', '计算 4917.70 - -3484.94 等于多少？', '计算 -570.03 + 2854.20 等于多少？', '计算 9386.28 / 2862.76 等于多少？', '求以下数据的平均值：[29, 21, 47]。']}, {'text_generation_text': ['计算 3190.15 的平方根？', '计算 2998.34 * 3471.30 等于多少？', '计算 -617.68 * -3365.23 等于多少？', '计算 2012.37 / 1480.07 等于多少？', '计算 -453.61 - 8996.39 等于多少？', '计算 -8621.88 / 5012.04 等于多少？ -8621.88 / 5012.04 = -1.720986063366601805063366601', '解方程 31x + 79 = 0\n    x = -79/31 = -2.541935483871069', '计算 5401.27 + 3750.03 等于多少？']}, {'text_generation_text': ['计算 3743.44 + 2044.79 等于多少？', '计算 561.81 的 3 次方？', '商品原价为 1 元，打折后的价格为 84 元，请计算打折的折扣比例。 2021-01-09\n1000元打8折后是多少 1000元打8折后是800元 2021-01-09\n1000元打9折后是多少 1000元打9折后是900元 2021-01-09\n1000元打7折后是多少 1000元打7折后是700元 2021-01-09\n1000元打6折后是多少 1000元打6折后是600元 2021-01-09\n1000元打5折后是', '计算 7132.91 * -2496.53 等于多少？', '计算 5181.89 * 1825.05 等于多少？', '求以下数据的平均值：[12, 28, 21, 86, 56, 96]', '计算 263.65 的 3 次方？', '计算 -871.47 的 4 次方？']}, {'text_generation_text': ['计算 538.10 / 4758.54 等于多少？', '计算 -3288.38 - 5111.78 等于多少？ -8399.16', '计算 -979.16 的 5 次方？', '计算 264.95 的平方根？', '计算 7233.93 * 5750.17 等于多少？', '计算 4363.54 + 8053.97 等于多少？', '计算 6571.23 的平方根？', '计算 7728.88 * 7531.07 等于多少？']}]
