# 交互式推理

run_mode: 'predict'
seed: 0

# mindspore context init config
context:
  mode: 0                   # 0--Graph Mode; 1--Pynative Mode
  device_target: "Ascend"
  max_call_depth: 10000
  max_device_memory: "28GB"
  save_graphs: True
  save_graphs_path: "./graph/llama3"
  device_id: 0
  runtime_num_threads: 1

# model config
model:
  model_config:
    type: LlamaConfig
    batch_size: 1       # add for increase predict
    seq_length: 256
    hidden_size: 4096
    num_layers: 32
    num_heads: 32
    n_kv_heads: 8
    vocab_size: 128256
    intermediate_size: 14336
    rms_norm_eps: 1.0e-5
    bos_token_id: 128000
    eos_token_id: 128001
    pad_token_id: 128002
    ignore_token_id: -100
    compute_dtype: "bfloat16"
    layernorm_compute_type: "float32"
    softmax_compute_type: "float32"
    rotary_dtype: "float32"
    param_init_type: "bfloat16"
    use_past: False             # support from mindspore==2.3, FA is not available on CPU
    use_flash_attention: False  # FA is not available on CPU
    scaling_factor: 1.0
    extend_method: "None"       # support "None", "PI", "NTK"
    theta: 500000
    offset: 0
    fine_grain_interleave: 1
    #checkpoint_name_or_path: "./llama3-8B.ckpt"
    checkpoint_name_or_path: "./mindformers/research/output/checkpoint_network/rank_0/llama3_8b_rank_0-network.ckpt"
    repetition_penalty: 1
    max_decode_length: 128
    max_new_tokens: 128
    min_new_tokens: 1
    top_k: 3
    top_p: 1
    do_sample: False
  arch:
    type: LlamaForCausalLM

profile: False
profile_start_step: 4
profile_stop_step: 8
init_start_profile: False
profile_communication: False
profile_memory: True
layer_scale: False
layer_decay: 0.65
lr_scale_factor: 256
